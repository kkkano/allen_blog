---
title: 'K-means 聚类算法：从理论到图像压缩'
date: '2025-04-20'
tags: ['机器学习', '聚类', '图像处理']
draft: false
summary: '在这篇博客中，我们将深入探索 K-means 聚类算法，了解它如何将数据分组，并将其应用于图像压缩，减少图像颜色数量。'
authors: ['allen']
---

# K-means 聚类算法：从理论到图像压缩

欢迎体验这场关于 K-means 聚类算法的学习之旅！在这篇博客中，我们将从零开始实现 K-means 算法，探索它如何将数据分组为簇，并将其应用于图像压缩，将一张彩色图片的颜色数量从数千种减少到仅 16 种。无论你是机器学习新手还是想巩固知识，这篇教程都将通过理论、代码和可视化带你一步步掌握这一经典算法。准备好了吗？让我们开始吧！

## 目录
- [1 - 实现 K-means 算法](#1)
  - [1.1 寻找最近的簇中心](#1.1)
    - [练习 1：实现 `find_closest_centroids`](#ex01)
  - [1.2 计算簇中心均值](#1.2)
    - [练习 2：实现 `compute_centroids`](#ex02)
- [2 - 在样例数据集上运行 K-means](#2)
- [3 - 随机初始化簇中心](#3)
- [4 - 使用 K-means 进行图像压缩](#4)
  - [4.1 图像数据集](#4.1)
  - [4.2 对图像像素应用 K-means](#4.2)
  - [4.3 压缩图像](#4.3)

> **注意：** 为确保代码在部署（如 Vercel）时不出错，请勿修改非练习代码块。完成练习后，如果想进一步实验，可以参考文末的指引。

---

<a name="1"></a>
## 1 - 实现 K-means 算法

K-means 是一种无监督学习算法，用于将相似的数据点自动分组为簇。想象你有一堆散乱的珠子，想把它们按颜色或大小整理成几堆，这就是 K-means 的工作原理！具体来说，它的工作流程如下：
- 随机猜测初始的簇中心（称为 *质心* 或 *centroids*）。
- 不断重复两个步骤，直到质心稳定：
  1. 将每个数据点分配到离它最近的质心。
  2. 根据分配的点，更新每个质心的位置为这些点的平均位置。

### K-means 伪代码
```python
# 初始化质心
centroids = kMeans_init_centroids(X, K)

# 循环固定次数
for iter in range(iterations):
    # 步骤 1：将数据点分配到最近的质心
    idx = find_closest_centroids(X, centroids)
    # 步骤 2：根据分配更新质心位置
    centroids = compute_centroids(X, idx, K)
```

我们将分两部分实现这个算法：寻找最近的质心和更新质心位置。

<a name="1.1"></a>
### 1.1 寻找最近的簇中心

在 *簇分配* 阶段，K-means 算法为每个数据点找到离它最近的质心，并记录质心的索引。

<a name="ex01"></a>
#### 练习 1：实现 `find_closest_centroids`

我们需要实现以下功能：
- **输入：** 数据矩阵 `X`（形状为 `(m, n)`，其中 `m` 是数据点数量，`n` 是特征数量）和质心矩阵 `centroids`（形状为 `(K, n)`，其中 `K` 是簇数量）。
- **输出：** 数组 `idx`（形状为 `(m,)`），记录每个数据点最近的质心索引（0 到 `K-1`）。

对于每个数据点 $x^{(i)}$，我们找到使其距离最小的质心 $mu_j $：
- 距离定义为欧氏距离的平方：$\| x^{(i)} - \mu_j \|^2$



