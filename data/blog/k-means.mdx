---
title: 'K-means 聚类算法：从理论到图像压缩'
date: '2025-04-20'
tags: ['机器学习', '聚类', '图像处理']
draft: false
summary: '在这篇博客中，我们将深入探索 K-means 聚类算法，了解它如何将数据分组，并将其应用于图像压缩，减少图像颜色数量。'
authors: ['allen']
---

# K-means 聚类算法：从理论到图像压缩

欢迎体验这场关于 K-means 聚类算法的学习之旅！在这篇博客中，我们将从零开始实现 K-means 算法，探索它如何将数据分组为簇，并将其应用于图像压缩，将一张彩色图片的颜色数量从数千种减少到仅 16 种。无论你是机器学习新手还是想巩固知识，这篇教程都将通过理论、代码和可视化带你一步步掌握这一经典算法。准备好了吗？让我们开始吧！

## 目录
- [1 - 实现 K-means 算法](#1)
  - [1.1 寻找最近的簇中心](#1.1)
    - [练习 1：实现 `find_closest_centroids`](#ex01)
  - [1.2 计算簇中心均值](#1.2)
    - [练习 2：实现 `compute_centroids`](#ex02)
- [2 - 在样例数据集上运行 K-means](#2)
- [3 - 随机初始化簇中心](#3)
- [4 - 使用 K-means 进行图像压缩](#4)
  - [4.1 图像数据集](#4.1)
  - [4.2 对图像像素应用 K-means](#4.2)
  - [4.3 压缩图像](#4.3)

> **注意：** 为确保代码在部署（如 Vercel）时不出错，请勿修改非练习代码块。完成练习后，如果想进一步实验，可以参考文末的指引。

---

<a name="1"></a>
## 1 - 实现 K-means 算法

K-means 是一种无监督学习算法，用于将相似的数据点自动分组为簇。想象你有一堆散乱的珠子，想把它们按颜色或大小整理成几堆，这就是 K-means 的工作原理！具体来说，它的工作流程如下：
- 随机猜测初始的簇中心（称为 *质心* 或 *centroids*）。
- 不断重复两个步骤，直到质心稳定：
  1. 将每个数据点分配到离它最近的质心。
  2. 根据分配的点，更新每个质心的位置为这些点的平均位置。

### K-means 伪代码
```python
# 初始化质心
centroids = kMeans_init_centroids(X, K)

# 循环固定次数
for iter in range(iterations):
    # 步骤 1：将数据点分配到最近的质心
    idx = find_closest_centroids(X, centroids)
    # 步骤 2：根据分配更新质心位置
    centroids = compute_centroids(X, idx, K)
```

我们将分两部分实现这个算法：寻找最近的质心和更新质心位置。

<a name="1.1"></a>
### 1.1 寻找最近的簇中心

在 *簇分配* 阶段，K-means 算法为每个数据点找到离它最近的质心，并记录质心的索引。

<a name="ex01"></a>
#### 练习 1：实现 `find_closest_centroids`

我们需要实现以下功能：
- **输入：** 数据矩阵 `X`（形状为 `(m, n)`，其中 `m` 是数据点数量，`n` 是特征数量）和质心矩阵 `centroids`（形状为 `(K, n)`，其中 `K` 是簇数量）。
- **输出：** 数组 `idx`（形状为 `(m,)`），记录每个数据点最近的质心索引（0 到 `K-1`）。

对于每个数据点 \(x^{(i)}\)，我们找到使其距离最小的质心 \(\mu_j\)：
- 距离定义为欧氏距离的平方：$\| x^{(i)} - \mu_j \|^2$

代码如下：

```python
import numpy as np

def find_closest_centroids(X, centroids):

    为每个数据点分配最近的质心。
    
    参数:
        X (ndarray): 形状 (m, n)，数据点
        centroids (ndarray): 形状 (K, n)，质心
    
    返回:
        idx (array_like): 形状 (m,)，每个数据点的最近质心索引

    K = centroids.shape[0]
    idx = np.zeros(X.shape[0], dtype=int)
    
    for i in range(X.shape[0]):
        distances = []
        for j in range(K):
            # 计算数据点 X[i] 到质心 j 的欧氏距离
            norm_ij = np.linalg.norm(X[i] - centroids[j])
            distances.append(norm_ij)
        # 选择距离最小的质心索引
        idx[i] = np.argmin(distances)
    
    return idx
```

**代码详解：**
- `np.linalg.norm(X[i] - centroids[j])` 计算点和质心之间的欧氏距离。
- `np.argmin(distances)` 返回最小距离对应的质心索引。
- 循环遍历每个数据点，确保每个点都被正确分配。

**测试代码：**
为了直观理解，假设我们有一个小型数据集：

```python
# 样例数据（2D 点）
X = np.array([[1.8, 4.6], [5.6, 4.8], [6.3, 3.3], [2.9, 4.6], [3.2, 4.9]])
initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])
idx = find_closest_centroids(X, initial_centroids)
print("前三个数据点的最近质心索引:", idx[:3])
```

**预期输出：**
```
前三个数据点的最近质心索引: [0 2 1]
```

这表示第一个点分配到质心 0，第二个点到质心 2，第三个点到质心 1。

---

<a name="1.2"></a>
### 1.2 计算簇中心均值

在 *质心更新* 阶段，我们根据分配结果重新计算每个质心的位置，取分配到该质心的所有点的平均值。

<a name="ex02"></a>
#### 练习 2：实现 `compute_centroids`

任务：
- **输入：** 数据 `X`、分配索引 `idx` 和簇数量 `K`。
- **输出：** 更新后的质心矩阵（形状为 `(K, n)`）。

对于每个质心 \(\mu_k\)，更新公式为：
- 取所有分配到簇 \(k\) 的点的平均值：$\mu_k = \frac{1}{|C_k|} \sum_{i \in C_k} x^{(i)}$

实现代码如下：

```python
def compute_centroids(X, idx, K):
 
    根据分配结果更新质心位置。
    
    参数:
        X (ndarray): 形状 (m, n)，数据点
        idx (ndarray): 形状 (m,)，质心分配索引
        K (int): 簇数量
    
    返回:
        centroids (ndarray): 形状 (K, n)，更新后的质心

    m, n = X.shape
    centroids = np.zeros((K, n))
    
    for k in range(K):
        # 获取分配到簇 k 的所有点
        points = X[idx == k]
        # 计算这些点的均值
        centroids[k] = np.mean(points, axis=0)
    
    return centroids
```

