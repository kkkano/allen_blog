---
title: 'æœºå™¨å­¦ä¹ æ¨¡å‹è¯„ä¼°ä¸æ”¹è¿›'
date: '2025-04-05'
tags: ['Machine Learning', 'Model Evaluation', 'Overfitting', 'Regularization', 'Neural Networks']
draft: false
summary: 'æœ¬æ–‡ç³»ç»Ÿè®²è§£æœºå™¨å­¦ä¹ æ¨¡å‹è¯„ä¼°çš„æ ¸å¿ƒæ–¹æ³•è®ºï¼Œé€šè¿‡å¤šé¡¹å¼å›å½’ä¸ç¥ç»ç½‘ç»œåŒè§†è§’ï¼Œæ·±å…¥è§£æè®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†çš„ååŒå·¥ä½œæœºåˆ¶ã€‚ç»“åˆæ¢¯åº¦ä¸‹é™å¯è§†åŒ–ä¸æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå®Œæ•´å‘ˆç°ä»æ¬ æ‹Ÿåˆè¯†åˆ«ã€è¿‡æ‹Ÿåˆä¿®æ­£åˆ°æ¨¡å‹æ³›åŒ–çš„å…¨æµç¨‹ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶æä¾›å¯å¤ç°çš„Pythonä»£ç å®ç°ã€‚'
authors: ['allen']
---

æ·±å…¥æ¢è®¨å¦‚ä½•è¯„ä¼°å’Œä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤šé¡¹å¼å›å½’å’Œç¥ç»ç½‘ç»œã€‚é€šè¿‡ç†è®ºè®²è§£ã€ä»£ç å®ç°å’Œå¯è§†åŒ–åˆ†æï¼Œä½ å°†å­¦ä¼šå¦‚ä½•è¯†åˆ«å’Œè§£å†³è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆé—®é¢˜ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## ç›®å½•

- [1 - å‡†å¤‡å·¥å…·åŒ…](#1)
- [2 - è¯„ä¼°å­¦ä¹ ç®—æ³•ï¼ˆå¤šé¡¹å¼å›å½’ï¼‰](#2)
  - [2.1 æ•°æ®é›†åˆ†å‰²](#2.1)
  - [2.2 æ¨¡å‹è¯„ä¼°çš„è¯¯å·®è®¡ç®—ï¼ˆçº¿æ€§å›å½’ï¼‰](#2.2)
  - [2.3 æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„æ€§èƒ½](#2.3)
- [3 - åå·®ä¸æ–¹å·®](#3)
  - [3.1 ç»˜åˆ¶è®­ç»ƒã€äº¤å‰éªŒè¯å’Œæµ‹è¯•æ•°æ®](#3.1)
  - [3.2 å¯»æ‰¾æœ€ä¼˜å¤šé¡¹å¼é˜¶æ•°](#3.2)
  - [3.3 è°ƒæ•´æ­£åˆ™åŒ–](#3.3)
  - [3.4 è·å–æ›´å¤šæ•°æ®ï¼šå¢åŠ è®­ç»ƒé›†è§„æ¨¡](#3.4)
- [4 - è¯„ä¼°å­¦ä¹ ç®—æ³•ï¼ˆç¥ç»ç½‘ç»œï¼‰](#4)
  - [4.1 æ•°æ®é›†](#4.1)
  - [4.2 é€šè¿‡è®¡ç®—åˆ†ç±»è¯¯å·®è¯„ä¼°åˆ†ç±»æ¨¡å‹](#4.2)
- [5 - æ¨¡å‹å¤æ‚åº¦](#5)
  - [5.1 ç®€å•æ¨¡å‹](#5.1)
- [6 - æ­£åˆ™åŒ–](#6)
- [7 - è¿­ä»£å¯»æ‰¾æœ€ä¼˜æ­£åˆ™åŒ–å€¼](#7)
  - [7.1 æµ‹è¯•](#7.1)

---

<a name="1"></a>
## 1 - å‡†å¤‡å·¥å…·åŒ…

åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥ä¸€äº› Python å·¥å…·åŒ…ï¼Œå®ƒä»¬å°†å¸®åŠ©æˆ‘ä»¬å®Œæˆæ•°æ®å¤„ç†ã€å¯è§†åŒ–å’Œæ¨¡å‹å®ç°ã€‚è¿è¡Œä¸‹æ–¹ä»£ç ä»¥åŠ è½½è¿™äº›å·¥å…·ï¼š

- **[NumPy](https://numpy.org/)**ï¼šPython ä¸­å¤„ç†æ•°ç»„å’ŒçŸ©é˜µè¿ç®—çš„åŸºç¡€åº“ã€‚
- **[Matplotlib](https://matplotlib.org/)**ï¼šä¸€ä¸ªå¼ºå¤§çš„ç»˜å›¾åº“ï¼Œç”¨äºç”Ÿæˆæ•°æ®å¯è§†åŒ–å›¾è¡¨ã€‚
- **[Scikit-learn](https://scikit-learn.org/stable/)**ï¼šä¸€ä¸ªç”¨äºæ•°æ®æŒ–æ˜å’Œæœºå™¨å­¦ä¹ çš„åº“ã€‚
- **[TensorFlow](https://www.tensorflow.org/)**ï¼šä¸€ä¸ªæµè¡Œçš„æœºå™¨å­¦ä¹ å¹³å°ã€‚

```python
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.activations import relu,linear
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

tf.keras.backend.set_floatx('float64')
from assignment_utils import *

tf.autograph.set_verbosity(0)
```

è¿™äº›å·¥å…·æ˜¯æˆ‘ä»¬å®ç°å’Œè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„åŸºçŸ³ã€‚

---

<a name="2"></a>
## 2 - è¯„ä¼°å­¦ä¹ ç®—æ³•ï¼ˆå¤šé¡¹å¼å›å½’ï¼‰

![è®­ç»ƒä¸æ–°æ•°æ®](/static/images/MLc2/output2w3_01.png)

å‡è®¾ä½ å·²ç»åˆ›å»ºäº†ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸”å‘ç°å®ƒåœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆå¾—éå¸¸å¥½ã€‚ä½ å®Œæˆäº†å—ï¼Ÿè¿˜æ²¡é‚£ä¹ˆå¿«ã€‚åˆ›å»ºæ¨¡å‹çš„ç›®çš„æ˜¯èƒ½å¤Ÿé¢„æµ‹**æ–°**æ ·æœ¬çš„å€¼ã€‚

å¦‚ä½•åœ¨éƒ¨ç½²æ¨¡å‹ä¹‹å‰æµ‹è¯•å…¶åœ¨æ–°æ•°æ®ä¸Šçš„æ€§èƒ½ï¼Ÿç­”æ¡ˆæœ‰ä¸¤éƒ¨åˆ†ï¼š

- å°†åŸå§‹æ•°æ®é›†åˆ†ä¸ºâ€œè®­ç»ƒâ€å’Œâ€œæµ‹è¯•â€é›†ã€‚
  - ä½¿ç”¨è®­ç»ƒæ•°æ®æ‹Ÿåˆæ¨¡å‹çš„å‚æ•°ã€‚
  - ä½¿ç”¨æµ‹è¯•æ•°æ®è¯„ä¼°æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚
- å¼€å‘ä¸€ä¸ªè¯¯å·®å‡½æ•°æ¥è¯„ä¼°æ¨¡å‹ã€‚

<a name="2.1"></a>
### 2.1 æ•°æ®é›†åˆ†å‰²

è®²åº§ä¸­å»ºè®®å°† 20-40% çš„æ•°æ®é›†ä¿ç•™ä¸ºæµ‹è¯•é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `sklearn` çš„ [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) å‡½æ•°æ¥æ‰§è¡Œåˆ†å‰²ã€‚è¿è¡Œä»¥ä¸‹ä»£ç åï¼Œè¯·æ£€æŸ¥æ•°æ®çš„å½¢çŠ¶ã€‚

```python
# ç”Ÿæˆä¸€äº›æ•°æ®
X, y, x_ideal, y_ideal = gen_data(18, 2, 0.7)
print("X.shape", X.shape, "y.shape", y.shape)

# ä½¿ç”¨ sklearn ä¾‹ç¨‹åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)
print("X_train.shape", X_train.shape, "y_train.shape", y_train.shape)
print("X_test.shape", X_test.shape, "y_test.shape", y_test.shape)
```

> X.shape (18,) y.shape (18,)
> X_train.shape (12,) y_train.shape (12,)
> X_test.shape (6,) y_test.shape (6,)

```python
fig, ax = plt.subplots(1,1,figsize=(4,4))
ax.plot(x_ideal, y_ideal, "--", color = "orangered", label="y_ideal", lw=1)
ax.set_title("Training, Test",fontsize = 14)
ax.set_xlabel("x")
ax.set_ylabel("y")

ax.scatter(X_train, y_train, color = "red",           label="train")
ax.scatter(X_test, y_test,   color = dlc["dlblue"],   label="test")
ax.legend(loc='upper left')
plt.show()
```

![æ•°æ®å›¾2](/static/images/MLc2/output2w3_02.png)

<a name="2.2"></a>

### 2.2 æ¨¡å‹è¯„ä¼°çš„è¯¯å·®è®¡ç®—ï¼ˆçº¿æ€§å›å½’ï¼‰

åœ¨è¯„ä¼°çº¿æ€§å›å½’æ¨¡å‹æ—¶ï¼Œä½ éœ€è¦è®¡ç®—é¢„æµ‹å€¼ä¸ç›®æ ‡å€¼ä¹‹é—´çš„å¹³æ–¹è¯¯å·®å·®çš„å¹³å‡å€¼ã€‚

$$
J_{\text{test}}(\mathbf{w}, b) = \frac{1}{2m_{\text{test}}} \sum_{i=0}^{m_{\text{test}}-1} \left( f_{\mathbf{w}, b}(\mathbf{x}^{(i)}_{\text{test}}) - y^{(i)}_{\text{test}} \right)^2
$$

#### ç»ƒä¹  1

è¯·å®Œæˆä»¥ä¸‹ `eval_mse` å‡½æ•°ï¼Œè®¡ç®—æ•°æ®é›†ä¸Šçš„å‡æ–¹è¯¯å·®ã€‚

```python
def eval_mse(y, yhat):
    """
    è®¡ç®—æ•°æ®é›†ä¸Šçš„å‡æ–¹è¯¯å·®ã€‚
    å‚æ•°:
        y (ndarray): å½¢çŠ¶ (m,) æˆ– (m,1)ï¼Œæ¯ä¸ªæ ·æœ¬çš„ç›®æ ‡å€¼
        yhat (ndarray): å½¢çŠ¶ (m,) æˆ– (m,1)ï¼Œæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼
    è¿”å›:
        err (scalar): å‡æ–¹è¯¯å·®
    """
    m = len(y)
    err = 0.0
    for i in range(m):
        err_i = (yhat[i] - y[i]) ** 2
        err += err_i
    err = err / (2 * m)
    return err
```

<a name="2.3"></a>
### 2.3 æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„æ€§èƒ½

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªé«˜é˜¶å¤šé¡¹å¼æ¨¡å‹æ¥æœ€å°åŒ–è®­ç»ƒè¯¯å·®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `sklearn` çš„çº¿æ€§å›å½’å‡½æ•°ã€‚ä»¥ä¸‹æ˜¯æ­¥éª¤ï¼š

- åˆ›å»ºå¹¶æ‹Ÿåˆæ¨¡å‹ï¼ˆâ€œæ‹Ÿåˆâ€æ˜¯è®­ç»ƒæˆ–è¿è¡Œæ¢¯åº¦ä¸‹é™çš„å¦ä¸€ç§è¯´æ³•ï¼‰ã€‚
- è®¡ç®—è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®ã€‚
- è®¡ç®—æµ‹è¯•æ•°æ®ä¸Šçš„è¯¯å·®ã€‚

```python
# åœ¨ sklearn ä¸­åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶åœ¨è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒ
degree = 10
lmodel = lin_model(degree)
lmodel.fit(X_train, y_train)

# é¢„æµ‹è®­ç»ƒæ•°æ®ï¼Œè®¡ç®—è®­ç»ƒè¯¯å·®
yhat = lmodel.predict(X_train)
err_train = lmodel.mse(y_train, yhat)

# é¢„æµ‹æµ‹è¯•æ•°æ®ï¼Œè®¡ç®—è¯¯å·®
yhat = lmodel.predict(X_test)
err_test = lmodel.mse(y_test, yhat)
```

```python
print(f"training err {err_train:0.2f}, test err {err_test:0.2f}")
```

> training err 58.01, test err 171215.01

è®­ç»ƒé›†ä¸Šçš„è¯¯å·®è¿œå°äºæµ‹è¯•é›†ä¸Šçš„è¯¯å·®ã€‚ä»¥ä¸‹å›¾è¡¨æ˜¾ç¤ºäº†åŸå› ã€‚æ¨¡å‹éå¸¸å¥½åœ°æ‹Ÿåˆäº†è®­ç»ƒæ•°æ®ï¼Œä½†ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªå¤æ‚çš„å‡½æ•°ã€‚æµ‹è¯•æ•°æ®ä¸æ˜¯è®­ç»ƒçš„ä¸€éƒ¨åˆ†ï¼Œæ¨¡å‹åœ¨è¿™äº›æ•°æ®ä¸Šçš„é¢„æµ‹æ•ˆæœå¾ˆå·®ã€‚è¿™ç§æ¨¡å‹å¯ä»¥æè¿°ä¸º 1) è¿‡æ‹Ÿåˆï¼Œ2) å…·æœ‰é«˜æ–¹å·®ï¼Œ3) â€œæ³›åŒ–â€èƒ½åŠ›å·®ã€‚

```python
# ç»˜åˆ¶é¢„æµ‹æ›²çº¿
x = np.linspace(0, int(X.max()), 100)
y_pred = lmodel.predict(x).reshape(-1, 1)

plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree)
```

---

![æ•°æ®å›¾3](/static/images/MLc2/output2w3_03.png)

ä¸‹è¡¨ä¸­æ˜¾ç¤ºçš„è®­ç»ƒé›†ã€äº¤å‰éªŒè¯é›†å’Œæµ‹è¯•é›†çš„åˆ†å¸ƒæ˜¯å…¸å‹åˆ†å¸ƒï¼Œä½†å¯èƒ½ä¼šå› å¯ç”¨æ•°æ®é‡è€Œå¼‚ã€‚

|                 data æ•°æ® | % of total å æ€»æ•°çš„ç™¾åˆ†æ¯” | Description æè¿°                                                                                                                                                                             |
| ------------------------: | :-----------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|             training è®­ç»ƒ |            60             | Data used to tune model parameters ğ‘¤w and ğ‘b in training or fitting ç”¨äºè°ƒæ•´æ¨¡å‹å‚æ•° ğ‘¤w ä»¥åŠ ğ‘b ç”¨äºè®­ç»ƒæˆ–æ‹Ÿåˆçš„æ•°æ®                                                                         |
| cross-validation äº¤å‰éªŒè¯ |            20             | Data used to tune other model parameters like degree of polynomial, regularization or the architecture of a neural network. ç”¨äºè°ƒæ•´å…¶ä»–æ¨¡å‹å‚æ•°çš„æ•°æ®ï¼Œä¾‹å¦‚å¤šé¡¹å¼ã€æ­£åˆ™åŒ–æˆ–ç¥ç»ç½‘ç»œçš„æ¶æ„ã€‚ |
|                 test æµ‹è¯• |            20             | Data used to test the model after tuning to gauge performance on new data ä¼˜åŒ–åç”¨äºæµ‹è¯•æ¨¡å‹ä»¥è¡¡é‡æ–°æ•°æ®æ€§èƒ½çš„æ•°æ®                                                                           |

è®©æˆ‘ä»¬åœ¨ä¸‹é¢ç”Ÿæˆä¸‰ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨ `train_test_split` from `sklearn` ï¼Œä½†å°†è°ƒç”¨å®ƒä¸¤æ¬¡ä»¥è·å¾—ä¸‰ä¸ªæ‹†åˆ†ï¼š

```python
# Generate  data
X,y, x_ideal,y_ideal = gen_data(40, 5, 0.7)
print("X.shape", X.shape, "y.shape", y.shape)

#split the data using sklearn routine
X_train, X_, y_train, y_ = train_test_split(X,y,test_size=0.40, random_state=1)
X_cv, X_test, y_cv, y_test = train_test_split(X_,y_,test_size=0.50, random_state=1)
print("X_train.shape", X_train.shape, "y_train.shape", y_train.shape)
print("X_cv.shape", X_cv.shape, "y_cv.shape", y_cv.shape)
print("X_test.shape", X_test.shape, "y_test.shape", y_test.shape)
```

> X.shape (40,) y.shape (40,)
> X_train.shape (24,) y_train.shape (24,)
> X_cv.shape (8,) y_cv.shape (8,)
> X_test.shape (8,) y_test.shape (8,)

<a name="3"></a>

## 3 - åå·®ä¸æ–¹å·®

![åå·®ä¸æ–¹å·®](/static/images/MLc2/output2w3_04.png)

ä¸Šé¢å¾ˆæ˜æ˜¾ï¼Œå¤šé¡¹å¼æ¨¡å‹çš„é˜¶æ•°è¿‡é«˜ã€‚å¦‚ä½•é€‰æ‹©ä¸€ä¸ªå¥½çš„å€¼ï¼Ÿäº‹å®è¯æ˜ï¼Œå¦‚å›¾æ‰€ç¤ºï¼Œè®­ç»ƒå’Œäº¤å‰éªŒè¯çš„æ€§èƒ½å¯ä»¥æä¾›æŒ‡å¯¼ã€‚é€šè¿‡å°è¯•ä¸€ç³»åˆ—é˜¶æ•°å€¼ï¼Œå¯ä»¥è¯„ä¼°è®­ç»ƒå’Œäº¤å‰éªŒè¯çš„æ€§èƒ½ã€‚éšç€é˜¶æ•°å˜å¾—è¿‡å¤§ï¼Œäº¤å‰éªŒè¯çš„æ€§èƒ½å°†å¼€å§‹ç›¸å¯¹äºè®­ç»ƒæ€§èƒ½ä¸‹é™ã€‚è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­å°è¯•è¿™ä¸€ç‚¹ã€‚

<a name="3.1"></a>
### 3.1 ç»˜åˆ¶è®­ç»ƒã€äº¤å‰éªŒè¯å’Œæµ‹è¯•æ•°æ®

ä½ å¯ä»¥çœ‹åˆ°ä»¥ä¸‹æ•°æ®ç‚¹ï¼Œè®­ç»ƒï¼ˆçº¢è‰²ï¼‰ã€äº¤å‰éªŒè¯ï¼ˆæ©™è‰²ï¼‰å’Œæµ‹è¯•ï¼ˆè“è‰²ï¼‰æ•°æ®ç‚¹æ˜¯æ··åœ¨ä¸€èµ·çš„ã€‚

```python
fig, ax = plt.subplots(1, 1, figsize=(4, 4))
ax.plot(x_ideal, y_ideal, "--", color="orangered", label="y_ideal", lw=1)
ax.set_title("Training, CV, Test", fontsize=14)
ax.set_xlabel("x")
ax.set_ylabel("y")

ax.scatter(X_train, y_train, color="red", label="train")
ax.scatter(X_cv, y_cv, color="orange", label="cv")
ax.scatter(X_test, y_test, color="blue", label="test")
ax.legend(loc='upper left')
plt.show()
```

![æ•°æ®å›¾5](/static/images/MLc2/output2w3_05.png)

<a name="3.2"></a>

### 3.2 å¯»æ‰¾æœ€ä¼˜å¤šé¡¹å¼é˜¶æ•°

åœ¨ä¹‹å‰çš„å®éªŒä¸­ï¼Œä½ å‘ç°å¯ä»¥é€šè¿‡ä½¿ç”¨å¤šé¡¹å¼æ¥åˆ›å»ºèƒ½å¤Ÿæ‹Ÿåˆå¤æ‚æ›²çº¿çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä½ è¿˜å±•ç¤ºäº†é€šè¿‡å¢åŠ å¤šé¡¹å¼çš„é˜¶æ•°ï¼Œä½ å¯ä»¥â€œåˆ¶é€ â€è¿‡æ‹Ÿåˆã€‚è®©æˆ‘ä»¬åœ¨è¿™é‡Œåˆ©ç”¨è¿™äº›çŸ¥è¯†ï¼Œæµ‹è¯•æˆ‘ä»¬åŒºåˆ†è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„èƒ½åŠ›ã€‚

æˆ‘ä»¬å°†åå¤è®­ç»ƒæ¨¡å‹ï¼Œæ¯æ¬¡è¿­ä»£å¢åŠ å¤šé¡¹å¼çš„é˜¶æ•°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) çš„çº¿æ€§å›å½’æ¨¡å‹ä»¥è¿½æ±‚é€Ÿåº¦å’Œç®€æ´æ€§ã€‚

```python
max_degree = 9
err_train = np.zeros(max_degree)
err_cv = np.zeros(max_degree)
x = np.linspace(0, int(X.max()), 100)
y_pred = np.zeros((100, max_degree))

for degree in range(max_degree):
    lmodel = lin_model(degree + 1)
    lmodel.fit(X_train, y_train)
    yhat = lmodel.predict(X_train)
    err_train[degree] = lmodel.mse(y_train, yhat)
    yhat = lmodel.predict(X_cv)
    err_cv[degree] = lmodel.mse(y_cv, yhat)
    y_pred[:, degree] = lmodel.predict(x)

optimal_degree = np.argmin(err_cv) + 1
```

è®©æˆ‘ä»¬ç»˜åˆ¶ç»“æœï¼š

```python
plt.close("all")
plt_optimal_degree(X_train, y_train, X_cv, y_cv, x, y_pred, x_ideal, y_ideal,
                   err_train, err_cv, optimal_degree, max_degree)
```

![æ•°æ®å›¾6](/static/images/MLc2/output2w3_06.png)

ä¸Šè¿°å›¾è¡¨å±•ç¤ºäº†å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒå’Œæœªè®­ç»ƒé›†å¯ä»¥å¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ¨¡å‹æ˜¯æ¬ æ‹Ÿåˆè¿˜æ˜¯è¿‡æ‹Ÿåˆã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¢åŠ å¤šé¡¹å¼çš„é˜¶æ•°åˆ›å»ºäº†ä»æ¬ æ‹Ÿåˆåˆ°è¿‡æ‹Ÿåˆçš„å„ç§æ¨¡å‹ã€‚

- å·¦ä¾§å›¾è¡¨ä¸­ï¼Œå®çº¿ä»£è¡¨è¿™äº›æ¨¡å‹çš„é¢„æµ‹ã€‚é˜¶æ•°ä¸º 1 çš„å¤šé¡¹å¼æ¨¡å‹äº§ç”Ÿä¸€æ¡ç›´çº¿ï¼Œå‡ ä¹ä¸ç©¿è¿‡æ•°æ®ç‚¹ï¼Œè€Œæœ€å¤§é˜¶æ•°çš„æ¨¡å‹åˆ™éå¸¸è´´åˆæ¯ä¸ªæ•°æ®ç‚¹ã€‚
- å³ä¾§å›¾è¡¨ä¸­ï¼š
  - è®­ç»ƒæ•°æ®çš„è¯¯å·®ï¼ˆè“è‰²ï¼‰éšç€æ¨¡å‹å¤æ‚åº¦çš„å¢åŠ è€Œå‡å°ï¼Œè¿™åœ¨æ„æ–™ä¹‹ä¸­ã€‚
  - äº¤å‰éªŒè¯æ•°æ®çš„è¯¯å·®æœ€åˆéšç€æ¨¡å‹å¼€å§‹é€‚åº”æ•°æ®è€Œå‡å°ï¼Œä½†éšç€æ¨¡å‹å¼€å§‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¿‡æ‹Ÿåˆï¼ˆæœªèƒ½â€œæ³›åŒ–â€ï¼‰è€Œå¢åŠ ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›ç¤ºä¾‹ä¸­çš„æ›²çº¿å¹¶ä¸åƒè®²åº§ä¸­é‚£æ ·å¹³æ»‘ã€‚è¿™æ˜¯å¯ä»¥é¢„æœŸçš„ã€‚é‡è¦çš„æ˜¯æ€»ä½“è¶‹åŠ¿ï¼šéšç€æ¨¡å‹å¤æ‚åº¦çš„å¢åŠ ï¼Œè®­ç»ƒè¯¯å·®å‡å°ï¼Œè€Œäº¤å‰éªŒè¯è¯¯å·®å…ˆå‡åå¢ã€‚

<a name="3.3"></a>
### 3.3 è°ƒæ•´æ­£åˆ™åŒ–

åœ¨ä¹‹å‰çš„å®éªŒä¸­ï¼Œä½ åˆ©ç”¨*æ­£åˆ™åŒ–*æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚ä¸é˜¶æ•°ç±»ä¼¼ï¼Œä½ å¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•æ¥è°ƒæ•´æ­£åˆ™åŒ–å‚æ•° Î»ã€‚

è®©æˆ‘ä»¬é€šè¿‡ä»é«˜é˜¶å¤šé¡¹å¼å¼€å§‹ï¼Œå¹¶æ”¹å˜æ­£åˆ™åŒ–å‚æ•°æ¥æ¼”ç¤ºè¿™ä¸€ç‚¹ã€‚

```python
lambda_range = np.array([0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100])
num_steps = len(lambda_range)
degree = 10
err_train = np.zeros(num_steps)
err_cv = np.zeros(num_steps)
x = np.linspace(0, int(X.max()), 100)
y_pred = np.zeros((100, num_steps))

for i in range(num_steps):
    lambda_ = lambda_range[i]
    lmodel = lin_model(degree, regularization=True, lambda_=lambda_)
    lmodel.fit(X_train, y_train)
    yhat = lmodel.predict(X_train)
    err_train[i] = lmodel.mse(y_train, yhat)
    yhat = lmodel.predict(X_cv)
    err_cv[i] = lmodel.mse(y_cv, yhat)
    y_pred[:, i] = lmodel.predict(x)

optimal_reg_idx = np.argmin(err_cv)
```

```python
plt.close("all")
plt_tune_regularization(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, optimal_reg_idx, lambda_range)
```

![æ•°æ®å›¾7](/static/images/MLc2/output2w3_07.png)

ä¸Šè¿°å›¾è¡¨æ˜¾ç¤ºï¼Œéšç€æ­£åˆ™åŒ–çš„å¢åŠ ï¼Œæ¨¡å‹ä»é«˜æ–¹å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰æ¨¡å‹è½¬å˜ä¸ºé«˜åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰æ¨¡å‹ã€‚å³ä¾§å›¾è¡¨ä¸­çš„å‚ç›´çº¿æ˜¾ç¤ºäº† Î» çš„æœ€ä¼˜å€¼ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œå¤šé¡¹å¼é˜¶æ•°è®¾ç½®ä¸º 10ã€‚

<a name="3.4"></a>
### 3.4 è·å–æ›´å¤šæ•°æ®ï¼šå¢åŠ è®­ç»ƒé›†è§„æ¨¡

å½“æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆé«˜æ–¹å·®ï¼‰æ—¶ï¼Œæ”¶é›†æ›´å¤šæ•°æ®å¯ä»¥æé«˜æ€§èƒ½ã€‚è®©æˆ‘ä»¬åœ¨è¿™é‡Œå°è¯•ã€‚

```python
X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range, degree = tune_m()
plt_tune_m(X_train, y_train, X_cv, y_cv, x, y_pred, err_train, err_cv, m_range, degree)
```

![æ•°æ®å›¾8](/static/images/MLc2/output2w3_08.png)

ä¸Šè¿°å›¾è¡¨æ˜¾ç¤ºï¼Œå½“æ¨¡å‹å…·æœ‰é«˜æ–¹å·®å¹¶è¿‡æ‹Ÿåˆæ—¶ï¼Œå¢åŠ æ ·æœ¬æ•°é‡å¯ä»¥æé«˜æ€§èƒ½ã€‚è¯·æ³¨æ„å·¦ä¾§å›¾è¡¨ä¸­çš„æ›²çº¿ã€‚å…·æœ‰æœ€é«˜ m å€¼çš„æœ€ç»ˆæ›²çº¿æ˜¯ä¸€æ¡å¹³æ»‘çš„æ›²çº¿ï¼Œä½äºæ•°æ®çš„ä¸­å¿ƒã€‚åœ¨å³ä¾§ï¼Œéšç€æ ·æœ¬æ•°é‡çš„å¢åŠ ï¼Œè®­ç»ƒé›†å’Œäº¤å‰éªŒè¯é›†çš„æ€§èƒ½æ”¶æ•›åˆ°ç›¸ä¼¼çš„å€¼ã€‚è¯·æ³¨æ„ï¼Œæ›²çº¿å¹¶ä¸åƒè®²åº§ä¸­é‚£æ ·å¹³æ»‘ã€‚è¿™æ˜¯å¯ä»¥é¢„æœŸçš„ã€‚è¶‹åŠ¿ä»ç„¶æ¸…æ™°ï¼šæ›´å¤šæ•°æ®å¯ä»¥æ”¹å–„æ³›åŒ–ã€‚

> **æ³¨æ„ï¼š** å½“æ¨¡å‹å…·æœ‰é«˜åå·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰æ—¶ï¼Œå¢åŠ æ ·æœ¬æ•°é‡å¹¶ä¸èƒ½æ”¹å–„æ€§èƒ½ã€‚

---

<a name="4"></a>
## 4 - è¯„ä¼°å­¦ä¹ ç®—æ³•ï¼ˆç¥ç»ç½‘ç»œï¼‰

åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œä½ è°ƒæ•´äº†å¤šé¡¹å¼å›å½’æ¨¡å‹çš„å‚æ•°ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†è½¬å‘ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è®©æˆ‘ä»¬ä»åˆ›å»ºä¸€ä¸ªåˆ†ç±»æ•°æ®é›†å¼€å§‹ã€‚

<a name="4.1"></a>
### 4.1 æ•°æ®é›†

è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œç”Ÿæˆæ•°æ®é›†å¹¶å°†å…¶åˆ†ä¸ºè®­ç»ƒã€äº¤å‰éªŒè¯ï¼ˆCVï¼‰å’Œæµ‹è¯•é›†ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¢åŠ äº†äº¤å‰éªŒè¯æ•°æ®ç‚¹çš„ç™¾åˆ†æ¯”ä»¥å¼ºè°ƒå…¶é‡è¦æ€§ã€‚

```python
# ç”Ÿæˆå’Œåˆ†å‰²æ•°æ®é›†
X, y, centers, classes, std = gen_blobs()

# åˆ†å‰²æ•°æ®ã€‚è¾ƒå¤§çš„ CV ç¾¤ä½“ä»¥ä¾›æ¼”ç¤º
X_train, X_, y_train, y_ = train_test_split(X, y, test_size=0.50, random_state=1)
X_cv, X_test, y_cv, y_test = train_test_split(X_, y_, test_size=0.20, random_state=1)
print("X_train.shape:", X_train.shape, "X_cv.shape:", X_cv.shape, "X_test.shape:", X_test.shape)
```

> X_train.shape: (400, 2) X_cv.shape: (320, 2) X_test.shape: (80, 2)

```python
plt_train_eq_dist(X_train, y_train, classes, X_cv, y_cv, centers, std)
```

![æ•°æ®å›¾9](/static/images/MLc2/output2w3_09.png)

åœ¨å·¦ä¾§å›¾è¡¨ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æ•°æ®ç‚¹æŒ‰é¢œè‰²åˆ†ä¸ºå…­ä¸ªç°‡ã€‚è®­ç»ƒç‚¹ï¼ˆåœ†ç‚¹ï¼‰å’Œäº¤å‰éªŒè¯ç‚¹ï¼ˆä¸‰è§’å½¢ï¼‰éƒ½æ˜¾ç¤ºå‡ºæ¥ã€‚æœ‰è¶£çš„æ˜¯é‚£äº›è½åœ¨æ¨¡ç³Šä½ç½®çš„ç‚¹ï¼Œä»»ä½•ç°‡éƒ½å¯èƒ½å°†å®ƒä»¬è§†ä¸ºæˆå‘˜ã€‚ä½ æœŸæœ›ç¥ç»ç½‘ç»œæ¨¡å‹ä¼šå¦‚ä½•å¤„ç†ï¼Ÿä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„ä¾‹å­ï¼Ÿ

åœ¨å³ä¾§æ˜¯ä¸€ä¸ªâ€œç†æƒ³â€æ¨¡å‹çš„ç¤ºä¾‹ï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªçŸ¥é“æ•°æ®æ¥æºçš„äººå¯èƒ½ä¼šåˆ›å»ºçš„æ¨¡å‹ã€‚çº¿æ¡ä»£è¡¨â€œç­‰è·ç¦»â€è¾¹ç•Œï¼Œå…¶ä¸­åˆ°ä¸­å¿ƒç‚¹çš„è·ç¦»ç›¸ç­‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªæ¨¡å‹ä¼šâ€œè¯¯åˆ†ç±»â€å¤§çº¦ 8% çš„æ€»æ•°æ®é›†ã€‚

<a name="4.2"></a>
### 4.2 é€šè¿‡è®¡ç®—åˆ†ç±»è¯¯å·®è¯„ä¼°åˆ†ç±»æ¨¡å‹

åˆ†ç±»æ¨¡å‹çš„è¯„ä¼°å‡½æ•°å¾ˆç®€å•ï¼šä¸æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ã€‚

$$
J_{cv} = \frac{1}{m} \sum_{i=0}^{m-1}
\begin{cases}
1, & \text{if } \hat{y}^{(i)} \neq y^{(i)} \\
0, & \text{otherwise}
\end{cases}
$$

#### ç»ƒä¹  2

è¯·å®Œæˆä»¥ä¸‹ `eval_cat_err` å‡½æ•°ï¼Œè®¡ç®—åˆ†ç±»è¯¯å·®ã€‚æ³¨æ„ï¼Œåœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œç›®æ ‡å€¼æ˜¯ç±»åˆ«çš„ç´¢å¼•ï¼Œè€Œä¸æ˜¯[one-hot ç¼–ç ](https://en.wikipedia.org/wiki/One-hot)ã€‚

```python
def eval_cat_err(y, yhat):
    """
    è®¡ç®—åˆ†ç±»è¯¯å·®ã€‚
    å‚æ•°:
        y (ndarray): å½¢çŠ¶ (m,) æˆ– (m,1)ï¼Œæ¯ä¸ªæ ·æœ¬çš„ç›®æ ‡å€¼
        yhat (ndarray): å½¢çŠ¶ (m,) æˆ– (m,1)ï¼Œæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼
    è¿”å›:
        cerr (scalar): åˆ†ç±»è¯¯å·®
    """
    m = len(y)
    incorrect = 0
    for i in range(m):
        if yhat[i] != y[i]:
            incorrect += 1
    cerr = incorrect / m
    return cerr
```

---

```python
y_hat = np.array([1, 2, 0])
y_tmp = np.array([1, 2, 3])
print(f"categorization error {np.squeeze(eval_cat_err(y_hat, y_tmp)):0.3f}, expected:0.333" )
y_hat = np.array([[1], [2], [0], [3]])
y_tmp = np.array([[1], [2], [1], [3]])
print(f"categorization error {np.squeeze(eval_cat_err(y_hat, y_tmp)):0.3f}, expected:0.250" )

test_eval_cat_err(eval_cat_err)
```

> categorization error 0.333, expected:0.333
> categorization error 0.250, expected:0.250

<a name="5"></a>

## 5 - æ¨¡å‹å¤æ‚åº¦

æ¥ä¸‹æ¥ï¼Œä½ å°†æ„å»ºä¸¤ä¸ªæ¨¡å‹ï¼šä¸€ä¸ªå¤æ‚æ¨¡å‹å’Œä¸€ä¸ªç®€å•æ¨¡å‹ã€‚ä½ å°†è¯„ä¼°è¿™äº›æ¨¡å‹ï¼Œä»¥ç¡®å®šå®ƒä»¬æ˜¯è¿‡æ‹Ÿåˆè¿˜æ˜¯æ¬ æ‹Ÿåˆã€‚

#### ç»ƒä¹  3

è¯·å®Œæˆä»¥ä¸‹ä»£ç ï¼Œæ„å»ºä¸€ä¸ªä¸‰å±‚æ¨¡å‹ï¼š

- å…·æœ‰ 120 ä¸ªå•å…ƒçš„ Dense å±‚ï¼Œrelu æ¿€æ´»
- å…·æœ‰ 40 ä¸ªå•å…ƒçš„ Dense å±‚ï¼Œrelu æ¿€æ´»
- å…·æœ‰ 6 ä¸ªå•å…ƒçš„ Dense å±‚ï¼Œçº¿æ€§æ¿€æ´»ï¼ˆä¸æ˜¯ softmaxï¼‰

ä½¿ç”¨ä»¥ä¸‹é…ç½®ç¼–è¯‘æ¨¡å‹ï¼š

- æŸå¤±å‡½æ•°ä¸º `SparseCategoricalCrossentropy`ï¼Œè®°å¾—ä½¿ç”¨ `from_logits=True`
- Adam ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º 0.01

```python
tf.random.set_seed(1234)
model = Sequential(
    [
        Dense(120, activation='relu', name="L1"),
        Dense(40, activation='relu', name="L2"),
        Dense(classes, activation='linear', name="L3")
    ], name="Complex"
)
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(0.01),
)
```

```python
model.fit(X_train, y_train, epochs=1000)
```

```py
model.summary()
model_test(model, classes, X_train.shape[1])
```

```text
Model: "Complex"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 L1 (Dense)                  (None, 120)               360

 L2 (Dense)                  (None, 40)                4840

 L3 (Dense)                  (None, 6)                 246

=================================================================
Total params: 5,446
Trainable params: 5,446
Non-trainable params: 0
_________________________________________________________________
```

è¿™ä¸ªæ¨¡å‹éå¸¸åŠªåŠ›åœ°æ•æ‰æ¯ä¸ªç±»åˆ«çš„ç¦»ç¾¤ç‚¹ã€‚ç»“æœï¼Œå®ƒé”™è¯¯åˆ†ç±»äº†ä¸€äº›äº¤å‰éªŒè¯æ•°æ®ã€‚è®©æˆ‘ä»¬è®¡ç®—åˆ†ç±»è¯¯å·®ã€‚

```python
training_cerr_complex = eval_cat_err(y_train, model_predict(X_train))
cv_cerr_complex = eval_cat_err(y_cv, model_predict(X_cv))
print(f"åˆ†ç±»è¯¯å·®ï¼Œè®­ç»ƒï¼Œå¤æ‚æ¨¡å‹: {training_cerr_complex:0.3f}")
print(f"åˆ†ç±»è¯¯å·®ï¼ŒCVï¼Œå¤æ‚æ¨¡å‹: {cv_cerr_complex:0.3f}")
```

> categorization error, training, complex model: 0.003
> categorization error, cv, complex model: 0.122

<a name="5.1"></a>

### 5.1 ç®€å•æ¨¡å‹

ç°åœ¨ï¼Œå°è¯•ä¸€ä¸ªç®€å•æ¨¡å‹ã€‚

#### ç»ƒä¹  4

è¯·å®Œæˆä»¥ä¸‹ä»£ç ï¼Œæ„å»ºä¸€ä¸ªä¸¤å±‚æ¨¡å‹ï¼š

- å…·æœ‰ 6 ä¸ªå•å…ƒçš„ Dense å±‚ï¼Œrelu æ¿€æ´»
- å…·æœ‰ 6 ä¸ªå•å…ƒçš„ Dense å±‚ï¼Œçº¿æ€§æ¿€æ´»

ä½¿ç”¨ä»¥ä¸‹é…ç½®ç¼–è¯‘æ¨¡å‹ï¼š

- æŸå¤±å‡½æ•°ä¸º `SparseCategoricalCrossentropy`ï¼Œä½¿ç”¨ `from_logits=True`
- Adam ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º 0.01

```python
tf.random.set_seed(1234)
model_s = Sequential(
    [
        Dense(6, activation='relu', name="L1"),
        Dense(classes, activation='linear', name="L2")
    ], name="Simple"
)
model_s.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(0.01),
)
```

```python
model_s.fit(X_train, y_train, epochs=1000)
```

```python
model_s.summary()
model_s_test(model_s, classes, X_train.shape[1])
```

```python
#make a model for plotting routines to call
model_predict_s = lambda Xl: np.argmax(tf.nn.softmax(model_s.predict(Xl)).numpy(),axis=1)
plt_nn(model_predict_s,X_train,y_train, classes, X_cv, y_cv, suptitle="Simple Model")
```

```text
Model: "Simple"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 L1 (Dense)                  (None, 6)                 18

 L2 (Dense)                  (None, 6)                 42

=================================================================
Total params: 60
Trainable params: 60
Non-trainable params: 0
_________________________________________________________________
```

è¿™ä¸ªç®€å•æ¨¡å‹è¡¨ç°å¾—ç›¸å½“ä¸é”™ã€‚è®©æˆ‘ä»¬è®¡ç®—åˆ†ç±»è¯¯å·®ã€‚

```python
training_cerr_simple = eval_cat_err(y_train, model_predict_s(X_train))
cv_cerr_simple = eval_cat_err(y_cv, model_predict_s(X_cv))
print(f"åˆ†ç±»è¯¯å·®ï¼Œè®­ç»ƒï¼Œç®€å•æ¨¡å‹: {training_cerr_simple:0.3f}, å¤æ‚æ¨¡å‹: {training_cerr_complex:0.3f}")
print(f"åˆ†ç±»è¯¯å·®ï¼ŒCVï¼Œç®€å•æ¨¡å‹: {cv_cerr_simple:0.3f}, å¤æ‚æ¨¡å‹: {cv_cerr_complex:0.3f}")
```

> categorization error, training, simple model, 0.062, complex model: 0.003
> categorization error, cv, simple model, 0.087, complex model: 0.122

æˆ‘ä»¬çš„ç®€å•æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„åˆ†ç±»è¯¯å·®ç•¥é«˜ï¼Œä½†åœ¨äº¤å‰éªŒè¯é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ›´å¤æ‚çš„æ¨¡å‹ã€‚

---

<a name="6"></a>
## 6 - æ­£åˆ™åŒ–

ä¸å¤šé¡¹å¼å›å½’ä¸€æ ·ï¼Œå¯ä»¥åº”ç”¨æ­£åˆ™åŒ–æ¥è°ƒèŠ‚æ›´å¤æ‚æ¨¡å‹çš„å½±å“ã€‚è®©æˆ‘ä»¬å°è¯•åœ¨ä¸‹é¢è¿™æ ·åšã€‚

#### ç»ƒä¹  5

è¯·é‡å»ºä½ çš„å¤æ‚æ¨¡å‹ï¼Œä½†è¿™æ¬¡åŒ…å«æ­£åˆ™åŒ–ã€‚æ„å»ºä¸€ä¸ªä¸‰å±‚æ¨¡å‹ï¼š

- å…·æœ‰ 120 ä¸ªå•å…ƒçš„ Dense å±‚ï¼Œrelu æ¿€æ´»ï¼Œ`kernel_regularizer=tf.keras.regularizers.l2(0.1)`
- å…·æœ‰ 40 ä¸ªå•å…ƒçš„ Dense å±‚ï¼Œrelu æ¿€æ´»ï¼Œ`kernel_regularizer=tf.keras.regularizers.l2(0.1)`
- å…·æœ‰ 6 ä¸ªå•å…ƒçš„ Dense å±‚ï¼Œçº¿æ€§æ¿€æ´»

ä½¿ç”¨ä»¥ä¸‹é…ç½®ç¼–è¯‘æ¨¡å‹ï¼š

- æŸå¤±å‡½æ•°ä¸º `SparseCategoricalCrossentropy`ï¼Œä½¿ç”¨ `from_logits=True`
- Adam ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º 0.01

```python
tf.random.set_seed(1234)
model_r = Sequential(
    [
        Dense(120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), name="L1"),
        Dense(40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), name="L2"),
        Dense(classes, activation='linear', name="L3")
    ], name="ComplexRegularized"
)
model_r.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(0.01),
)
```

```python
model_r.fit(X_train, y_train, epochs=1000)
```

```python
model_r.summary()
model_r_test(model_r, classes, X_train.shape[1])
```

```text
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 L1 (Dense)                  (None, 120)               360

 L2 (Dense)                  (None, 40)                4840

 L3 (Dense)                  (None, 6)                 246

=================================================================
Total params: 5,446
Trainable params: 5,446
Non-trainable params: 0
_________________________________________________________________
```

```python
#make a model for plotting routines to call
model_predict_r = lambda Xl: np.argmax(tf.nn.softmax(model_r.predict(Xl)).numpy(),axis=1)
plt_nn(model_predict_r, X_train,y_train, classes, X_cv, y_cv, suptitle="Regularized")
```

ç»“æœçœ‹èµ·æ¥ä¸â€œç†æƒ³â€æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚è®©æˆ‘ä»¬æ£€æŸ¥åˆ†ç±»è¯¯å·®ã€‚

```python
training_cerr_reg = eval_cat_err(y_train, model_predict_r(X_train))
cv_cerr_reg = eval_cat_err(y_cv, model_predict_r(X_cv))
print(f"åˆ†ç±»è¯¯å·®ï¼Œè®­ç»ƒï¼Œæ­£åˆ™åŒ–æ¨¡å‹: {training_cerr_reg:0.3f}, ç®€å•æ¨¡å‹: {training_cerr_simple:0.3f}, å¤æ‚æ¨¡å‹: {training_cerr_complex:0.3f}")
print(f"åˆ†ç±»è¯¯å·®ï¼ŒCVï¼Œæ­£åˆ™åŒ–æ¨¡å‹: {cv_cerr_reg:0.3f}, ç®€å•æ¨¡å‹: {cv_cerr_simple:0.3f}, å¤æ‚æ¨¡å‹: {cv_cerr_complex:0.3f}")
```

> categorization error, training, regularized: 0.072, simple model, 0.062, complex model: 0.003
> categorization error, cv, regularized: 0.066, simple model, 0.087, complex model: 0.122

ç®€å•æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„åˆ†ç±»è¯¯å·®ç•¥é«˜ï¼Œä½†åœ¨äº¤å‰éªŒè¯é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ­£åˆ™åŒ–æ¨¡å‹ã€‚

---

<a name="7"></a>
## 7 - è¿­ä»£å¯»æ‰¾æœ€ä¼˜æ­£åˆ™åŒ–å€¼

ä½ å¯ä»¥å°è¯•è®¸å¤šæ­£åˆ™åŒ–å€¼ã€‚ä»¥ä¸‹ä»£ç éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½è¿è¡Œã€‚å¦‚æœä½ æœ‰æ—¶é—´ï¼Œå¯ä»¥è¿è¡Œå¹¶æ£€æŸ¥ç»“æœã€‚å¦‚æœæ²¡æœ‰ï¼Œä½ å·²ç»å®Œæˆäº†ä½œä¸šçš„è¯„åˆ†éƒ¨åˆ†ï¼

```python
tf.random.set_seed(1234)
lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]
models = [None] * len(lambdas)

for i in range(len(lambdas)):
    lambda_ = lambdas[i]
    models[i] = Sequential(
        [
            Dense(120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
            Dense(40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
            Dense(classes, activation='linear')
        ]
    )
    models[i].compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=tf.keras.optimizers.Adam(0.01),
    )
    models[i].fit(X_train, y_train, epochs=1000)
    print(f"å®Œæˆ lambda = {lambda_}")
```

```python
plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv)
```

éšç€æ­£åˆ™åŒ–çš„å¢åŠ ï¼Œæ¨¡å‹åœ¨è®­ç»ƒå’Œäº¤å‰éªŒè¯æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶‹äºæ”¶æ•›ã€‚å¯¹äºè¿™ä¸ªæ•°æ®é›†å’Œæ¨¡å‹ï¼ŒÎ» > 0.01 ä¼¼ä¹æ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚

<a name="7.1"></a>

### 7.1 æµ‹è¯•

è®©æˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šå°è¯•æˆ‘ä»¬ä¼˜åŒ–çš„æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸â€œç†æƒ³â€æ€§èƒ½è¿›è¡Œæ¯”è¾ƒã€‚

```python
plt_compare(X_test, y_test, classes, model_predict_s, model_predict_r, centers)
```

![æ•°æ®å›¾10](/static/images/MLc2/output2w3_10.png)

æˆ‘ä»¬çš„æµ‹è¯•é›†å¾ˆå°ï¼Œå¹¶ä¸”ä¼¼ä¹æœ‰è®¸å¤šç¦»ç¾¤ç‚¹ï¼Œå› æ­¤åˆ†ç±»è¯¯å·®å¾ˆé«˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½ä¸ç†æƒ³æ€§èƒ½ç›¸å½“ã€‚

---

<a name="8"></a>

## 8 - äº¤äº’å¼å¯è§†åŒ–æ¢ç´¢

ä¸ºäº†æ›´ç›´è§‚åœ°ç†è§£åå·®-æ–¹å·®æƒè¡¡å’Œæ­£åˆ™åŒ–æ•ˆæœï¼Œæˆ‘ä»¬å‡†å¤‡äº†ä¸‰ä¸ªäº¤äº’å¼å¯è§†åŒ–å·¥å…·ï¼š

### 8.1 åå·®-æ–¹å·®æƒè¡¡å¯è§†åŒ–

<IframeEmbed
  src="/static/html/bias_variance_tradeoff.html"
  minHeight={920}
  title="åå·®-æ–¹å·®æƒè¡¡å¯è§†åŒ–"
/>

è¿™ä¸ªå¯è§†åŒ–å±•ç¤ºäº†æ¨¡å‹å¤æ‚åº¦å¦‚ä½•å½±å“åå·®å’Œæ–¹å·®ï¼š

- è°ƒæ•´å¤šé¡¹å¼é˜¶æ•°è§‚å¯Ÿæ¬ æ‹Ÿåˆåˆ°è¿‡æ‹Ÿåˆçš„è½¬å˜
- ç†è§£è®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®çš„å˜åŒ–è§„å¾‹
- æ‰¾åˆ°æœ€ä¼˜æ¨¡å‹å¤æ‚åº¦çš„"ç”œèœœç‚¹"

### 8.2 æ­£åˆ™åŒ–æ•ˆæœåŠ¨ç”»

<IframeEmbed
  src="/static/html/regularization_animation.html"
  minHeight={860}
  title="æ­£åˆ™åŒ–æ•ˆæœåŠ¨ç”»"
/>

å®æ—¶è§‚å¯Ÿæ­£åˆ™åŒ–å‚æ•°Î»çš„å½±å“ï¼š

- Î»=0æ—¶çš„è¿‡æ‹Ÿåˆç°è±¡
- Î»å¢å¤§æ—¶æ¨¡å‹é€æ¸å¹³æ»‘
- Î»è¿‡å¤§å¯¼è‡´çš„æ¬ æ‹Ÿåˆ
- å­¦ä¹ æ›²çº¿çš„åŠ¨æ€å˜åŒ–

### 8.3 å­¦ä¹ æ›²çº¿3Då¯è§†åŒ–

<IframeEmbed src="/static/html/learning_curve_3d.html" minHeight={920} title="å­¦ä¹ æ›²çº¿3Då¯è§†åŒ–" />

é€šè¿‡3Dè§†è§’ç†è§£è®­ç»ƒé›†å¤§å°çš„å½±å“ï¼š

- å°æ•°æ®é›†æ—¶çš„é«˜æ–¹å·®é—®é¢˜
- æ•°æ®å¢åŠ å¦‚ä½•æ”¹å–„æ³›åŒ–èƒ½åŠ›
- è¯†åˆ«æ˜¯å¦éœ€è¦æ›´å¤šæ•°æ®

---

<a name="9"></a>

## 9 - æ·±å…¥ç†è§£æ¨¡å‹è¯Šæ–­

### 9.1 åå·®-æ–¹å·®åˆ†è§£çš„æ•°å­¦åŸç†

**æœŸæœ›æ³›åŒ–è¯¯å·®åˆ†è§£**ï¼š

$$
\mathbb{E}[(y - \hat{f}(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$

å…¶ä¸­ï¼š

- **åå·®ï¼ˆBiasï¼‰**ï¼šæ¨¡å‹é¢„æµ‹çš„æœŸæœ›å€¼ä¸çœŸå®å€¼çš„å·®è·
  $$\text{Bias} = \mathbb{E}[\hat{f}(x)] - f(x)$$
- **æ–¹å·®ï¼ˆVarianceï¼‰**ï¼šæ¨¡å‹é¢„æµ‹çš„æ³¢åŠ¨ç¨‹åº¦
  $$\text{Variance} = \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2]$$
- **ä¸å¯çº¦è¯¯å·®**ï¼šæ•°æ®æœ¬èº«çš„å™ªå£°

**å®é™…åº”ç”¨**ï¼š

```python
def bias_variance_decomposition(model, X_train, y_train, X_test, y_test, n_iterations=100):
    """
    é€šè¿‡Bootstrapé‡‡æ ·ä¼°è®¡åå·®å’Œæ–¹å·®
    """
    predictions = []

    for i in range(n_iterations):
        # Bootstrapé‡‡æ ·
        indices = np.random.choice(len(X_train), len(X_train), replace=True)
        X_boot = X_train[indices]
        y_boot = y_train[indices]

        # è®­ç»ƒæ¨¡å‹
        model_copy = clone(model)
        model_copy.fit(X_boot, y_boot)
        predictions.append(model_copy.predict(X_test))

    predictions = np.array(predictions)

    # è®¡ç®—åå·®å’Œæ–¹å·®
    mean_prediction = predictions.mean(axis=0)
    bias_squared = np.mean((mean_prediction - y_test) ** 2)
    variance = np.mean(predictions.var(axis=0))

    return bias_squared, variance

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.base import clone
bias_sq, var = bias_variance_decomposition(model, X_train, y_train, X_cv, y_cv)
print(f"åå·®Â²: {bias_sq:.4f}, æ–¹å·®: {var:.4f}")
```

### 9.2 æ­£åˆ™åŒ–çš„å‡ ä½•è§£é‡Š

**L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰**ï¼š

```text
# ç›®æ ‡å‡½æ•°
J(w) = MSE(w) + Î»||w||Â²

# ç­‰ä»·äºçº¦æŸä¼˜åŒ–
minimize MSE(w)
subject to ||w||Â² â‰¤ t
```

**å‡ ä½•æ„ä¹‰**ï¼šåœ¨æƒé‡ç©ºé—´ä¸­ï¼ŒL2æ­£åˆ™åŒ–å°†è§£çº¦æŸåœ¨ä¸€ä¸ªçƒä½“å†…ï¼Œé¼“åŠ±æƒé‡å‡åŒ€åˆ†å¸ƒã€‚

**L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰**ï¼š

```text
# ç›®æ ‡å‡½æ•°
J(w) = MSE(w) + Î»||w||â‚

# ç­‰ä»·äºçº¦æŸä¼˜åŒ–
minimize MSE(w)
subject to ||w||â‚ â‰¤ t
```

**å‡ ä½•æ„ä¹‰**ï¼šL1æ­£åˆ™åŒ–å°†è§£çº¦æŸåœ¨è±å½¢å†…ï¼Œå®¹æ˜“äº§ç”Ÿç¨€ç–è§£ï¼ˆéƒ¨åˆ†æƒé‡ä¸º0ï¼‰ï¼Œå®ç°ç‰¹å¾é€‰æ‹©ã€‚

**å¯¹æ¯”å®éªŒ**ï¼š

```python
from sklearn.linear_model import Ridge, Lasso

# Ridgeå›å½’
ridge = Ridge(alpha=0.1)
ridge.fit(X_train, y_train)
print("Ridgeæƒé‡:", ridge.coef_)

# Lassoå›å½’
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
print("Lassoæƒé‡:", lasso.coef_)
print("éé›¶æƒé‡æ•°é‡:", np.sum(lasso.coef_ != 0))
```

### 9.3 äº¤å‰éªŒè¯çš„é«˜çº§æŠ€å·§

**KæŠ˜äº¤å‰éªŒè¯**ï¼š

```python
from sklearn.model_selection import KFold, cross_val_score

# 5æŠ˜äº¤å‰éªŒè¯
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')

print(f"CV MSE: {-scores.mean():.4f} (+/- {scores.std():.4f})")
```

**åˆ†å±‚KæŠ˜ï¼ˆç”¨äºåˆ†ç±»ï¼‰**ï¼š

```python
from sklearn.model_selection import StratifiedKFold

# ä¿æŒç±»åˆ«æ¯”ä¾‹çš„5æŠ˜äº¤å‰éªŒè¯
skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skfold, scoring='accuracy')
```

**æ—¶é—´åºåˆ—äº¤å‰éªŒè¯**ï¼š

```python
from sklearn.model_selection import TimeSeriesSplit

# æ—¶é—´åºåˆ—ä¸“ç”¨äº¤å‰éªŒè¯ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
tscv = TimeSeriesSplit(n_splits=5)
for train_idx, test_idx in tscv.split(X):
    X_train_fold, X_test_fold = X[train_idx], X[test_idx]
    y_train_fold, y_test_fold = y[train_idx], y[test_idx]
    # è®­ç»ƒå’Œè¯„ä¼°
```

---

<a name="10"></a>

## 10 - å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜1ï¼šå¦‚ä½•åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´å¤šæ•°æ®ï¼Ÿ

**å­¦ä¹ æ›²çº¿è¯Šæ–­**ï¼š

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring='neg_mean_squared_error'
)

plt.plot(train_sizes, -train_scores.mean(axis=1), label='è®­ç»ƒè¯¯å·®')
plt.plot(train_sizes, -val_scores.mean(axis=1), label='éªŒè¯è¯¯å·®')
plt.xlabel('è®­ç»ƒé›†å¤§å°')
plt.ylabel('MSE')
plt.legend()
plt.show()
```

**åˆ¤æ–­æ ‡å‡†**ï¼š

- âœ… **éœ€è¦æ›´å¤šæ•°æ®**ï¼šè®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®éƒ½å¾ˆé«˜ï¼Œä¸”éšæ•°æ®å¢åŠ æŒç»­ä¸‹é™
- âŒ **ä¸éœ€è¦æ›´å¤šæ•°æ®**ï¼šéªŒè¯è¯¯å·®å·²è¶‹äºå¹³ç¨³ï¼Œå¢åŠ æ•°æ®æ— æ˜æ˜¾æ”¹å–„

### é—®é¢˜2ï¼šæ­£åˆ™åŒ–å‚æ•°Î»å¦‚ä½•é€‰æ‹©ï¼Ÿ

**ç½‘æ ¼æœç´¢**ï¼š

```python
from sklearn.model_selection import GridSearchCV

param_grid = {'alpha': np.logspace(-4, 4, 20)}
grid_search = GridSearchCV(Ridge(), param_grid, cv=5,
                           scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

print(f"æœ€ä¼˜Î»: {grid_search.best_params_['alpha']:.4f}")
print(f"æœ€ä¼˜CV MSE: {-grid_search.best_score_:.4f}")
```

**è´å¶æ–¯ä¼˜åŒ–**ï¼ˆæ›´é«˜æ•ˆï¼‰ï¼š

```python
from skopt import BayesSearchCV

bayes_search = BayesSearchCV(
    Ridge(),
    {'alpha': (1e-4, 1e4, 'log-uniform')},
    n_iter=32,
    cv=5,
    scoring='neg_mean_squared_error'
)
bayes_search.fit(X_train, y_train)
```

### é—®é¢˜3ï¼šç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆä¸¥é‡æ€ä¹ˆåŠï¼Ÿ

**ç»¼åˆè§£å†³æ–¹æ¡ˆ**ï¼š

```python
from tensorflow.keras.layers import Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

model = Sequential([
    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),  # æ‰¹å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒ
    Dropout(0.3),          # 30%ç¥ç»å…ƒéšæœºå¤±æ´»

    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    Dropout(0.2),

    Dense(10, activation='softmax')
])

# æ—©åœæ³•
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# å­¦ä¹ ç‡è¡°å‡
lr_schedule = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6
)

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train, y_train,
                    validation_data=(X_cv, y_cv),
                    epochs=100,
                    callbacks=[early_stop, lr_schedule])
```

### é—®é¢˜4ï¼šå¤šé¡¹å¼ç‰¹å¾å¯¼è‡´ç»´åº¦çˆ†ç‚¸

**ç‰¹å¾é€‰æ‹©**ï¼š

```python
from sklearn.feature_selection import SelectKBest, f_regression

# ç”Ÿæˆå¤šé¡¹å¼ç‰¹å¾
poly = PolynomialFeatures(degree=5)
X_poly = poly.fit_transform(X)

# é€‰æ‹©æœ€é‡è¦çš„Kä¸ªç‰¹å¾
selector = SelectKBest(f_regression, k=20)
X_selected = selector.fit_transform(X_poly, y)

print(f"åŸå§‹ç‰¹å¾æ•°: {X_poly.shape[1]}")
print(f"é€‰æ‹©åç‰¹å¾æ•°: {X_selected.shape[1]}")
```

**ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰é™ç»´**ï¼š

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
X_pca = pca.fit_transform(X_poly)

print(f"é™ç»´åç‰¹å¾æ•°: {X_pca.shape[1]}")
print(f"è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_.sum():.2%}")
```

---

<a name="11"></a>

## 11 - å®æˆ˜æŠ€å·§æ€»ç»“

### æ¨¡å‹è¯„ä¼°æ£€æŸ¥æ¸…å•

```python
# âœ… æ•°æ®åˆ†å‰²æ£€æŸ¥
assert len(X_train) + len(X_cv) + len(X_test) == len(X), "æ•°æ®åˆ†å‰²é”™è¯¯"
assert 0.6 <= len(X_train)/len(X) <= 0.8, "è®­ç»ƒé›†æ¯”ä¾‹å¼‚å¸¸"

# âœ… æ•°æ®æ³„éœ²æ£€æŸ¥
train_indices = set(range(len(X_train)))
cv_indices = set(range(len(X_train), len(X_train)+len(X_cv)))
assert train_indices.isdisjoint(cv_indices), "è®­ç»ƒé›†å’ŒéªŒè¯é›†æœ‰é‡å ï¼"

# âœ… ç‰¹å¾ç¼©æ”¾æ£€æŸ¥
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_cv_scaled = scaler.transform(X_cv)  # æ³¨æ„ï¼šåªç”¨transformï¼Œä¸ç”¨fit
assert np.abs(X_train_scaled.mean()) < 0.1, "ç‰¹å¾æœªæ­£ç¡®æ ‡å‡†åŒ–"

# âœ… è¿‡æ‹Ÿåˆæ£€æŸ¥
train_error = mean_squared_error(y_train, model.predict(X_train))
cv_error = mean_squared_error(y_cv, model.predict(X_cv))
if cv_error > 2 * train_error:
    print("âš ï¸ è­¦å‘Šï¼šæ¨¡å‹å¯èƒ½è¿‡æ‹Ÿåˆï¼")
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å‘é‡åŒ–è®¡ç®—**ï¼ˆé¿å…Pythonå¾ªç¯ï¼‰ï¼š

   ```python
   # âŒ æ…¢é€Ÿå¾ªç¯
   errors = []
   for i in range(len(X)):
       pred = model.predict(X[i:i+1])
       errors.append((y[i] - pred) ** 2)

   # âœ… å‘é‡åŒ–
   predictions = model.predict(X)
   errors = (y - predictions) ** 2
   ```

2. **æ‰¹é‡é¢„æµ‹**ï¼š

   ```python
   # ç¥ç»ç½‘ç»œæ‰¹é‡é¢„æµ‹æ›´å¿«
   predictions = model.predict(X_test, batch_size=128)
   ```

3. **ç¼“å­˜ä¸­é—´ç»“æœ**ï¼š

   ```python
   from functools import lru_cache

   @lru_cache(maxsize=128)
   def compute_polynomial_features(degree):
       poly = PolynomialFeatures(degree)
       return poly.fit_transform(X_train)
   ```

### æ‰©å±•å­¦ä¹ æ–¹å‘

- **é›†æˆå­¦ä¹ **ï¼šç»“åˆå¤šä¸ªæ¨¡å‹ï¼ˆBaggingã€Boostingã€Stackingï¼‰
- **è‡ªåŠ¨æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰**ï¼šä½¿ç”¨Auto-sklearnæˆ–TPOTè‡ªåŠ¨è°ƒå‚
- **æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–**ï¼šDropoutã€Batch Normalizationã€Layer Normalization
- **è¿ç§»å­¦ä¹ **ï¼šåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹åŠ é€Ÿè®­ç»ƒ

---

## æ€»ç»“

æ­å–œä½ å®Œæˆäº†è¿™ç¯‡å…³äºæœºå™¨å­¦ä¹ æ¨¡å‹è¯„ä¼°å’Œæ”¹è¿›çš„å®è·µåšå®¢ï¼é€šè¿‡è¿™æ¬¡å­¦ä¹ ï¼Œä½ å·²ç»æŒæ¡äº†ï¼š

- å¦‚ä½•å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•é›†ï¼Œä»¥åŒºåˆ†æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆã€‚
- å¦‚ä½•åˆ›å»ºè®­ç»ƒã€äº¤å‰éªŒè¯å’Œæµ‹è¯•é›†ï¼Œä»¥è®­ç»ƒæ¨¡å‹å‚æ•°ã€è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ã€æ­£åˆ™åŒ–ç­‰ï¼Œå¹¶è¯„ä¼°æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚
- å¦‚ä½•é€šè¿‡æ¯”è¾ƒè®­ç»ƒå’Œäº¤å‰éªŒè¯çš„æ€§èƒ½ï¼Œæ´å¯Ÿæ¨¡å‹çš„è¿‡æ‹Ÿåˆï¼ˆé«˜æ–¹å·®ï¼‰æˆ–æ¬ æ‹Ÿåˆï¼ˆé«˜åå·®ï¼‰å€¾å‘ã€‚

---

å¸Œæœ›è¿™ç¯‡æ–‡ç« ä¸ºä½ æä¾›äº†å®è´µçš„å·¥å…·å’Œè§è§£ï¼Œå¸®åŠ©ä½ åœ¨æœªæ¥çš„æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­å–å¾—æˆåŠŸï¼å¦‚æœä½ æƒ³è¿›ä¸€æ­¥æ¢ç´¢ï¼Œå¯ä»¥å°è¯•è°ƒæ•´æ¨¡å‹çš„è¶…å‚æ•°æˆ–ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†è¿›è¡Œå®éªŒã€‚

å®Œæ•´ä»£ç å·²å¼€æºåœ¨[GitHubä»“åº“](https://github.com/kkkano/machine-learning-notebook/blob/main/C2/W3_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E6%94%B9%E8%BF%9B.ipynb)

**æœ¬ç¯‡æ–‡ç« çš„éƒ¨åˆ†å†…å®¹å’Œæ€æƒ³å‚è€ƒäº† [å´æ©è¾¾ (Andrew Ng)](https://www.andrewng.org/) åœ¨ [Coursera æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://www.coursera.org/learn/machine-learning) ä¸­çš„è®²è§£ï¼Œæ„Ÿè°¢ä»–å¯¹æœºå™¨å­¦ä¹ é¢†åŸŸçš„å“è¶Šè´¡çŒ®ã€‚**
