---
title: 'é€»è¾‘å›å½’ï¼šä»ç†è®ºåˆ°å®è·µ'
date: '2025-03-25'
tags: ['Machine Learning', 'Logistic Regression', 'Python']
draft: false
summary: 'æ¬¢è¿æ¥åˆ°è¿™åœºé€»è¾‘å›å½’çš„å­¦ä¹ ä¹‹æ—…ï¼æˆ‘ä»¬å°†ä»å¤´æ„å»ºé€»è¾‘å›å½’æ¨¡å‹ï¼Œé¢„æµ‹å­¦ç”Ÿå½•å–å’ŒèŠ¯ç‰‡è´¨é‡ã€‚ç»“åˆç†è®ºã€Pythonä»£ç å’Œå¯è§†åŒ–ï¼Œè®©ä½ è½»æ¾æŒæ¡è¿™ä¸€æ ¸å¿ƒç®—æ³•ã€‚'
authors: ['Allen']
---

æ¬¢è¿æ¥åˆ°è¿™åœºé€»è¾‘å›å½’çš„å­¦ä¹ ä¹‹æ—…ï¼æ— è®ºä½ æ˜¯æœºå™¨å­¦ä¹ æ–°æ‰‹è¿˜æ˜¯å¸Œæœ›å·©å›ºçŸ¥è¯†ï¼Œè¿™ç¯‡åšå®¢å°†å¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºé€»è¾‘å›å½’æ¨¡å‹ã€‚æˆ‘ä»¬å°†é€šè¿‡é¢„æµ‹å­¦ç”Ÿå½•å–å’ŒèŠ¯ç‰‡è´¨é‡çš„å®é™…é—®é¢˜ï¼Œç»“åˆç†è®ºã€Pythonä»£ç å’Œå¯è§†åŒ–ï¼Œè®©å­¦ä¹ å˜å¾—æ—¢æœ‰è¶£åˆå®ç”¨ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

## ä½ å°†å­¦åˆ°ä»€ä¹ˆ

- é€»è¾‘å›å½’åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­çš„å·¥ä½œåŸç†ã€‚
- å®ç°æ ¸å¿ƒå‡½æ•°ï¼Œå¦‚ Sigmoidã€æˆæœ¬å‡½æ•°å’Œæ¢¯åº¦ä¸‹é™ã€‚
- æ•°æ®å¯è§†åŒ–å’Œå†³ç­–è¾¹ç•Œã€‚
- ä½¿ç”¨æ­£åˆ™åŒ–å¤„ç†å¤æ‚æ•°æ®é›†ã€‚

## ç›®å½•

- [1 - å‡†å¤‡å·¥å…·åŒ…](#1)
- [2 - é€»è¾‘å›å½’åŸºç¡€](#2)
  - [2.1 é—®é¢˜èƒŒæ™¯](#2.1)
  - [2.2 æ•°æ®æ¢ç´¢](#2.2)
  - [2.3 Sigmoid å‡½æ•°](#2.3)
  - [2.4 æˆæœ¬å‡½æ•°](#2.4)
  - [2.5 æ¢¯åº¦ä¸‹é™](#2.5)
  - [2.6 è®­ç»ƒæ¨¡å‹](#2.6)
  - [2.7 å†³ç­–è¾¹ç•Œå¯è§†åŒ–](#2.7)
  - [2.8 è¯„ä¼°æ¨¡å‹](#2.8)
- [3 - æ­£åˆ™åŒ–é€»è¾‘å›å½’](#3)
  - [3.1 æ–°é—®é¢˜](#3.1)
  - [3.2 æ•°æ®æ´å¯Ÿ](#3.2)
  - [3.3 ç‰¹å¾æ˜ å°„](#3.3)
  - [3.4 æ­£åˆ™åŒ–æˆæœ¬å‡½æ•°](#3.4)
  - [3.5 æ­£åˆ™åŒ–æ¢¯åº¦](#3.5)
  - [3.6 å¸¦æ­£åˆ™åŒ–çš„è®­ç»ƒ](#3.6)
  - [3.7 å¤æ‚è¾¹ç•Œçš„å¯è§†åŒ–](#3.7)
  - [3.8 è¯„ä¼°æ­£åˆ™åŒ–æ¨¡å‹](#3.8)
- [æ€»ç»“](#wrap-up)

<a name="1"></a>
## 1 - å‡†å¤‡å·¥å…·åŒ…

åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€äº› Python å·¥å…·åŒ…ï¼Œå®ƒä»¬å°†å¸®åŠ©æˆ‘ä»¬å¤„ç†æ•°æ®ã€è¿›è¡Œè®¡ç®—å’Œå¯è§†åŒ–ç»“æœã€‚ä»¥ä¸‹æ˜¯ä½ éœ€è¦çš„å†…å®¹ï¼š

- **[NumPy](https://numpy.org/)**ï¼šç”¨äºå¿«é€Ÿæ•°ç»„æ“ä½œã€‚
- **[Matplotlib](https://matplotlib.org/)**ï¼šç”¨äºç»˜åˆ¶æ•°æ®å’Œç»“æœã€‚
- **`utils.py`**ï¼šä¸€ä¸ªè¾…åŠ©æ–‡ä»¶ï¼ŒåŒ…å« `load_data()` å’Œ `plot_data()` ç­‰å‡½æ•°ã€‚

è¿è¡Œä»¥ä¸‹ä»£ç æ¥è®¾ç½®ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
from utils import *
import copy
import math
%matplotlib inline
```

å‡†å¤‡å¥½è¿™äº›å·¥å…·åï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹æ¢ç´¢é€»è¾‘å›å½’äº†ï¼

---

<a name="2"></a>
## 2 - é€»è¾‘å›å½’åŸºç¡€

è®©æˆ‘ä»¬ä»ä¸€ä¸ªç»å…¸é—®é¢˜å¼€å§‹ï¼Œé€æ­¥æ„å»ºé€»è¾‘å›å½’æ¨¡å‹ã€‚

<a name="2.1"></a>
### 2.1 é—®é¢˜èƒŒæ™¯

å‡è®¾ä½ æ˜¯ä¸€åå¤§å­¦ç®¡ç†å‘˜ï¼Œä½ å¸Œæœ›æ ¹æ®ç”³è¯·è€…çš„ä¸¤æ¬¡è€ƒè¯•æˆç»©é¢„æµ‹ä»–ä»¬æ˜¯å¦ä¼šè¢«å½•å–ã€‚ä½ æœ‰å†å²æ•°æ®ï¼šè€ƒè¯•æˆç»©å’Œå½•å–ç»“æœï¼ˆ1 è¡¨ç¤ºå½•å–ï¼Œ0 è¡¨ç¤ºæœªå½•å–ï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œä¼°è®¡å½•å–çš„æ¦‚ç‡ã€‚

<a name="2.2"></a>
### 2.2 æ•°æ®æ¢ç´¢

é¦–å…ˆï¼ŒåŠ è½½æ•°æ®é›†ï¼š

```python
X_train, y_train = load_data("data/ex2data1.txt")
```

#### æŸ¥çœ‹æ•°æ®

è®©æˆ‘ä»¬çœ‹çœ‹æ•°æ®é•¿ä»€ä¹ˆæ ·ï¼š

```python
print("X_trainçš„å‰äº”è¡Œ:\n", X_train[:5])
print("y_trainçš„å‰äº”ä¸ªå€¼:\n", y_train[:5])
```

**è¾“å‡ºï¼š**

```
X_trainçš„å‰äº”è¡Œ:
 [[34.62365962 78.02469282]
 [30.28671077 43.89499752]
 [35.84740877 72.90219803]
 [60.18259939 86.3085521]
 [79.03273605 75.34437644]]
y_trainçš„å‰äº”ä¸ªå€¼:
 [0. 0. 0. 1. 1.]
```

- `X_train`ï¼šä¸€ä¸ª (100, 2) æ•°ç»„ï¼ŒåŒ…å«è€ƒè¯•æˆç»©ã€‚
- `y_train`ï¼šä¸€ä¸ª (100,) æ•°ç»„ï¼ŒåŒ…å«å½•å–æ ‡ç­¾ã€‚

#### å¯è§†åŒ–æ•°æ®

æ•£ç‚¹å›¾å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£æ•°æ®ï¼š

```python
plot_data(X_train, y_train, pos_label="å½•å–", neg_label="æœªå½•å–")
plt.ylabel('è€ƒè¯•2æˆç»©')
plt.xlabel('è€ƒè¯•1æˆç»©')
plt.legend(loc="upper right")
plt.show()
```

![æ•£ç‚¹å›¾](/static/images/MLc1/output_13_0.png)

å½•å–çš„å­¦ç”Ÿï¼ˆ1ï¼‰å¾€å¾€æœ‰æ›´é«˜çš„æˆç»©ï¼Œè€Œæœªå½•å–çš„å­¦ç”Ÿï¼ˆ0ï¼‰åˆ™é›†ä¸­åœ¨è¾ƒä½çš„åŒºåŸŸã€‚æˆ‘ä»¬çš„æ¨¡å‹å°†ç»˜åˆ¶ä¸€æ¡çº¿â€”â€”æˆ–è¾¹ç•Œâ€”â€”æ¥åˆ†éš”è¿™äº›ç¾¤ä½“ã€‚

<a name="2.3"></a>
### 2.3 Sigmoid å‡½æ•°

é€»è¾‘å›å½’ä½¿ç”¨ Sigmoid å‡½æ•°å°†é¢„æµ‹å€¼æ˜ å°„åˆ° 0 åˆ° 1 ä¹‹é—´:

$$g(z) = \frac{1}{1 + e^{-z}}$$

å…¶ä¸­ï¼Œ`z = w * x + b`ï¼Œ`w` æ˜¯æƒé‡å‘é‡ï¼Œ`b` æ˜¯åç½®ã€‚è®©æˆ‘ä»¬ç¼–å†™å®ƒï¼š

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

æµ‹è¯•ä¸€ä¸‹ï¼š

```python
print("sigmoid(0) =", sigmoid(0))
print("sigmoid([-1, 0, 1, 2]) =", sigmoid(np.array([-1, 0, 1, 2])))
```

**è¾“å‡ºï¼š**

```
sigmoid(0) = 0.5
sigmoid([-1, 0, 1, 2]) = [0.26894142 0.5        0.73105858 0.88079708]
```

Sigmoid å‡½æ•°å°†ä»»ä½•å®æ•°æ˜ å°„åˆ° (0, 1)â€”â€”éå¸¸é€‚åˆè¡¨ç¤ºæ¦‚ç‡ï¼

<a name="2.4"></a>
### 2.4 æˆæœ¬å‡½æ•°

ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæˆæœ¬å‡½æ•°æ¥è¡¡é‡é¢„æµ‹è¯¯å·®ï¼š

$$
J(w, b) = - \frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(f(x^{(i)})) + (1 - y^{(i)}) \log(1 - f(x^{(i)})) \right]
$$

å…¶ä¸­
`f(x^(i)) = sigmoid(w * x^(i) + b)`ã€‚å®ç°å®ƒï¼š

```python
def compute_cost(X, y, w, b):
    m = X.shape[0]
    cost = 0.0
    for i in range(m):
        z = np.dot(X[i], w) + b
        f_wb = sigmoid(z)
        cost += -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)
    return cost / m
```

ä½¿ç”¨åˆå§‹å‚æ•°æµ‹è¯•ï¼š

```python
initial_w = np.zeros(2)
initial_b = 0.0
cost = compute_cost(X_train, y_train, initial_w, initial_b)
print(f"åˆå§‹æˆæœ¬: {cost:.3f}")
```

**è¾“å‡ºï¼š**

```
åˆå§‹æˆæœ¬: 0.693
```

è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¼˜åŒ–çš„èµ·ç‚¹ã€‚

<a name="2.5"></a>
### 2.5 æ¢¯åº¦ä¸‹é™

æ¢¯åº¦ä¸‹é™é€šè¿‡è°ƒæ•´ `w` å’Œ `b` æ¥å‡å°æˆæœ¬ã€‚æ¢¯åº¦ä¸ºï¼š

$$
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} \left( f(x^{(i)}) - y^{(i)} \right) x_j^{(i)}
$$

$$
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} \left( f(x^{(i)}) - y^{(i)} \right)
$$

ç¼–ç å®ç°ï¼š

```python
def compute_gradient(X, y, w, b):
    m, n = X.shape
    dj_dw = np.zeros(n)
    dj_db = 0.0
    for i in range(m):
        z = np.dot(X[i], w) + b
        f_wb = sigmoid(z)
        err = f_wb - y[i]
        dj_db += err
        dj_dw += err * X[i]
    dj_dw /= m
    dj_db /= m
    return dj_dw, dj_db
```

æµ‹è¯•ï¼š

```python
dj_dw, dj_db = compute_gradient(X_train, y_train, initial_w, initial_b)
print(f"åˆå§‹æ¢¯åº¦: w = {dj_dw}, b = {dj_db}")
```

**è¾“å‡ºï¼š**

```
åˆå§‹æ¢¯åº¦: w = [-12.00921659 -11.26284221], b = -0.1
```

è¿™äº›æ¢¯åº¦å°†æŒ‡å¯¼æˆ‘ä»¬çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚

<a name="2.6"></a>
### 2.6 è®­ç»ƒæ¨¡å‹

ä»¥ä¸‹æ˜¯æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼š

$$
w_j = w_j - \alpha \frac{\partial J(w, b)}{\partial w_j}
$$

$$
b = b - \alpha \frac{\partial J(w, b)}{\partial b}
$$

ç¼–ç å®ç°ï¼š

```python
def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):
    J_history = []
    w = w_in
    b = b_in
    for i in range(num_iters):
        dj_dw, dj_db = gradient_function(X, y, w, b)
        w -= alpha * dj_dw
        b -= alpha * dj_db
        cost = cost_function(X, y, w, b)
        J_history.append(cost)
        if i % 1000 == 0:
            print(f"è¿­ä»£ {i:4}: æˆæœ¬ {cost:8.2f}")
    return w, b, J_history
```

è¿è¡Œå®ƒï¼š

```python
np.random.seed(1)
initial_w = 0.01 * (np.random.rand(2) - 0.5)
initial_b = -8.0
w, b, J_history = gradient_descent(X_train, y_train, initial_w, initial_b,
                                   compute_cost, compute_gradient, 0.001, 10000)
print(f"è®­ç»ƒåçš„ w, b: {w}, {b}")
```

**è¾“å‡ºï¼š**

```
è¿­ä»£    0: æˆæœ¬     0.96
è¿­ä»£ 1000: æˆæœ¬     0.31
...
è®­ç»ƒåçš„ w, b: [ 0.07125356 -0.06482832], -8.188577238533804
```

æˆæœ¬é€æ¸å‡å°ï¼Œè¡¨æ˜æ¨¡å‹æ­£åœ¨å­¦ä¹ ï¼

<a name="2.7"></a>
### 2.7 å†³ç­–è¾¹ç•Œå¯è§†åŒ–

å¯è§†åŒ–å†³ç­–è¾¹ç•Œï¼š

```python
plot_decision_boundary(w, b, X_train, y_train)
plt.ylabel('è€ƒè¯•2æˆç»©')
plt.xlabel('è€ƒè¯•1æˆç»©')
plt.legend(loc="upper right")
plt.show()
```

è¿™æ¡çº¿å¾ˆå¥½åœ°åˆ†éš”äº†å½•å–å’Œæœªå½•å–çš„å­¦ç”Ÿã€‚

<a name="2.8"></a>
### 2.8 è¯„ä¼°æ¨¡å‹

å®šä¹‰é¢„æµ‹å‡½æ•°ï¼š

```python
def predict(X, w, b):
    m = X.shape[0]
    p = np.zeros(m)
    for i in range(m):
        z = np.dot(X[i], w) + b
        f_wb = sigmoid(z)
        p[i] = 1 if f_wb >= 0.5 else 0
    return p
```

è®¡ç®—å‡†ç¡®ç‡ï¼š

```python
p = predict(X_train, w, b)
print(f"è®­ç»ƒå‡†ç¡®ç‡: {np.mean(p == y_train) * 100:.2f}%")
```

**è¾“å‡ºï¼š**

```
è®­ç»ƒå‡†ç¡®ç‡: 92.00%
```

92% çš„å‡†ç¡®ç‡â€”â€”æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°ä¸é”™ï¼

---

<a name="3"></a>
## 3 - æ­£åˆ™åŒ–é€»è¾‘å›å½’

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¤„ç†ä¸€ä¸ªæ›´å¤æ‚çš„æ•°æ®é›†ï¼Œå…¶ä¸­ç›´çº¿æ— æ³•æœ‰æ•ˆåˆ†éš”æ•°æ®ã€‚

<a name="3.1"></a>
### 3.1 æ–°é—®é¢˜

å‡è®¾ä½ æ˜¯ä¸€åå·¥å‚ç»ç†ï¼Œé€šè¿‡ä¸¤æ¬¡æµ‹è¯•æ¥æ£€æµ‹èŠ¯ç‰‡ã€‚ä½ å¸Œæœ›æ ¹æ®æµ‹è¯•æˆç»©é¢„æµ‹èŠ¯ç‰‡æ˜¯å¦é€šè¿‡è´¨é‡æ£€æµ‹ï¼ˆ1 è¡¨ç¤ºé€šè¿‡ï¼Œ0 è¡¨ç¤ºæœªé€šè¿‡ï¼‰ã€‚

<a name="3.2"></a>
### 3.2 æ•°æ®æ´å¯Ÿ

åŠ è½½æ•°æ®ï¼š

```python
X_train, y_train = load_data("data/ex2data2.txt")
```

æŸ¥çœ‹æ•°æ®ï¼š

```python
print("X_train[:5]:\n", X_train[:5])
print("y_train[:5]:\n", y_train[:5])
```

**è¾“å‡ºï¼š**

```
X_train[:5]:
 [[ 0.051267  0.69956 ]
 [-0.092742  0.68494 ]
 [-0.21371   0.69225 ]
 [-0.375     0.50219 ]
 [-0.51325   0.46564 ]]
y_train[:5]:
 [1. 1. 1. 1. 1.]
```

å¯è§†åŒ–ï¼š

```python
plot_data(X_train, y_train, pos_label="é€šè¿‡", neg_label="æœªé€šè¿‡")
plt.ylabel('èŠ¯ç‰‡æµ‹è¯•2')
plt.xlabel('èŠ¯ç‰‡æµ‹è¯•1')
plt.legend(loc="upper right")
plt.show()
```

![æ•£ç‚¹å›¾](/static/images/MLc1/output_64_0.png)

æ•°æ®å‘ˆç°éçº¿æ€§æ¨¡å¼ï¼Œè¡¨æ˜æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´çµæ´»çš„æ¨¡å‹ã€‚

<a name="3.3"></a>
### 3.3 ç‰¹å¾æ˜ å°„

ä¸ºäº†æ•æ‰å¤æ‚æ€§ï¼Œæˆ‘ä»¬å°†ç‰¹å¾æ˜ å°„åˆ°æœ€é«˜ 6 æ¬¡çš„å¤šé¡¹å¼ï¼š

```python
X_mapped = map_feature(X_train[:, 0], X_train[:, 1])
print("åŸå§‹å½¢çŠ¶:", X_train.shape)
print("æ˜ å°„åå½¢çŠ¶:", X_mapped.shape)
```

**è¾“å‡ºï¼š**

```
åŸå§‹å½¢çŠ¶: (118, 2)
æ˜ å°„åå½¢çŠ¶: (118, 27)
```

ä» 2 ä¸ªç‰¹å¾æ‰©å±•åˆ° 27 ä¸ªç‰¹å¾â€”â€”è¿™å°†æœ‰åŠ©äºæ‹Ÿåˆæ•°æ®ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚

<a name="3.4"></a>
### 3.4 æ­£åˆ™åŒ–æˆæœ¬å‡½æ•°

æ·»åŠ æƒ©ç½šé¡¹ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼š

$$
J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(f(x^{(i)})) + (1 - y^{(i)}) \log(1 - f(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} w_j^2
$$

å®ç°å®ƒï¼š

```python
def compute_cost_reg(X, y, w, b, lambda_=1):
    m = X.shape[0]
    cost = compute_cost(X, y, w, b)
    reg_cost = (lambda_ / (2 * m)) * np.sum(w ** 2)
    return cost + reg_cost
```

<a name="3.5"></a>
### 3.5 æ­£åˆ™åŒ–æ¢¯åº¦

è°ƒæ•´æ¢¯åº¦ï¼š

$$
\frac{\partial J}{\partial w_j} = \frac{1}{m} \sum_{i=1}^{m} \left( f(x^{(i)}) - y^{(i)} \right) x_j^{(i)} + \frac{\lambda}{m} w_j
$$

ç¼–ç ï¼š

```python
def compute_gradient_reg(X, y, w, b, lambda_=1):
    dj_dw, dj_db = compute_gradient(X, y, w, b)
    dj_dw += (lambda_ / X.shape[0]) * w
    return dj_dw, dj_db
```

<a name="3.6"></a>
### 3.6 å¸¦æ­£åˆ™åŒ–çš„è®­ç»ƒ

è®­ç»ƒæ¨¡å‹ï¼š

```python
np.random.seed(1)
initial_w = np.random.rand(X_mapped.shape[1]) - 0.5
initial_b = 1.0
lambda_ = 0.01
w, b, J_history = gradient_descent(X_mapped, y_train, initial_w, initial_b,
                                   compute_cost_reg, compute_gradient_reg, 0.01, 10000)
```

**è¾“å‡ºï¼š**

```
è¿­ä»£    0: æˆæœ¬     0.72
è¿­ä»£ 1000: æˆæœ¬     0.59
...
è¿­ä»£ 9999: æˆæœ¬     0.45
```

æˆæœ¬ç¨³æ­¥ä¸‹é™ã€‚

<a name="3.7"></a>
### 3.7 å¤æ‚è¾¹ç•Œçš„å¯è§†åŒ–

ç»˜åˆ¶å†³ç­–è¾¹ç•Œï¼š

```python
plot_decision_boundary(w, b, X_mapped, y_train)
plt.ylabel('èŠ¯ç‰‡æµ‹è¯•2')
plt.xlabel('èŠ¯ç‰‡æµ‹è¯•1')
plt.legend(loc="upper right")
plt.show()
```

![å†³ç­–è¾¹ç•Œ](/static/images/MLc1/output_88_0.png)

å¼¯æ›²çš„è¾¹ç•Œå¾ˆå¥½åœ°æ‹Ÿåˆäº†æ•°æ®ï¼

<a name="3.8"></a>
### 3.8 è¯„ä¼°æ­£åˆ™åŒ–æ¨¡å‹

æ£€æŸ¥å‡†ç¡®ç‡ï¼š

```python
p = predict(X_mapped, w, b)
print(f"è®­ç»ƒå‡†ç¡®ç‡: {np.mean(p == y_train) * 100:.2f}%")
```

**è¾“å‡ºï¼š**

```
è®­ç»ƒå‡†ç¡®ç‡: 82.20%
```

å¯¹äºä¸€ä¸ªéçº¿æ€§é—®é¢˜ï¼Œ82% çš„å‡†ç¡®ç‡ç›¸å½“ä¸é”™ï¼

---

## 4 - äº¤äº’å¼å¯è§†åŒ–æ¢ç´¢

ä¸ºäº†æ›´ç›´è§‚åœ°ç†è§£é€»è¾‘å›å½’çš„å·¥ä½œåŸç†ï¼Œæµ®æµ®é…±ä¸ºä½ å‡†å¤‡äº†ä¸‰ä¸ªäº¤äº’å¼å¯è§†åŒ–å·¥å…·å–µï½ (à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§

### 4.1 Sigmoidå‡½æ•°å¯è§†åŒ–

Sigmoidå‡½æ•°æ˜¯é€»è¾‘å›å½’çš„æ ¸å¿ƒï¼Œå®ƒå°†ä»»æ„å®æ•°æ˜ å°„åˆ°(0,1)åŒºé—´ï¼š

<IframeEmbed
  src="/visualizations/sigmoid-function.html"
  minHeight={720}
  title="Sigmoidå‡½æ•°å¯è§†åŒ–"
/>

**è§‚å¯Ÿè¦ç‚¹ï¼š**

- ğŸ“ˆ Så½¢æ›²çº¿ï¼šè¾“å‡ºå€¼åœ¨0åˆ°1ä¹‹é—´
- ğŸ¯ ä¸­å¿ƒç‚¹ï¼šz=0æ—¶ï¼Œsigmoid(0)=0.5
- ğŸ“Š é¥±å’ŒåŒºï¼š|z|å¾ˆå¤§æ—¶ï¼Œå‡½æ•°è¶‹äºå¹³å¦

### 4.2 å†³ç­–è¾¹ç•Œäº¤äº’å¼è°ƒæ•´

è¿™ä¸ªå¯è§†åŒ–å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è°ƒæ•´å‚æ•°æ¥æ”¹å˜å†³ç­–è¾¹ç•Œï¼š

<IframeEmbed
  src="/visualizations/logistic-decision-boundary.html"
  minHeight={760}
  title="å†³ç­–è¾¹ç•Œäº¤äº’å¼è°ƒæ•´"
/>

**ä½¿ç”¨æç¤ºï¼š**

- æ‹–åŠ¨æ»‘å—è°ƒæ•´æƒé‡å‚æ•°
- è§‚å¯Ÿå†³ç­–è¾¹ç•Œå¦‚ä½•å˜åŒ–
- æŸ¥çœ‹åˆ†ç±»å‡†ç¡®ç‡çš„å®æ—¶æ›´æ–°

### 4.3 æ­£åˆ™åŒ–æ•ˆæœå¯¹æ¯”

è¿™ä¸ªå¯è§†åŒ–å±•ç¤ºäº†ä¸åŒæ­£åˆ™åŒ–å¼ºåº¦å¯¹æ¨¡å‹çš„å½±å“ï¼š

<IframeEmbed
  src="/visualizations/logistic-regularization.html"
  minHeight={740}
  title="æ­£åˆ™åŒ–æ•ˆæœå¯¹æ¯”"
/>

**å¯¹æ¯”è§‚å¯Ÿï¼š**

- ğŸ”´ æ— æ­£åˆ™åŒ–ï¼šå¯èƒ½è¿‡æ‹Ÿåˆ
- ğŸŸ¢ é€‚åº¦æ­£åˆ™åŒ–ï¼šå¹³è¡¡æ‹Ÿåˆ
- ğŸ”µ è¿‡åº¦æ­£åˆ™åŒ–ï¼šæ¬ æ‹Ÿåˆ

---

## 5 - æ·±å…¥ç†è§£é€»è¾‘å›å½’

### 5.1 ä¸ºä»€ä¹ˆä½¿ç”¨Sigmoidå‡½æ•°ï¼Ÿ

é€»è¾‘å›å½’éœ€è¦è¾“å‡ºæ¦‚ç‡å€¼ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰ï¼ŒSigmoidå‡½æ•°å®Œç¾æ»¡è¶³è¿™ä¸ªéœ€æ±‚ã€‚

#### Sigmoidçš„æ•°å­¦æ€§è´¨

```python
def sigmoid(z):
    """
    Sigmoidå‡½æ•°ï¼šg(z) = 1 / (1 + e^(-z))
    """
    return 1 / (1 + np.exp(-z))

# é‡è¦æ€§è´¨
z_values = np.array([-10, -5, 0, 5, 10])
sigmoid_values = sigmoid(z_values)

print("zå€¼:", z_values)
print("sigmoid(z):", sigmoid_values)
print("\næ€§è´¨éªŒè¯:")
print("sigmoid(0) =", sigmoid(0))  # 0.5
print("sigmoid(-z) + sigmoid(z) =", sigmoid(-5) + sigmoid(5))  # 1.0
```

**è¾“å‡ºï¼š**

```
zå€¼: [-10  -5   0   5  10]
sigmoid(z): [0.0000454  0.0066929  0.5  0.9933071  0.9999546]

æ€§è´¨éªŒè¯:
sigmoid(0) = 0.5
sigmoid(-z) + sigmoid(z) = 1.0
```

**å…³é”®æ€§è´¨ï¼š**

1. **å¯¹ç§°æ€§**ï¼šsigmoid(-z) = 1 - sigmoid(z)
2. **å•è°ƒæ€§**ï¼šzè¶Šå¤§ï¼Œsigmoid(z)è¶Šæ¥è¿‘1
3. **å¯å¯¼æ€§**ï¼šå¯¼æ•°ä¸º g'(z) = g(z)(1-g(z))

### 5.2 æˆæœ¬å‡½æ•°çš„ç›´è§‰ç†è§£

ä¸ºä»€ä¹ˆé€»è¾‘å›å½’ä½¿ç”¨å¯¹æ•°æŸå¤±è€Œä¸æ˜¯å‡æ–¹è¯¯å·®ï¼Ÿ

#### å¯¹æ•°æŸå¤±çš„ä¼˜åŠ¿

```python
def visualize_loss_functions():
    """
    å¯¹æ¯”ä¸åŒæŸå¤±å‡½æ•°
    """
    # çœŸå®æ ‡ç­¾ y=1 çš„æƒ…å†µ
    predictions = np.linspace(0.01, 0.99, 100)

    # å¯¹æ•°æŸå¤±ï¼š-log(h)
    log_loss = -np.log(predictions)

    # å‡æ–¹è¯¯å·®ï¼š(1-h)^2
    mse_loss = (1 - predictions) ** 2

    plt.figure(figsize=(12, 5))

    # å­å›¾1ï¼šy=1æ—¶çš„æŸå¤±
    plt.subplot(1, 2, 1)
    plt.plot(predictions, log_loss, 'b-', label='å¯¹æ•°æŸå¤±', linewidth=2)
    plt.plot(predictions, mse_loss, 'r--', label='å‡æ–¹è¯¯å·®', linewidth=2)
    plt.xlabel('é¢„æµ‹æ¦‚ç‡ h(x)')
    plt.ylabel('æŸå¤±å€¼')
    plt.title('å½“çœŸå®æ ‡ç­¾ y=1 æ—¶')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­å›¾2ï¼šy=0æ—¶çš„æŸå¤±
    plt.subplot(1, 2, 2)
    log_loss_0 = -np.log(1 - predictions)
    mse_loss_0 = predictions ** 2
    plt.plot(predictions, log_loss_0, 'b-', label='å¯¹æ•°æŸå¤±', linewidth=2)
    plt.plot(predictions, mse_loss_0, 'r--', label='å‡æ–¹è¯¯å·®', linewidth=2)
    plt.xlabel('é¢„æµ‹æ¦‚ç‡ h(x)')
    plt.ylabel('æŸå¤±å€¼')
    plt.title('å½“çœŸå®æ ‡ç­¾ y=0 æ—¶')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

visualize_loss_functions()
```

**å¯¹æ•°æŸå¤±çš„ä¼˜åŠ¿ï¼š**

- âœ… **å¼ºæƒ©ç½š**ï¼šé”™è¯¯é¢„æµ‹çš„æƒ©ç½šæ›´å¤§
- âœ… **å‡¸å‡½æ•°**ï¼šä¿è¯å…¨å±€æœ€ä¼˜è§£
- âœ… **æ¢¯åº¦å‹å¥½**ï¼šå¯¼æ•°å½¢å¼ç®€æ´

### 5.3 æ¢¯åº¦ä¸‹é™çš„æ”¶æ•›åˆ†æ

#### å­¦ä¹ ç‡çš„å½±å“

```python
def compare_learning_rates(X, y, alphas=[0.001, 0.01, 0.1, 1.0]):
    """
    å¯¹æ¯”ä¸åŒå­¦ä¹ ç‡çš„æ”¶æ•›æƒ…å†µ
    """
    plt.figure(figsize=(12, 8))

    for alpha in alphas:
        w = np.zeros(X.shape[1])
        b = 0
        costs = []

        for i in range(1000):
            z = np.dot(X, w) + b
            h = sigmoid(z)

            # è®¡ç®—æˆæœ¬
            cost = compute_cost(X, y, w, b)
            costs.append(cost)

            # æ¢¯åº¦ä¸‹é™
            dw = np.dot(X.T, (h - y)) / len(y)
            db = np.sum(h - y) / len(y)

            w = w - alpha * dw
            b = b - alpha * db

        plt.plot(costs, label=f'Î±={alpha}', linewidth=2)

    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('æˆæœ¬å‡½æ•°å€¼')
    plt.title('ä¸åŒå­¦ä¹ ç‡çš„æ”¶æ•›å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # å¯¹æ•°åˆ»åº¦æ›´æ¸…æ™°
    plt.show()

# è¿è¡Œå¯¹æ¯”
compare_learning_rates(X_train, y_train)
```

**è§‚å¯Ÿç»“æœï¼š**

- ğŸŒ **Î±=0.001**ï¼šæ”¶æ•›æ…¢ä½†ç¨³å®š
- âœ… **Î±=0.01**ï¼šæ”¶æ•›é€Ÿåº¦é€‚ä¸­
- âš¡ **Î±=0.1**ï¼šå¿«é€Ÿæ”¶æ•›
- âŒ **Î±=1.0**ï¼šå¯èƒ½éœ‡è¡æˆ–å‘æ•£

### 5.4 æ­£åˆ™åŒ–çš„æ•°å­¦åŸç†

æ­£åˆ™åŒ–é€šè¿‡åœ¨æˆæœ¬å‡½æ•°ä¸­æ·»åŠ æƒ©ç½šé¡¹æ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

#### L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰

```python
def regularized_cost(X, y, w, b, lambda_):
    """
    å¸¦L2æ­£åˆ™åŒ–çš„æˆæœ¬å‡½æ•°

    J(w,b) = åŸå§‹æˆæœ¬ + (Î»/2m) * Î£wÂ²
    """
    m = len(y)

    # åŸå§‹æˆæœ¬
    z = np.dot(X, w) + b
    h = sigmoid(z)
    cost = -np.mean(y * np.log(h) + (1-y) * np.log(1-h))

    # æ­£åˆ™åŒ–é¡¹ï¼ˆä¸åŒ…æ‹¬åç½®bï¼‰
    reg_cost = (lambda_ / (2 * m)) * np.sum(w ** 2)

    return cost + reg_cost
```

#### æ­£åˆ™åŒ–å¼ºåº¦çš„é€‰æ‹©

```python
def find_best_lambda(X_train, y_train, X_val, y_val):
    """
    é€šè¿‡éªŒè¯é›†é€‰æ‹©æœ€ä½³æ­£åˆ™åŒ–å‚æ•°
    """
    lambdas = [0, 0.001, 0.01, 0.1, 1, 10, 100]
    train_errors = []
    val_errors = []

    for lambda_ in lambdas:
        # è®­ç»ƒæ¨¡å‹
        w, b = train_with_regularization(X_train, y_train, lambda_)

        # è®¡ç®—è¯¯å·®
        train_error = compute_cost(X_train, y_train, w, b)
        val_error = compute_cost(X_val, y_val, w, b)

        train_errors.append(train_error)
        val_errors.append(val_error)

    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(lambdas, train_errors, 'b-o', label='è®­ç»ƒè¯¯å·®', linewidth=2)
    plt.plot(lambdas, val_errors, 'r-o', label='éªŒè¯è¯¯å·®', linewidth=2)
    plt.xlabel('æ­£åˆ™åŒ–å‚æ•° Î»')
    plt.ylabel('æˆæœ¬å‡½æ•°å€¼')
    plt.title('æ­£åˆ™åŒ–å‚æ•°é€‰æ‹©')
    plt.xscale('log')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    # é€‰æ‹©éªŒè¯è¯¯å·®æœ€å°çš„Î»
    best_lambda = lambdas[np.argmin(val_errors)]
    print(f"æœ€ä½³æ­£åˆ™åŒ–å‚æ•°: Î» = {best_lambda}")

    return best_lambda
```

---

## 6 - å®æˆ˜æŠ€å·§ä¸å¸¸è§é—®é¢˜

### 6.1 ç‰¹å¾å·¥ç¨‹æŠ€å·§

#### ç‰¹å¾ç¼©æ”¾çš„é‡è¦æ€§

```python
def feature_scaling_comparison(X, y):
    """
    å¯¹æ¯”ç‰¹å¾ç¼©æ”¾å‰åçš„æ”¶æ•›é€Ÿåº¦
    """
    from sklearn.preprocessing import StandardScaler

    # æœªç¼©æ”¾çš„è®­ç»ƒ
    w1, b1, costs1 = train_logistic_regression(X, y, alpha=0.01, iterations=1000)

    # ç¼©æ”¾åçš„è®­ç»ƒ
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    w2, b2, costs2 = train_logistic_regression(X_scaled, y, alpha=0.01, iterations=1000)

    # å¯¹æ¯”æ”¶æ•›æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(costs1, label='æœªç¼©æ”¾', linewidth=2)
    plt.plot(costs2, label='å·²ç¼©æ”¾', linewidth=2)
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('æˆæœ¬å‡½æ•°å€¼')
    plt.title('ç‰¹å¾ç¼©æ”¾å¯¹æ”¶æ•›é€Ÿåº¦çš„å½±å“')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
```

**ç‰¹å¾ç¼©æ”¾æ–¹æ³•ï¼š**

1. **æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰**

```python
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)
```

2. **å½’ä¸€åŒ–ï¼ˆMin-Maxï¼‰**

```python
X_normalized = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
```

#### å¤šé¡¹å¼ç‰¹å¾åˆ›å»º

```python
def create_polynomial_features(X, degree=2):
    """
    åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾

    ä¾‹å¦‚ï¼š[x1, x2] -> [x1, x2, x1Â², x1*x2, x2Â²]
    """
    from sklearn.preprocessing import PolynomialFeatures

    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(X)

    print(f"åŸå§‹ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"å¤šé¡¹å¼ç‰¹å¾æ•°: {X_poly.shape[1]}")
    print(f"ç‰¹å¾åç§°: {poly.get_feature_names_out()}")

    return X_poly
```

### 6.2 æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

å‡†ç¡®ç‡ä¸æ˜¯å”¯ä¸€çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ã€‚

#### æ··æ·†çŸ©é˜µ

```python
def plot_confusion_matrix(y_true, y_pred):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    """
    from sklearn.metrics import confusion_matrix
    import seaborn as sns

    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['é¢„æµ‹è´Ÿç±»', 'é¢„æµ‹æ­£ç±»'],
                yticklabels=['å®é™…è´Ÿç±»', 'å®é™…æ­£ç±»'])
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('å®é™…æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.show()

    # è®¡ç®—æŒ‡æ ‡
    tn, fp, fn, tp = cm.ravel()

    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1_score = 2 * (precision * recall) / (precision + recall)

    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"å¬å›ç‡ (Recall): {recall:.4f}")
    print(f"F1åˆ†æ•°: {f1_score:.4f}")
```

#### ROCæ›²çº¿å’ŒAUC

```python
def plot_roc_curve(X, y, w, b):
    """
    ç»˜åˆ¶ROCæ›²çº¿
    """
    from sklearn.metrics import roc_curve, auc

    # è·å–é¢„æµ‹æ¦‚ç‡
    z = np.dot(X, w) + b
    y_prob = sigmoid(z)

    # è®¡ç®—ROCæ›²çº¿
    fpr, tpr, thresholds = roc_curve(y, y_prob)
    roc_auc = auc(fpr, tpr)

    # ç»˜åˆ¶
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, 'b-', linewidth=2,
             label=f'ROCæ›²çº¿ (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='éšæœºçŒœæµ‹')
    plt.xlabel('å‡æ­£ç‡ (FPR)')
    plt.ylabel('çœŸæ­£ç‡ (TPR)')
    plt.title('ROCæ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    return roc_auc
```

### 6.3 ç±»åˆ«ä¸å¹³è¡¡å¤„ç†

å½“æ­£è´Ÿæ ·æœ¬æ•°é‡å·®å¼‚å¾ˆå¤§æ—¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ã€‚

#### æ–¹æ³•1ï¼šè°ƒæ•´ç±»åˆ«æƒé‡

```python
def weighted_logistic_regression(X, y, pos_weight=1.0):
    """
    å¸¦ç±»åˆ«æƒé‡çš„é€»è¾‘å›å½’

    pos_weight: æ­£ç±»æ ·æœ¬çš„æƒé‡å€æ•°
    """
    def weighted_cost(w, b):
        z = np.dot(X, w) + b
        h = sigmoid(z)

        # ä¸ºæ­£ç±»æ ·æœ¬å¢åŠ æƒé‡
        weights = np.where(y == 1, pos_weight, 1.0)
        cost = -np.mean(weights * (y * np.log(h) + (1-y) * np.log(1-h)))

        return cost

    # è®­ç»ƒè¿‡ç¨‹ç±»ä¼¼ï¼Œä½†ä½¿ç”¨åŠ æƒæˆæœ¬
    # ...
```

#### æ–¹æ³•2ï¼šé‡é‡‡æ ·

```python
def balance_dataset(X, y, method='oversample'):
    """
    å¹³è¡¡æ•°æ®é›†

    method: 'oversample' æˆ– 'undersample'
    """
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler

    if method == 'oversample':
        # SMOTEè¿‡é‡‡æ ·
        smote = SMOTE(random_state=42)
        X_balanced, y_balanced = smote.fit_resample(X, y)
    else:
        # æ¬ é‡‡æ ·
        rus = RandomUnderSampler(random_state=42)
        X_balanced, y_balanced = rus.fit_resample(X, y)

    print(f"åŸå§‹æ•°æ®: æ­£ç±»={np.sum(y==1)}, è´Ÿç±»={np.sum(y==0)}")
    print(f"å¹³è¡¡å: æ­£ç±»={np.sum(y_balanced==1)}, è´Ÿç±»={np.sum(y_balanced==0)}")

    return X_balanced, y_balanced
```

### 6.4 è°ƒè¯•æ£€æŸ¥æ¸…å•

é‡åˆ°é—®é¢˜æ—¶ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºæ£€æŸ¥ï¼š

- [ ] **æ•°æ®æ£€æŸ¥**
  - æ ‡ç­¾æ˜¯å¦ä¸º0/1ï¼Ÿ
  - æ˜¯å¦æœ‰ç¼ºå¤±å€¼ï¼Ÿ
  - ç‰¹å¾èŒƒå›´æ˜¯å¦åˆç†ï¼Ÿ

- [ ] **Sigmoidå‡½æ•°æ£€æŸ¥**
  - æ˜¯å¦å¤„ç†äº†æ•°å€¼æº¢å‡ºï¼Ÿ
  - è¾“å‡ºæ˜¯å¦åœ¨(0,1)èŒƒå›´å†…ï¼Ÿ

- [ ] **æˆæœ¬å‡½æ•°æ£€æŸ¥**
  - åˆå§‹æˆæœ¬æ˜¯å¦åˆç†ï¼ˆçº¦0.693ï¼‰ï¼Ÿ
  - æˆæœ¬æ˜¯å¦å•è°ƒé€’å‡ï¼Ÿ
  - æ˜¯å¦å‡ºç°NaNæˆ–Infï¼Ÿ

- [ ] **æ¢¯åº¦æ£€æŸ¥**
  - ä½¿ç”¨æ•°å€¼æ¢¯åº¦éªŒè¯
  - æ¢¯åº¦é‡çº§æ˜¯å¦åˆç†ï¼Ÿ

- [ ] **å­¦ä¹ ç‡è°ƒä¼˜**
  - ä»å°å€¼å¼€å§‹ï¼ˆ0.001ï¼‰
  - è§‚å¯Ÿæˆæœ¬æ›²çº¿
  - é¿å…éœ‡è¡æˆ–å‘æ•£

### 6.5 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### å‘é‡åŒ–å®ç°

```python
# âŒ æ…¢é€Ÿå¾ªç¯ç‰ˆæœ¬
def predict_slow(X, w, b):
    predictions = []
    for i in range(len(X)):
        z = np.dot(X[i], w) + b
        predictions.append(sigmoid(z))
    return np.array(predictions)

# âœ… å¿«é€Ÿå‘é‡åŒ–ç‰ˆæœ¬
def predict_fast(X, w, b):
    z = np.dot(X, w) + b
    return sigmoid(z)

# æ€§èƒ½å¯¹æ¯”
import time

start = time.time()
pred_slow = predict_slow(X_train, w, b)
time_slow = time.time() - start

start = time.time()
pred_fast = predict_fast(X_train, w, b)
time_fast = time.time() - start

print(f"å¾ªç¯ç‰ˆæœ¬: {time_slow*1000:.2f} ms")
print(f"å‘é‡åŒ–ç‰ˆæœ¬: {time_fast*1000:.2f} ms")
print(f"åŠ é€Ÿæ¯”: {time_slow/time_fast:.1f}x")
```

---

## 7 - æ‰©å±•å­¦ä¹ æ–¹å‘

æŒæ¡äº†é€»è¾‘å›å½’åï¼Œä½ å¯ä»¥ç»§ç»­æ¢ç´¢ï¼š

### 7.1 å¤šåˆ†ç±»é—®é¢˜

**One-vs-Restç­–ç•¥ï¼š**

```python
def one_vs_rest_classifier(X, y, num_classes):
    """
    ä¸€å¯¹å¤šåˆ†ç±»å™¨
    """
    models = []

    for c in range(num_classes):
        # å°†ç±»åˆ«cè§†ä¸ºæ­£ç±»ï¼Œå…¶ä»–ä¸ºè´Ÿç±»
        y_binary = (y == c).astype(int)

        # è®­ç»ƒäºŒåˆ†ç±»å™¨
        w, b = train_logistic_regression(X, y_binary)
        models.append((w, b))

    return models

def predict_multiclass(X, models):
    """
    å¤šåˆ†ç±»é¢„æµ‹
    """
    probabilities = []

    for w, b in models:
        z = np.dot(X, w) + b
        prob = sigmoid(z)
        probabilities.append(prob)

    # é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
    probabilities = np.array(probabilities).T
    predictions = np.argmax(probabilities, axis=1)

    return predictions
```

### 7.2 Softmaxå›å½’

å¤šåˆ†ç±»çš„å¦ä¸€ç§æ–¹æ³•ï¼š

```python
def softmax(z):
    """
    Softmaxå‡½æ•°ï¼šå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    """
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # æ•°å€¼ç¨³å®š
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)
```

### 7.3 é«˜çº§ä¼˜åŒ–ç®—æ³•

é™¤äº†æ¢¯åº¦ä¸‹é™ï¼Œè¿˜æœ‰æ›´é«˜æ•ˆçš„ä¼˜åŒ–å™¨ï¼š

- **Adam**ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡
- **RMSprop**ï¼šé€‚åˆéå¹³ç¨³ç›®æ ‡
- **L-BFGS**ï¼šäºŒé˜¶ä¼˜åŒ–æ–¹æ³•

### 7.4 æ·±åº¦å­¦ä¹ çš„æ¡¥æ¢

é€»è¾‘å›å½’æ˜¯ç¥ç»ç½‘ç»œçš„åŸºç¡€ï¼š

```
é€»è¾‘å›å½’ â†’ å•å±‚ç¥ç»ç½‘ç»œ â†’ å¤šå±‚ç¥ç»ç½‘ç»œ â†’ æ·±åº¦å­¦ä¹ 
```

---

<a name="wrap-up"></a>
## æ€»ç»“

æ­å–œä½ å®Œæˆäº†è¿™åœºé€»è¾‘å›å½’çš„å®è·µä¹‹æ—…ï¼ä½ å·²ç»å­¦ä¼šäº†ï¼š

- ä½¿ç”¨çœŸå®æ•°æ®é¢„æµ‹äºŒåˆ†ç±»ç»“æœã€‚
- ç¼–ç æ ¸å¿ƒå‡½æ•°å¹¶ä¼˜åŒ–å®ƒä»¬ã€‚
- å¯è§†åŒ–ç»“æœä»¥ç†è§£æ¨¡å‹ã€‚
- ä½¿ç”¨æ­£åˆ™åŒ–å¤„ç†å¤æ‚æ¨¡å¼ã€‚

ä½ å¯ä»¥å°è¯•è°ƒæ•´å‚æ•°ï¼Œå¦‚ `alpha` æˆ– `lambda_`ï¼Œçœ‹çœ‹æ¨¡å‹å¦‚ä½•å˜åŒ–ã€‚ç»§ç»­æ¢ç´¢ï¼Œå¿«ä¹å­¦ä¹ ï¼

---

**æœ¬ç¯‡æ–‡ç« çš„éƒ¨åˆ†å†…å®¹å’Œæ€æƒ³å‚è€ƒäº† [å´æ©è¾¾ (Andrew Ng)](https://www.andrewng.org/) åœ¨ [Coursera æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://www.coursera.org/learn/machine-learning) ä¸­çš„è®²è§£ï¼Œæ„Ÿè°¢ä»–å¯¹æœºå™¨å­¦ä¹ é¢†åŸŸçš„å“è¶Šè´¡çŒ®ã€‚**
![å†³ç­–è¾¹ç•Œ](/static/images/MLc1/output_46_0.png)
