---
title: 'Transformer å®Œå…¨æŒ‡å—ï¼šä»æ³¨æ„åŠ›æœºåˆ¶åˆ° GPT/DeepSeek æ¶æ„ï¼Œå†åˆ° LLM ä½¿ç”¨æŠ€å·§'
date: '2026-02-09'
tags:
  [
    'AI',
    'Transformer',
    'Attention',
    'Self-Attention',
    'BERT',
    'GPT',
    'NLP',
    'PyTorch',
    'Deep Learning',
  ]
draft: false
comments: true
summary: 'æœ€å…¨é¢çš„ Transformer ä¸­æ–‡æ•™ç¨‹ï¼šä»æ³¨æ„åŠ›ç›´è§‰åˆ° Q/K/V æ‰‹ç®—ï¼Œä»å¤šå¤´æ³¨æ„åŠ›åˆ°ä½ç½®ç¼–ç ï¼Œä» Encoder-Decoder åˆ° PyTorch å®ç°ã€‚æ¶µç›– GPT-4/Claude/DeepSeek/LLaMA æ¶æ„å¯¹æ¯”ã€LLM ä½¿ç”¨æŠ€å·§çš„ Transformer åŸç†è§£æã€2025-2026 å‰æ²¿è¶‹åŠ¿ï¼ˆFlashAttentionã€Mamba/SSMã€MoEï¼‰ï¼Œé™„ 10 ä¸ªäº¤äº’å¼å¯è§†åŒ–å’Œå®Œæ•´ä»£ç ã€‚'
authors: ['allen']
---

# Transformer å®Œå…¨æŒ‡å—ï¼šä»æ³¨æ„åŠ›æœºåˆ¶åˆ° GPT/DeepSeek æ¶æ„ï¼Œå†åˆ° LLM ä½¿ç”¨æŠ€å·§

> ğŸ¯ **æœ¬æ–‡ç›®æ ‡**ï¼šå¦‚æœä½ å¬è¿‡ Transformer ä½†æ€»è§‰å¾—"ä¼¼æ‡‚éæ‡‚"ï¼Œæˆ–è€…æƒ³çŸ¥é“ä¸ºä»€ä¹ˆ ChatGPT å¼€å¤´å’Œç»“å°¾è®°å¾—æœ€ç‰¢ã€ä¸ºä»€ä¹ˆ Chain-of-Thought æœ‰æ•ˆâ€”â€”è¿™ç¯‡æ–‡ç« å°±æ˜¯ä¸ºä½ å†™çš„ã€‚æˆ‘ä»¬å°†ä»æœ€æœ´ç´ çš„ç›´è§‰å‡ºå‘ï¼Œä¸€æ­¥æ­¥æ‹†è§£ Transformer çš„æ¯ä¸€ä¸ªç»„ä»¶ï¼Œç›´åˆ°ä½ ä¸ä»…èƒ½ç”¨ PyTorch ä»é›¶å†™å‡ºä¸€ä¸ªå®Œæ•´çš„ Transformerï¼Œè¿˜èƒ½çœŸæ­£ç†è§£ä½ æ¯å¤©ä½¿ç”¨çš„ AI èƒŒåçš„åŸç†ã€‚

---

## ğŸ“‘ ç›®å½•

- [ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ Transformerï¼Ÿ](#ä¸€ä¸ºä»€ä¹ˆéœ€è¦-transformer)
- [äºŒã€æ³¨æ„åŠ›æœºåˆ¶ï¼šä»"æŸ¥å­—å…¸"è¯´èµ·](#äºŒæ³¨æ„åŠ›æœºåˆ¶ä»æŸ¥å­—å…¸è¯´èµ·)
- [ä¸‰ã€Qã€Kã€V çŸ©é˜µè¿ç®—ï¼šæ³¨æ„åŠ›çš„æ•°å­¦å®ç°](#ä¸‰qkv-çŸ©é˜µè¿ç®—æ³¨æ„åŠ›çš„æ•°å­¦å®ç°)
- [å››ã€ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼šä¸ºä»€ä¹ˆè¦é™¤ä»¥ âˆšd_k](#å››ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ä¸ºä»€ä¹ˆè¦é™¤ä»¥-dk)
- [äº”ã€å¤šå¤´æ³¨æ„åŠ›ï¼šä¸€ä¸ªè„‘è¢‹æ€ä¹ˆå¤Ÿï¼Ÿ](#äº”å¤šå¤´æ³¨æ„åŠ›ä¸€ä¸ªè„‘è¢‹æ€ä¹ˆå¤Ÿ)
- [å…­ã€ä½ç½®ç¼–ç ï¼šè®©æ¨¡å‹çŸ¥é“"è°åœ¨å‰è°åœ¨å"](#å…­ä½ç½®ç¼–ç è®©æ¨¡å‹çŸ¥é“è°åœ¨å‰è°åœ¨å)
- [ä¸ƒã€Encoder æ¶æ„ï¼šé˜…è¯»ç†è§£ä¸“å®¶](#ä¸ƒencoder-æ¶æ„é˜…è¯»ç†è§£ä¸“å®¶)
- [å…«ã€Decoder æ¶æ„ï¼šä¸¥ç¦å·çœ‹çš„å†™ä½œä¸“å®¶](#å…«decoder-æ¶æ„ä¸¥ç¦å·çœ‹çš„å†™ä½œä¸“å®¶)
- [ä¹ã€å®Œæ•´ Transformerï¼šç»„è£…ç¼–ç å™¨ä¸è§£ç å™¨](#ä¹å®Œæ•´-transformerç»„è£…ç¼–ç å™¨ä¸è§£ç å™¨)
- [åã€ä¸‰å¤§æ¶æ„å®¶æ—ä¸ä¸»æµ LLM æ¶æ„è¯¦è§£](#åä¸‰å¤§æ¶æ„å®¶æ—ä¸ä¸»æµ-llm-æ¶æ„è¯¦è§£)
- [åä¸€ã€PyTorch ä»£ç å®æˆ˜ï¼šä»é›¶æ­å»º Transformer](#åä¸€pytorch-ä»£ç å®æˆ˜ä»é›¶æ­å»º-transformer)
- [åäºŒã€å¸¸è§å‘ç‚¹ä¸æœ€ä½³å®è·µ](#åäºŒå¸¸è§å‘ç‚¹ä¸æœ€ä½³å®è·µ)
- [åä¸‰ã€ä» Transformer åŸç†åˆ° LLM ä½¿ç”¨æŠ€å·§](#åä¸‰ä»-transformer-åŸç†åˆ°-llm-ä½¿ç”¨æŠ€å·§)
- [åå››ã€2025-2026 Transformer å‰æ²¿è¶‹åŠ¿](#åå››2025-2026-transformer-å‰æ²¿è¶‹åŠ¿)
- [åäº”ã€æ€»ç»“ä¸å­¦ä¹ è·¯çº¿](#åäº”æ€»ç»“ä¸å­¦ä¹ è·¯çº¿)

---

## ä¸€ã€ä¸ºä»€ä¹ˆéœ€è¦ Transformerï¼Ÿ

åœ¨ Transformer å‡ºç°ä¹‹å‰ï¼ˆ2017 å¹´ä»¥å‰ï¼‰ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†çš„"æ ‡é…"æ¨¡å‹æ˜¯ **RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä¸€ç§åƒæµæ°´çº¿ä¸€æ ·ï¼Œä¸€ä¸ªè¯ä¸€ä¸ªè¯é¡ºåºå¤„ç†æ–‡æœ¬çš„ç¥ç»ç½‘ç»œï¼‰å’Œå®ƒçš„å‡çº§ç‰ˆ **LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šRNN åŠ äº†ä¸€ä¸ª"è®°å¿†æœ¬"æ¥ç¼“è§£é—å¿˜é—®é¢˜ï¼‰ã€‚

ä½†æ˜¯å®ƒä»¬æœ‰ä¸‰ä¸ªè‡´å‘½çš„ç¼ºé™·ï¼š

### 1.1 RNN çš„ä¸‰å¤§ç—›ç‚¹

**ç—›ç‚¹ â‘ ï¼šä¸²è¡Œå¤„ç†ï¼Œæ…¢å¦‚èœ—ç‰› ğŸŒ**

æƒ³è±¡ä½ åœ¨ç¿»è¯‘ä¸€æœ¬ 1000 é¡µçš„ä¹¦ã€‚RNN çš„åšæ³•æ˜¯ï¼šè¯»å®Œç¬¬ 1 é¡µâ†’ç¿»è¯‘â†’è¯»ç¬¬ 2 é¡µâ†’ç¿»è¯‘â†’â€¦â€¦â†’ç¬¬ 1000 é¡µã€‚å®ƒå¿…é¡»**ä¸€é¡µä¸€é¡µæ¥**ï¼Œå‰é¢æ²¡è¯»å®Œå°±ä¸èƒ½è¯»åé¢çš„ã€‚

```python
# RNN çš„ä¼ªä»£ç  â€” æ³¨æ„è¿™é‡Œæ˜¯ä¸€ä¸ª for å¾ªç¯ï¼Œå¿…é¡»ä¸²è¡Œæ‰§è¡Œ
hidden = zeros(hidden_size)
for word in sentence:      # å¿…é¡»é¡ºåºå¤„ç†æ¯ä¸ªè¯ï¼
    hidden = rnn_cell(word, hidden)  # å½“å‰è¯ + ä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€
output = hidden  # æœ€ç»ˆçŠ¶æ€åŒ…å«"å…¨éƒ¨ä¿¡æ¯"
```

è¿™æ„å‘³ç€ä¸€ä¸ª 1000 è¯çš„å¥å­ï¼Œéœ€è¦åš 1000 æ­¥è®¡ç®—ï¼Œå®Œå…¨æ— æ³•åˆ©ç”¨ GPU çš„**å¹¶è¡Œè®¡ç®—**èƒ½åŠ›ã€‚

**ç—›ç‚¹ â‘¡ï¼šé•¿è·ç¦»é—å¿˜ ğŸ§ ğŸ’¨**

```
"é‚£åªåœ¨å…¬å›­é‡Œè¿½è´è¶çš„ã€æ¯›è‰²é‡‘é»„çš„ã€å¸¦ç€çº¢è‰²é¡¹åœˆçš„å°çŒ«ï¼Œæœ€ç»ˆè¿˜æ˜¯æ²¡æœ‰æŠ“åˆ°å®ƒã€‚"
```

å½“ RNN è¯»åˆ°"å®ƒ"è¿™ä¸ªå­—æ—¶ï¼Œå®ƒéœ€è¦çŸ¥é“"å®ƒ"æŒ‡çš„æ˜¯"å°çŒ«"ã€‚ä½†é—®é¢˜æ˜¯ï¼šä»"å°çŒ«"åˆ°"å®ƒ"ä¸­é—´éš”äº†åå‡ ä¸ªè¯ï¼ŒRNN çš„è®°å¿†ä¼šéšç€æ¯ä¸€æ­¥è®¡ç®—é€æ¸**è¡°å‡**â€”â€”å°±åƒä¼ è¯æ¸¸æˆï¼Œä¼ äº†åå‡ ä¸ªäººä»¥åï¼ŒåŸå§‹ä¿¡æ¯æ—©å·²é¢ç›®å…¨éã€‚

**ç—›ç‚¹ â‘¢ï¼šä¿¡æ¯ç“¶é¢ˆ ğŸ¾**

åœ¨ Encoder-Decoder ç»“æ„ï¼ˆæ¯”å¦‚æœºå™¨ç¿»è¯‘ï¼‰ä¸­ï¼ŒRNN æŠŠæ•´ä¸ªè¾“å…¥å¥å­å‹ç¼©æˆä¸€ä¸ª**å›ºå®šå¤§å°çš„å‘é‡**ï¼Œç„¶åè®© Decoder ä»è¿™ä¸ªå‘é‡ç”Ÿæˆç¿»è¯‘ã€‚ä¸€ä¸ªå‡ ç™¾ç»´çš„å‘é‡è¦æ‰¿è½½æ•´ä¸ªå¥å­çš„æ‰€æœ‰ä¿¡æ¯â€”â€”è¿™å°±åƒæŠŠä¸€æœ¬ä¹¦çš„å†…å®¹å‹ç¼©åˆ°ä¸€å¼ ä¾¿åˆ©è´´ä¸Šã€‚

### 1.2 Transformer çš„é©å‘½æ€§æ€æƒ³

2017 å¹´ï¼ŒGoogle çš„è®ºæ–‡ _"Attention Is All You Need"_ æå‡ºäº† Transformerï¼Œæ ¸å¿ƒæ€æƒ³ç®€å•åˆ°ä»¤äººæƒŠå¹ï¼š

> **ä¸éœ€è¦æŒ‰é¡ºåºè¯»ï¼Œç›´æ¥"å…¨å±€æ‰«æ"ï¼Œè®©æ¯ä¸ªè¯åŒæ—¶çœ‹åˆ°æ‰€æœ‰å…¶ä»–è¯ã€‚**

| å¯¹æ¯”ç»´åº¦   | RNN/LSTM           | Transformer        |
| ---------- | ------------------ | ------------------ |
| å¤„ç†æ–¹å¼   | ä¸²è¡Œï¼ˆä¸€ä¸ªæ¥ä¸€ä¸ªï¼‰ | å¹¶è¡Œï¼ˆä¸€æ¬¡çœ‹å…¨éƒ¨ï¼‰ |
| é•¿è·ç¦»ä¾èµ– | éšè·ç¦»è¡°å‡         | ç›´æ¥è¿æ¥ï¼Œæ— è¡°å‡   |
| è®­ç»ƒé€Ÿåº¦   | æ…¢ï¼ˆæ— æ³•å¹¶è¡Œï¼‰     | å¿«ï¼ˆå……åˆ†åˆ©ç”¨ GPUï¼‰ |
| ä¿¡æ¯ä¼ é€’   | é€šè¿‡éšè—çŠ¶æ€ä¼ é€’   | é€šè¿‡æ³¨æ„åŠ›ç›´æ¥è®¿é—® |

ç”¨ä¸€ä¸ªç”Ÿæ´»ä¸­çš„ç±»æ¯”ï¼š

- **RNN** åƒæ˜¯åœ¨ä¸€ä¸ªå˜ˆæ‚çš„æ´¾å¯¹ä¸Šï¼Œä½ åªèƒ½å’Œ**æ—è¾¹çš„äºº**è¯´è¯ï¼Œç„¶åè®©ä»–å¸®ä½ ä¼ è¯ç»™è¿œå¤„çš„äººã€‚ä¼ åˆ°ç¬¬åä¸ªäººæ—¶ï¼Œä¿¡æ¯æ—©å°±å˜äº†ã€‚
- **Transformer** åƒæ˜¯ä½ æ‹¥æœ‰ä¸€å‰¯**åƒé‡Œçœ¼**ï¼Œå¯ä»¥åŒæ—¶çœ‹åˆ°æ´¾å¯¹ä¸Šçš„**æ¯ä¸€ä¸ªäºº**ï¼Œç›´æ¥å’Œä½ æƒ³äº¤æµçš„äººå¯¹è¯ã€‚

è¿™å°±æ˜¯ **æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰** çš„æ ¸å¿ƒç›´è§‰ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ·±å…¥å®ƒçš„ç»†èŠ‚ã€‚

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - RNN çš„ä¸‰å¤§é—®é¢˜ï¼šä¸²è¡Œå¤„ç†ã€é•¿è·ç¦»é—å¿˜ã€ä¿¡æ¯ç“¶é¢ˆ
> - Transformer ç”¨"æ³¨æ„åŠ›æœºåˆ¶"æ›¿ä»£äº†å¾ªç¯ç»“æ„ï¼Œå®ç°å¹¶è¡Œå¤„ç†å’Œå…¨å±€ä¿¡æ¯è®¿é—®
> - è®ºæ–‡å _"Attention Is All You Need"_ ä¸æ˜¯å¤¸å¼ â€”â€”å®ƒçœŸçš„åªé æ³¨æ„åŠ›å°±å®Œæˆäº†ä¸€åˆ‡

---

## äºŒã€æ³¨æ„åŠ›æœºåˆ¶ï¼šä»"æŸ¥å­—å…¸"è¯´èµ·

**æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä¸€ç§è®©æ¨¡å‹åœ¨å¤„ç†æŸä¸ªè¯æ—¶ï¼Œèƒ½å¤Ÿ"å›å¤´çœ‹"æ‰€æœ‰å…¶ä»–è¯ï¼Œå¹¶å†³å®šæ¯ä¸ªè¯æœ‰å¤šé‡è¦çš„æŠ€æœ¯ï¼‰æ˜¯ Transformer çš„çµé­‚ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªäººäººéƒ½æ‡‚çš„ä¾‹å­æ¥ç†è§£å®ƒã€‚

### 2.1 ç›´è§‰ç±»æ¯”ï¼šæŸ¥å­—å…¸

æƒ³è±¡ä½ åœ¨æŸ¥ä¸€æœ¬**è‹±æ±‰å­—å…¸**ï¼š

1. ä½ æœ‰ä¸€ä¸ªæƒ³æŸ¥çš„è¯ï¼ˆæ¯”å¦‚ "apple"ï¼‰â€”â€”è¿™æ˜¯ä½ çš„ **Queryï¼ˆæŸ¥è¯¢ï¼‰**
2. å­—å…¸é‡Œæ¯ä¸ªè¯æ¡æœ‰ä¸€ä¸ª**ç´¢å¼•è¯**ï¼ˆæ¯”å¦‚ "apple", "apply", "banana"ï¼‰â€”â€”è¿™äº›æ˜¯ **Keyï¼ˆé”®ï¼‰**
3. æ¯ä¸ªè¯æ¡å¯¹åº”ä¸€ä¸ª**ç¿»è¯‘**ï¼ˆ"è‹¹æœ", "åº”ç”¨", "é¦™è•‰"ï¼‰â€”â€”è¿™äº›æ˜¯ **Valueï¼ˆå€¼ï¼‰**

æŸ¥å­—å…¸çš„è¿‡ç¨‹å°±æ˜¯ï¼š

```
1. æ‹¿ç€ Queryï¼ˆ"apple"ï¼‰å’Œæ¯ä¸ª Key æ¯”è¾ƒç›¸ä¼¼åº¦
2. å‘ç° Key="apple" å®Œå…¨åŒ¹é… â†’ å¾—åˆ†æœ€é«˜
3. Key="apply" æœ‰ç‚¹åƒ â†’ å¾—åˆ†è¾ƒé«˜
4. Key="banana" å®Œå…¨ä¸åƒ â†’ å¾—åˆ†å¾ˆä½
5. æ ¹æ®å¾—åˆ†é«˜ä½ï¼ŒåŠ æƒè·å–å¯¹åº”çš„ Value
6. æœ€ç»ˆç»“æœï¼šä¸»è¦è·å–"è‹¹æœ"çš„ç¿»è¯‘ï¼Œç•¥å¸¦ä¸€ç‚¹"åº”ç”¨"çš„ä¿¡æ¯
```

> ğŸ’¡ **è¿™å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨ï¼šæ ¹æ®ç›¸ä¼¼åº¦ï¼Œå¯¹ä¿¡æ¯è¿›è¡ŒåŠ æƒç»„åˆã€‚**

### 2.2 ä»å­—å…¸åˆ°æ³¨æ„åŠ›çš„æ•°å­¦

è®©æˆ‘ä»¬æŠŠ"æŸ¥å­—å…¸"ç¿»è¯‘æˆæ•°å­¦è¯­è¨€ï¼š

**ç¬¬ä¸€æ­¥ï¼šè®¡ç®—ç›¸ä¼¼åº¦**

æœ€ç›´è§‚çš„ç›¸ä¼¼åº¦åº¦é‡å°±æ˜¯ **ç‚¹ç§¯ï¼ˆDot Productï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä¸¤ä¸ªå‘é‡å¯¹åº”å…ƒç´ ç›¸ä¹˜å†æ±‚å’Œï¼Œæ•°å€¼è¶Šå¤§è¯´æ˜ä¸¤ä¸ªå‘é‡æ–¹å‘è¶Šä¸€è‡´ï¼Œå³è¶Š"åƒ"ï¼‰ï¼š

$$
\text{score}(Q, K_i) = Q \cdot K_i = \sum_{j=1}^{d} Q_j \times K_{i,j}
$$

```python
import torch

# ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š3ä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”¨4ç»´å‘é‡è¡¨ç¤º
query = torch.tensor([1.0, 0.5, 0.0, 0.3])      # "apple" çš„å‘é‡
keys = torch.tensor([
    [1.0, 0.4, 0.1, 0.3],   # "apple" â€” éå¸¸ç›¸ä¼¼
    [0.8, 0.3, 0.2, 0.2],   # "apply" â€” æœ‰ç‚¹åƒ
    [0.0, 0.1, 0.9, 0.7],   # "banana" â€” å®Œå…¨ä¸åƒ
])

# ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦
scores = torch.matmul(keys, query)
print(f"åŸå§‹å¾—åˆ†: {scores}")
# è¾“å‡ºç±»ä¼¼: tensor([1.39, 1.01, 0.26])
```

**ğŸ” æ‰‹ç®—å®Œæ•´è¿‡ç¨‹ï¼šQuery å¦‚ä½•æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ Key**

è®©æˆ‘ä»¬ç”¨ä¸Šé¢çš„ä¾‹å­ï¼Œ**ä¸€æ­¥ä¸€æ­¥**æ‰‹ç®—å‡º Query "apple" æ˜¯æ€ä¹ˆåœ¨ 3 ä¸ª Key ä¸­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„é‚£ä¸ªçš„ã€‚ä¸è¦è·³æ­¥ï¼Œæˆ‘ä»¬æ¯ä¸€æ­¥éƒ½å†™å‡ºæ¥ï¼š

```
Queryï¼ˆæˆ‘è¦æŸ¥çš„è¯ "apple"ï¼‰:  [1.0, 0.5, 0.0, 0.3]

Keyâ‚ï¼ˆ"apple" çš„ç´¢å¼•ï¼‰:      [1.0, 0.4, 0.1, 0.3]
Keyâ‚‚ï¼ˆ"apply" çš„ç´¢å¼•ï¼‰:      [0.8, 0.3, 0.2, 0.2]
Keyâ‚ƒï¼ˆ"banana" çš„ç´¢å¼•ï¼‰:     [0.0, 0.1, 0.9, 0.7]
```

**è®¡ç®— Query å’Œ Keyâ‚ çš„ç‚¹ç§¯ï¼ˆ"apple" vs "apple"ï¼‰ï¼š**

```
  Q  Â·  Kâ‚  =  1.0Ã—1.0  +  0.5Ã—0.4  +  0.0Ã—0.1  +  0.3Ã—0.3
             =    1.0    +    0.2    +    0.0    +    0.09
             =    1.29
```

ğŸ§ æ¯ä¸€å¯¹å¯¹åº”ä½ç½®çš„æ•°å­—ç›¸ä¹˜ï¼Œç„¶åå…¨éƒ¨åŠ èµ·æ¥ã€‚ä¸¤ä¸ªå‘é‡è¶Š"æœåŒä¸€ä¸ªæ–¹å‘"ï¼Œä¹˜å‡ºæ¥çš„å’Œå°±è¶Šå¤§ã€‚

**è®¡ç®— Query å’Œ Keyâ‚‚ çš„ç‚¹ç§¯ï¼ˆ"apple" vs "apply"ï¼‰ï¼š**

```
  Q  Â·  Kâ‚‚  =  1.0Ã—0.8  +  0.5Ã—0.3  +  0.0Ã—0.2  +  0.3Ã—0.2
             =    0.8    +    0.15   +    0.0    +    0.06
             =    1.01
```

**è®¡ç®— Query å’Œ Keyâ‚ƒ çš„ç‚¹ç§¯ï¼ˆ"apple" vs "banana"ï¼‰ï¼š**

```
  Q  Â·  Kâ‚ƒ  =  1.0Ã—0.0  +  0.5Ã—0.1  +  0.0Ã—0.9  +  0.3Ã—0.7
             =    0.0    +    0.05   +    0.0    +    0.21
             =    0.26
```

**æ±‡æ€»åŸå§‹å¾—åˆ†ï¼š**

```
Score("apple") = 1.29  â† æœ€é«˜ï¼æœç„¶ "apple" å’Œ "apple" æœ€åƒ
Score("apply") = 1.01  â† æœ‰ç‚¹åƒï¼Œä½†ä¸å¦‚å®Œå…¨åŒ¹é…
Score("banana") = 0.26 â† å¾—åˆ†å¾ˆä½ï¼Œ"banana" å’Œ "apple" å·®åˆ«å¤§
```

> ğŸ’¡ **ç›´è§‰ç†è§£**ï¼šç‚¹ç§¯è¶Šå¤§ = ä¸¤ä¸ªå‘é‡è¶Š"åƒ" = æ³¨æ„åŠ›è¶Šå¼ºã€‚å°±åƒä½ åœ¨å­—å…¸é‡Œç¿»åˆ°äº†ä¸€ä¸ªå’Œä½ è¦æŸ¥çš„è¯é•¿å¾—å¾ˆåƒçš„è¯æ¡ï¼Œä½ è‡ªç„¶ä¼šå¤šçœ‹å‡ çœ¼ã€‚

**ç¬¬äºŒæ­¥ï¼šSoftmax â€” æŠŠåˆ†æ•°å˜æˆ"èšå…‰ç¯"**

**Softmax**ï¼ˆç™½è¯ç‰ˆï¼šä¸€ç§æ•°å­¦å‡½æ•°ï¼ŒæŠŠä»»æ„æ•°å€¼è½¬æ¢æˆ 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ‰€æœ‰å€¼åŠ èµ·æ¥ç­‰äº 1ï¼‰å°±åƒä¸€ä¸ªèšå…‰ç¯â€”â€”æŠŠæ¨¡ç³Šçš„åˆ†æ•°å˜æˆæ¸…æ™°çš„æ¦‚ç‡åˆ†å¸ƒï¼š

$$
\text{attention\_weight}_i = \frac{e^{\text{score}_i}}{\sum_{j} e^{\text{score}_j}}
$$

```python
# åº”ç”¨ Softmax
weights = torch.softmax(scores, dim=0)
print(f"æ³¨æ„åŠ›æƒé‡: {weights}")
# è¾“å‡ºç±»ä¼¼: tensor([0.50, 0.34, 0.16])
# "apple" è·å¾—æœ€é«˜æƒé‡ï¼Œ"banana" æœ€ä½
```

**ç¬¬ä¸‰æ­¥ï¼šåŠ æƒç»„åˆ Value**

```python
values = torch.tensor([
    [1.0, 0.0, 0.0],   # "è‹¹æœ" çš„è¡¨ç¤º
    [0.0, 1.0, 0.0],   # "åº”ç”¨" çš„è¡¨ç¤º
    [0.0, 0.0, 1.0],   # "é¦™è•‰" çš„è¡¨ç¤º
])

# åŠ æƒç»„åˆ
output = torch.matmul(weights, values)
print(f"æœ€ç»ˆè¾“å‡º: {output}")
# è¾“å‡ºç±»ä¼¼: tensor([0.50, 0.34, 0.16])
# ä¸»è¦æ˜¯"è‹¹æœ"çš„ä¿¡æ¯ï¼Œå°‘é‡"åº”ç”¨"çš„ä¿¡æ¯
```

### 2.3 è‡ªæ³¨æ„åŠ›ï¼šè‡ªå·±æŸ¥è‡ªå·±çš„å­—å…¸

ä¸Šé¢çš„ä¾‹å­æ˜¯åœ¨ä¸¤ç§ä¸åŒçš„ä¸œè¥¿ä¹‹é—´åšæ³¨æ„åŠ›ï¼ˆè‹±æ–‡â†’ä¸­æ–‡ï¼‰ã€‚ä½† Transformer æœ€å¸¸ç”¨çš„æ˜¯ **è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä¸€ä¸ªå¥å­ä¸­çš„æ¯ä¸ªè¯ï¼Œéƒ½æŠŠ**è‡ªå·±å¥å­ä¸­çš„æ‰€æœ‰è¯**å½“ä½œå­—å…¸æ¥æŸ¥ï¼‰ã€‚

æ¥çœ‹ä¸€ä¸ªç»å…¸ä¾‹å­ï¼š

```
"åŠ¨ç‰©æ²¡æœ‰ç©¿è¿‡é©¬è·¯ï¼Œå› ä¸ºå®ƒå¤ªç´¯äº†ã€‚"
```

å½“æ¨¡å‹å¤„ç†"å®ƒ"è¿™ä¸ªè¯æ—¶ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼šè®©"å®ƒ"å»**æŸ¥çœ‹å¥å­ä¸­æ‰€æœ‰è¯**ï¼š

- "åŠ¨ç‰©" â†’ ç›¸ä¼¼åº¦å¾ˆé«˜ï¼ˆ"å®ƒ"æŒ‡ä»£"åŠ¨ç‰©"ï¼‰âœ…
- "é©¬è·¯" â†’ ç›¸ä¼¼åº¦ä½ï¼ˆ"å®ƒ"ä¸æ˜¯"é©¬è·¯"ï¼‰
- "ç´¯" â†’ ç›¸ä¼¼åº¦ä¸­ç­‰ï¼ˆå’Œ"å®ƒ"çš„çŠ¶æ€æœ‰å…³ï¼‰
- "ç©¿è¿‡" â†’ ç›¸ä¼¼åº¦ä½

è¿™æ ·ï¼Œæ¨¡å‹å°±è‡ªåŠ¨å­¦ä¼šäº†**æŒ‡ä»£æ¶ˆè§£ï¼ˆCoreference Resolutionï¼‰**â€”â€”"å®ƒ"æŒ‡çš„æ˜¯"åŠ¨ç‰©"ï¼Œè€Œä¸æ˜¯"é©¬è·¯"ã€‚

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–å±•ç¤ºäº†è‡ªæ³¨æ„åŠ›çš„å·¥ä½œè¿‡ç¨‹â€”â€”ç‚¹å‡»ä»»æ„è¯æŸ¥çœ‹å®ƒå¯¹å…¶ä»–è¯çš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚

<IframeEmbed
  src="/visualizations/transformer-attention-intuition.html"
  minHeight={760}
  title="æ³¨æ„åŠ›æœºåˆ¶ç›´è§‰å¯è§†åŒ–"
/>

### 2.4 æ³¨æ„åŠ›çš„ä¸å¯¹ç§°æ€§

ä¸€ä¸ªå¸¸è¢«å¿½ç•¥çš„é‡è¦æ€§è´¨ï¼š**æ³¨æ„åŠ›ä¸æ˜¯å¯¹ç§°çš„**ï¼

"è‹¹æœ"å¯¹"ç‰›é¡¿"çš„æ³¨æ„åŠ› â‰  "ç‰›é¡¿"å¯¹"è‹¹æœ"çš„æ³¨æ„åŠ›

> ğŸ’¡ **ç™½è¯ç‰ˆ**ï¼š"è‹¹æœç ¸åˆ°äº†ç‰›é¡¿"â€”â€”ä»è‹¹æœçš„è§’åº¦çœ‹ï¼Œç‰›é¡¿åªæ˜¯ä¸€ä¸ªè¢«ç ¸çš„è·¯äººï¼›ä½†ä»ç‰›é¡¿çš„è§’åº¦çœ‹ï¼Œè‹¹æœæ˜¯æ”¹å˜ä»–å‘½è¿çš„å…³é”®ï¼

è¿™æ˜¯å› ä¸º Q å’Œ K æ˜¯é€šè¿‡**ä¸åŒçš„æŠ•å½±çŸ©é˜µ**ç”Ÿæˆçš„ï¼Œæ‰€ä»¥ $Q_A \cdot K_B \neq Q_B \cdot K_A$ã€‚

### 2.5 å¸¸è§å‘ç‚¹

> âš ï¸ **å‘ 1ï¼šæ··æ·†æ³¨æ„åŠ›æƒé‡å’Œæ³¨æ„åŠ›è¾“å‡º**
>
> - æ³¨æ„åŠ›**æƒé‡**ï¼ˆweightsï¼‰æ˜¯ softmax åçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå½¢çŠ¶ä¸º `[seq_len, seq_len]`
> - æ³¨æ„åŠ›**è¾“å‡º**ï¼ˆoutputï¼‰æ˜¯æƒé‡ä¸ Value çš„åŠ æƒå’Œï¼Œå½¢çŠ¶ä¸º `[seq_len, d_v]`
> - å¾ˆå¤šåˆå­¦è€…æŠŠè¿™ä¸¤ä¸ªææ··äº†

> âš ï¸ **å‘ 2ï¼šä»¥ä¸ºæ³¨æ„åŠ›åªçœ‹"æœ€ç›¸å…³"çš„è¯**
>
> Softmax äº§ç”Ÿçš„æ˜¯**æ¦‚ç‡åˆ†å¸ƒ**ï¼Œæ‰€ä»¥æ¨¡å‹å®é™…ä¸Šä¼šçœ‹**æ‰€æœ‰è¯**ï¼Œåªæ˜¯æƒé‡ä¸åŒã€‚å³ä½¿ä¸€ä¸ªè¯çš„æƒé‡åªæœ‰ 0.01ï¼Œå®ƒçš„ä¿¡æ¯ä¹Ÿä¼šè¢«åŒ…å«åœ¨è¾“å‡ºä¸­â€”â€”åªæ˜¯è´¡çŒ®å¾ˆå°ã€‚

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - æ³¨æ„åŠ› = ç”¨ Query åœ¨ Key ä¸­æŸ¥æ‰¾ç›¸ä¼¼é¡¹ï¼Œç„¶åç”¨ç›¸ä¼¼åº¦åŠ æƒè·å– Value
> - è‡ªæ³¨æ„åŠ› = å¥å­ä¸­çš„æ¯ä¸ªè¯éƒ½å’Œè‡ªå·±å¥å­ä¸­çš„æ‰€æœ‰è¯åšæ³¨æ„åŠ›
> - æ³¨æ„åŠ›æ˜¯ä¸å¯¹ç§°çš„ï¼šA å…³æ³¨ B â‰  B å…³æ³¨ A
> - æ ¸å¿ƒå…¬å¼ï¼š`output = softmax(Q Â· K^T) Â· V`

---

## ä¸‰ã€Qã€Kã€V çŸ©é˜µè¿ç®—ï¼šæ³¨æ„åŠ›çš„æ•°å­¦å®ç°

ä¸Šä¸€èŠ‚æˆ‘ä»¬ç†è§£äº†æ³¨æ„åŠ›çš„ç›´è§‰ï¼Œä½†è¿˜æœ‰ä¸€ä¸ªå…³é”®é—®é¢˜ï¼š**Qã€Kã€V ä»å“ªæ¥ï¼Ÿ**

### 3.1 ç›´è§‰ï¼šåŒä¸€ä¸ªè¾“å…¥çš„"ä¸‰é‡äººæ ¼"

è¿˜è®°å¾—æŸ¥å­—å…¸çš„æ¯”å–»å—ï¼Ÿåœ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼ŒQã€Kã€V å…¨éƒ¨æ¥è‡ª**åŒä¸€ä¸ªè¾“å…¥**ï¼è¿™å°±åƒä¸€ä¸ªäººåœ¨é¢è¯•ä¸­åŒæ—¶æ‰®æ¼”ä¸‰ä¸ªè§’è‰²ï¼š

- **ä½œä¸º Queryï¼ˆæé—®è€…ï¼‰**ï¼šæˆ‘æƒ³äº†è§£ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
- **ä½œä¸º Keyï¼ˆç®€å†æ ‡é¢˜ï¼‰**ï¼šæˆ‘èƒ½æä¾›ä»€ä¹ˆç±»å‹çš„ä¿¡æ¯ï¼Ÿ
- **ä½œä¸º Valueï¼ˆç®€å†å†…å®¹ï¼‰**ï¼šæˆ‘å…·ä½“èƒ½æä¾›ä»€ä¹ˆå†…å®¹ï¼Ÿ

åŒä¸€ä¸ªè¯ï¼Œé€šè¿‡**ä¸åŒçš„çº¿æ€§å˜æ¢**ï¼Œè¢«æŠ•å½±åˆ°ä¸‰ä¸ªä¸åŒçš„"è§’è‰²ç©ºé—´"ï¼š

$$
Q = X \cdot W_Q, \quad K = X \cdot W_K, \quad V = X \cdot W_V
$$

å…¶ä¸­ **$X$** æ˜¯è¾“å…¥åµŒå…¥çŸ©é˜µï¼ˆç™½è¯ç‰ˆï¼šæŠŠæ¯ä¸ªè¯è½¬æ¢æˆä¸€ä¸²æ•°å­—åï¼ŒæŠŠæ•´ä¸ªå¥å­çš„æ•°å­—æ’æˆä¸€ä¸ªçŸ©é˜µï¼‰ï¼Œ$W_Q, W_K, W_V$ æ˜¯ä¸‰ä¸ªå¯å­¦ä¹ çš„ **æƒé‡çŸ©é˜µï¼ˆWeight Matrixï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä¸€ç»„å¯ä»¥åœ¨è®­ç»ƒä¸­ä¸æ–­è°ƒæ•´çš„å‚æ•°ï¼Œç”¨æ¥åšçº¿æ€§å˜æ¢ï¼‰ã€‚

### 3.2 ç°å®ä¾‹å­ï¼šä¸€æ­¥æ­¥ç®—æ³¨æ„åŠ›

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å« 4 ä¸ªè¯çš„å¥å­ "I love deep learning"ï¼ŒåµŒå…¥ç»´åº¦ $d = 3$ï¼ˆå®é™…ä¸­æ˜¯ 512 æˆ– 768ï¼Œè¿™é‡Œç¼©å°ä¾¿äºç†è§£ï¼‰ã€‚

```python
import torch
import torch.nn as nn

# æ¨¡æ‹Ÿè¾“å…¥ï¼š4ä¸ªè¯ï¼Œæ¯ä¸ªè¯3ç»´åµŒå…¥
X = torch.tensor([
    [1.0, 0.0, 1.0],   # "I"
    [0.0, 1.0, 1.0],   # "love"
    [1.0, 1.0, 0.0],   # "deep"
    [0.0, 1.0, 0.0],   # "learning"
], dtype=torch.float32)

d_model = 3  # åµŒå…¥ç»´åº¦

# ä¸‰ä¸ªæŠ•å½±çŸ©é˜µï¼ˆå®é™…ä¸­è¿™äº›æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼‰
W_Q = torch.tensor([[1, 0, 1], [0, 1, 0], [1, 0, 0]], dtype=torch.float32)
W_K = torch.tensor([[0, 1, 0], [1, 0, 1], [0, 1, 1]], dtype=torch.float32)
W_V = torch.tensor([[1, 0, 0], [0, 0, 1], [0, 1, 0]], dtype=torch.float32)

# Step 1: æŠ•å½±å¾—åˆ° Q, K, V
Q = X @ W_Q   # [4, 3] @ [3, 3] = [4, 3]
K = X @ W_K   # [4, 3] @ [3, 3] = [4, 3]
V = X @ W_V   # [4, 3] @ [3, 3] = [4, 3]
print(f"Q:\n{Q}")
print(f"K:\n{K}")
print(f"V:\n{V}")
```

```python
# Step 2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° Q Â· K^T
scores = Q @ K.T   # [4, 3] @ [3, 4] = [4, 4]
print(f"æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ:\n{scores}")
# è¿™æ˜¯ä¸€ä¸ª 4Ã—4 çš„çŸ©é˜µï¼šscores[i][j] è¡¨ç¤ºç¬¬ i ä¸ªè¯å¯¹ç¬¬ j ä¸ªè¯çš„åŸå§‹æ³¨æ„åŠ›åˆ†æ•°
```

```python
# Step 3: ç¼©æ”¾ï¼ˆä¸‹ä¸€èŠ‚ä¼šè¯¦ç»†è§£é‡Šä¸ºä»€ä¹ˆè¦ç¼©æ”¾ï¼‰
import math
d_k = d_model  # Key çš„ç»´åº¦
scaled_scores = scores / math.sqrt(d_k)
print(f"ç¼©æ”¾åçš„åˆ†æ•°:\n{scaled_scores}")
```

```python
# Step 4: Softmax â€” æ¯ä¸€è¡Œå˜æˆæ¦‚ç‡åˆ†å¸ƒ
attention_weights = torch.softmax(scaled_scores, dim=-1)
print(f"æ³¨æ„åŠ›æƒé‡:\n{attention_weights}")
# æ¯ä¸€è¡Œçš„å’Œéƒ½ç­‰äº 1.0
print(f"æ¯è¡Œæ±‚å’ŒéªŒè¯: {attention_weights.sum(dim=-1)}")
```

```python
# Step 5: åŠ æƒç»„åˆ Value
output = attention_weights @ V   # [4, 4] @ [4, 3] = [4, 3]
print(f"æ³¨æ„åŠ›è¾“å‡º:\n{output}")
# æ¯ä¸€è¡Œæ˜¯å¯¹æ‰€æœ‰ Value å‘é‡çš„åŠ æƒç»„åˆ
```

> ğŸ’¡ **ç»´åº¦å˜åŒ–è·¯å¾„**
>
> | æ­¥éª¤ | æ“ä½œ            | å½¢çŠ¶                         |
> | ---- | --------------- | ---------------------------- |
> | è¾“å…¥ | X               | `[4, 3]` (seq_len Ã— d_model) |
> | æŠ•å½± | X Ã— W_Q         | `[4, 3]` (seq_len Ã— d_k)     |
> | åˆ†æ•° | Q Ã— K^T         | `[4, 4]` (seq_len Ã— seq_len) |
> | æƒé‡ | softmax(scores) | `[4, 4]`                     |
> | è¾“å‡º | weights Ã— V     | `[4, 3]` (seq_len Ã— d_v)     |

**ğŸ” æ‰‹ç®—çŸ©é˜µä¹˜æ³•ï¼šX Ã— W_Q åˆ°åº•åœ¨ç®—ä»€ä¹ˆï¼Ÿ**

å¾ˆå¤šåˆå­¦è€…çœ‹åˆ° `Q = X @ W_Q` å°±ä¸€å¤´é›¾æ°´â€”â€”è¿™ä¸ªçŸ©é˜µä¹˜æ³•åˆ°åº•åœ¨åšä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬ç”¨ä¸Šé¢çš„ä¾‹å­**ä¸€ä¸ªæ ¼å­ä¸€ä¸ªæ ¼å­**åœ°ç®—æ¸…æ¥šï¼š

```
è¾“å…¥ Xï¼ˆ4ä¸ªè¯ Ã— 3ç»´åµŒå…¥ï¼‰:        æƒé‡çŸ©é˜µ W_Qï¼ˆ3Ã—3ï¼‰:
         dâ‚   dâ‚‚   dâ‚ƒ                  dâ‚'  dâ‚‚'  dâ‚ƒ'
  "I"   [1.0, 0.0, 1.0]           dâ‚ [ 1,   0,   1 ]
  "love"[0.0, 1.0, 1.0]     Ã—     dâ‚‚ [ 0,   1,   0 ]
  "deep"[1.0, 1.0, 0.0]           dâ‚ƒ [ 1,   0,   0 ]
  "learning"[0.0, 1.0, 0.0]
```

**è®¡ç®— Q[0][0]ï¼ˆ"I" çš„ Query ç¬¬ 1 ç»´ï¼‰ï¼š**

```
Q[0][0] = X[0][0]Ã—W_Q[0][0] + X[0][1]Ã—W_Q[1][0] + X[0][2]Ã—W_Q[2][0]
        = 1.0 Ã— 1  +  0.0 Ã— 0  +  1.0 Ã— 1
        = 1.0 + 0.0 + 1.0
        = 2.0
```

ğŸ§ å°±æ˜¯å– X çš„**ç¬¬ 0 è¡Œ**ï¼ˆ"I" çš„åµŒå…¥ï¼‰å’Œ W_Q çš„**ç¬¬ 0 åˆ—**ï¼Œå¯¹åº”ç›¸ä¹˜å†æ±‚å’Œã€‚

**è®¡ç®— Q[0][1]ï¼ˆ"I" çš„ Query ç¬¬ 2 ç»´ï¼‰ï¼š**

```
Q[0][1] = 1.0 Ã— 0  +  0.0 Ã— 1  +  1.0 Ã— 0  =  0.0
```

**è®¡ç®— Q[0][2]ï¼ˆ"I" çš„ Query ç¬¬ 3 ç»´ï¼‰ï¼š**

```
Q[0][2] = 1.0 Ã— 1  +  0.0 Ã— 0  +  1.0 Ã— 0  =  1.0
```

**æ‰€ä»¥ "I" çš„ Query å‘é‡ = [2.0, 0.0, 1.0]**

åŒç†å¯ä»¥ç®—å‡ºæ‰€æœ‰è¯çš„ Queryï¼š

```
å®Œæ•´çš„ Q çŸ©é˜µï¼š
         dâ‚'  dâ‚‚'  dâ‚ƒ'
  "I"   [2.0, 0.0, 1.0]   â† 1Ã—1+0Ã—0+1Ã—1, 1Ã—0+0Ã—1+1Ã—0, 1Ã—1+0Ã—0+1Ã—0
  "love"[1.0, 1.0, 0.0]   â† 0Ã—1+1Ã—0+1Ã—1, 0Ã—0+1Ã—1+1Ã—0, 0Ã—1+1Ã—0+1Ã—0
  "deep"[1.0, 1.0, 1.0]   â† 1Ã—1+1Ã—0+0Ã—1, 1Ã—0+1Ã—1+0Ã—0, 1Ã—1+1Ã—0+0Ã—0
  "learning"[0.0, 1.0, 0.0] â† 0Ã—1+1Ã—0+0Ã—1, 0Ã—0+1Ã—1+0Ã—0, 0Ã—1+1Ã—0+0Ã—0
```

> ğŸ’¡ **ç›´è§‰ç†è§£**ï¼šçŸ©é˜µä¹˜æ³• X Ã— W_Q çš„æœ¬è´¨æ˜¯â€”â€”å¯¹è¾“å…¥ X çš„æ¯ä¸ªè¯å‘é‡åšä¸€æ¬¡**çº¿æ€§å˜æ¢**ï¼ŒæŠŠå®ƒä»"åŸå§‹åµŒå…¥ç©ºé—´"æŠ•å½±åˆ°"Query ç©ºé—´"ã€‚W_Q å°±åƒä¸€å‰¯"Query çœ¼é•œ"ï¼Œè®©åŒä¸€ä¸ªè¯ä»ä¸åŒè§’åº¦ï¼ˆQ/K/Vï¼‰è¢«"çœ‹åˆ°"ã€‚

åŒæ ·çš„ X ä¹˜ä»¥ä¸åŒçš„æƒé‡çŸ©é˜µ W_Kã€W_Vï¼Œå°±å¾—åˆ° Key å’Œ Valueâ€”â€”**åŒä¸€ä¸ªè¾“å…¥ï¼Œä¸‰ç§ä¸åŒçš„"è§†è§’"**ã€‚

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–è®©ä½ **ç‚¹å‡»ç»“æœçŸ©é˜µçš„ä»»æ„ä¸€ä¸ªæ ¼å­**ï¼Œå°±èƒ½é«˜äº®çœ‹åˆ°å®ƒæ˜¯ X çš„å“ªä¸€è¡Œå’Œ W_Q çš„å“ªä¸€åˆ—ç›¸ä¹˜å¾—åˆ°çš„ï¼Œè¿˜æœ‰é€æ­¥è®¡ç®—è¿‡ç¨‹çš„åŠ¨ç”»ã€‚

<IframeEmbed
  src="/visualizations/transformer-matrix-multiply.html"
  minHeight={860}
  title="çŸ©é˜µä¹˜æ³•å¯è§†åŒ–ï¼šX Ã— W_Q = Q"
/>

### 3.3 ç”¨ PyTorch nn.Linear çš„ä¼˜é›…å†™æ³•

```python
import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        # ä¸‰ä¸ªçº¿æ€§å±‚åˆ†åˆ«ç”Ÿæˆ Q, K, V
        self.W_Q = nn.Linear(d_model, d_model, bias=False)
        self.W_K = nn.Linear(d_model, d_model, bias=False)
        self.W_V = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x):
        Q = self.W_Q(x)   # [batch, seq_len, d_model]
        K = self.W_K(x)
        V = self.W_V(x)

        # è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_model ** 0.5)
        weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(weights, V)

        return output, weights

# æµ‹è¯•
attn = SelfAttention(d_model=64)
x = torch.randn(2, 10, 64)  # batch=2, seq_len=10, d_model=64
output, weights = attn(x)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")    # [2, 10, 64]
print(f"æƒé‡å½¢çŠ¶: {weights.shape}")    # [2, 10, 10]
```

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–å°† QÃ—K^Tâ†’ç¼©æ”¾â†’Softmaxâ†’Ã—V çš„æ¯ä¸€æ­¥ä»¥åŠ¨ç”»å½¢å¼å±•ç¤ºï¼Œä½ å¯ä»¥é€æ­¥æŸ¥çœ‹çŸ©é˜µè¿ç®—çš„è¿‡ç¨‹ã€‚

<IframeEmbed
  src="/visualizations/transformer-qkv-computation.html"
  minHeight={820}
  title="Q/K/V çŸ©é˜µè¿ç®—å¯è§†åŒ–"
/>

### 3.4 å¸¸è§å‘ç‚¹

> âš ï¸ **å‘ 1ï¼šQã€Kã€V çš„ç»´åº¦å¯ä»¥ä¸åŒ**
>
> è™½ç„¶åœ¨è‡ªæ³¨æ„åŠ›ä¸­ $d_q = d_k = d_v = d_{model}$ æ˜¯æœ€å¸¸è§çš„è®¾å®šï¼Œä½†å®é™…ä¸Š Q å’Œ K çš„ç»´åº¦å¿…é¡»ç›¸åŒï¼ˆå› ä¸ºè¦åšç‚¹ç§¯ï¼‰ï¼Œè€Œ V çš„ç»´åº¦å¯ä»¥ä¸åŒã€‚åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªå¤´çš„ $d_k = d_{model} / h$ã€‚

> âš ï¸ **å‘ 2ï¼šæ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µæ˜¯æ–¹é˜µ**
>
> åœ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼ŒQ å’Œ K æ¥è‡ªåŒä¸€ä¸ªåºåˆ—ï¼Œæ‰€ä»¥åˆ†æ•°çŸ©é˜µæ˜¯ `[seq_len, seq_len]` çš„æ–¹é˜µã€‚ä½†åœ¨**äº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰** ä¸­ï¼ŒQ æ¥è‡ª Decoderï¼ŒK/V æ¥è‡ª Encoderï¼Œåˆ†æ•°çŸ©é˜µæ˜¯ `[decoder_len, encoder_len]` çš„çŸ©å½¢ã€‚

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - Qã€Kã€V é€šè¿‡ä¸‰ä¸ªä¸åŒçš„çº¿æ€§å˜æ¢ä»åŒä¸€ä¸ªè¾“å…¥ X äº§ç”Ÿ
> - å®Œæ•´å…¬å¼ï¼š$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$
> - ç»´åº¦å˜åŒ–ï¼š`[seq, d] â†’ [seq, seq] â†’ [seq, d]`
> - Qã€K ç»´åº¦å¿…é¡»åŒ¹é…ï¼ŒV ç»´åº¦å¯ä»¥ä¸åŒ

---

## å››ã€ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼šä¸ºä»€ä¹ˆè¦é™¤ä»¥ âˆšd_k

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°äº†å…¬å¼é‡Œé‚£ä¸ª $\sqrt{d_k}$â€”â€”ä¸ºä»€ä¹ˆä¸ç›´æ¥ softmax(QK^T) å‘¢ï¼Ÿè¿™çœ‹èµ·æ¥æ˜¯ä¸ªå°ç»†èŠ‚ï¼Œä½†ç†è§£å®ƒå¯¹äºè®­ç»ƒç¨³å®šæ€§è‡³å…³é‡è¦ã€‚

### 4.1 ç›´è§‰ç±»æ¯”ï¼šéŸ³é‡è°ƒèŠ‚æ—‹é’® ğŸ”Š

æƒ³è±¡ä½ åœ¨ä¸€ä¸ªè€ƒåœºé‡Œï¼Œå­¦ç”Ÿä»¬åœ¨å°å£°è®¨è®ºç­”æ¡ˆï¼ˆç‚¹ç§¯åˆ†æ•°ï¼‰ã€‚Softmax å°±åƒä¸€ä¸ª"éŸ³é‡è°ƒèŠ‚å™¨"â€”â€”æŠŠè®¨è®ºå£°å˜æˆæ¸…æ™°çš„"è°è¯´å¾—å¯¹"çš„æ’åã€‚

å¦‚æœè®¨è®ºå£°å¾ˆå°ï¼ˆåˆ†æ•°å€¼é€‚ä¸­ï¼‰ï¼Œä½ è¿˜èƒ½å¬æ¸…æ¯ä¸ªäººéƒ½åœ¨è¯´ä»€ä¹ˆï¼ˆ**åˆ†å¸ƒå‡åŒ€**ï¼‰ã€‚

ä½†å¦‚æœè®¨è®ºå£°è¶Šæ¥è¶Šå¤§ï¼ˆç»´åº¦ $d_k$ å¢å¤§å¯¼è‡´ç‚¹ç§¯æ•°å€¼æš´æ¶¨ï¼‰ï¼Œå£°éŸ³æœ€å¤§çš„é‚£ä¸ªäººä¼š**å®Œå…¨ç›–è¿‡**å…¶ä»–æ‰€æœ‰äººï¼ˆ**softmax é¥±å’Œ**ï¼Œå˜æˆ one-hotï¼‰â€”â€”ä½ åªå¬åˆ°ä¸€ä¸ªäººçš„å£°éŸ³ï¼Œå…¶ä»–æ‰€æœ‰ä¿¡æ¯éƒ½ä¸¢å¤±äº†ï¼

$\sqrt{d_k}$ å°±æ˜¯é‚£ä¸ª**éŸ³é‡æ—‹é’®**ï¼šæŠŠéŸ³é‡è°ƒå›åˆç†èŒƒå›´ï¼Œè®©ä½ å¬æ¸…æ‰€æœ‰äººçš„æ„è§ã€‚

### 4.2 æ•°å­¦åŸç†ï¼šæ–¹å·®çˆ†ç‚¸

å‡è®¾ $Q$ å’Œ $K$ ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ç‹¬ç«‹çš„ã€å‡å€¼ä¸º 0ã€æ–¹å·®ä¸º 1 çš„éšæœºå˜é‡ï¼Œé‚£ä¹ˆå®ƒä»¬çš„ç‚¹ç§¯ï¼š

$$
Q \cdot K = \sum_{i=1}^{d_k} Q_i \cdot K_i
$$

æ ¹æ®æ¦‚ç‡è®ºï¼Œè¿™ä¸ªå’Œçš„**æ–¹å·®**ä¸º $d_k$ï¼ˆæ¯ä¸€é¡¹çš„æ–¹å·®æ˜¯ 1ï¼Œå…± $d_k$ é¡¹ï¼Œç‹¬ç«‹ç›¸åŠ æ–¹å·®çº¿æ€§å¢é•¿ï¼‰ã€‚

- å½“ $d_k = 64$ æ—¶ï¼Œç‚¹ç§¯çš„æ ‡å‡†å·®çº¦ä¸º $\sqrt{64} = 8$
- å½“ $d_k = 512$ æ—¶ï¼Œç‚¹ç§¯çš„æ ‡å‡†å·®çº¦ä¸º $\sqrt{512} \approx 22.6$

è¿™æ„å‘³ç€ï¼Œéšç€ç»´åº¦å¢å¤§ï¼Œç‚¹ç§¯çš„æ•°å€¼ä¼šè¶Šæ¥è¶Šå¤§â€”â€”**å¤§åˆ° softmax å‡ ä¹å˜æˆ one-hot åˆ†å¸ƒ**ã€‚

```python
import torch

d_k_values = [1, 4, 16, 64, 256, 512]

for d_k in d_k_values:
    q = torch.randn(1, d_k)
    k = torch.randn(5, d_k)

    # æœªç¼©æ”¾
    raw_scores = q @ k.T
    raw_probs = torch.softmax(raw_scores, dim=-1)

    # ç¼©æ”¾å
    scaled_scores = raw_scores / (d_k ** 0.5)
    scaled_probs = torch.softmax(scaled_scores, dim=-1)

    print(f"d_k={d_k:>3d} | æœªç¼©æ”¾æœ€å¤§æƒé‡: {raw_probs.max():.4f} | "
          f"ç¼©æ”¾åæœ€å¤§æƒé‡: {scaled_probs.max():.4f}")
```

è¿è¡Œç»“æœï¼ˆå¤§è‡´ï¼‰ï¼š

```
d_k=  1 | æœªç¼©æ”¾æœ€å¤§æƒé‡: 0.3500 | ç¼©æ”¾åæœ€å¤§æƒé‡: 0.3500
d_k=  4 | æœªç¼©æ”¾æœ€å¤§æƒé‡: 0.5200 | ç¼©æ”¾åæœ€å¤§æƒé‡: 0.3800
d_k= 16 | æœªç¼©æ”¾æœ€å¤§æƒé‡: 0.7800 | ç¼©æ”¾åæœ€å¤§æƒé‡: 0.3500
d_k= 64 | æœªç¼©æ”¾æœ€å¤§æƒé‡: 0.9500 | ç¼©æ”¾åæœ€å¤§æƒé‡: 0.3200
d_k=256 | æœªç¼©æ”¾æœ€å¤§æƒé‡: 0.9990 | ç¼©æ”¾åæœ€å¤§æƒé‡: 0.3600
d_k=512 | æœªç¼©æ”¾æœ€å¤§æƒé‡: 0.9999 | ç¼©æ”¾åæœ€å¤§æƒé‡: 0.3400
```

### 4.3 æ¢¯åº¦æ¶ˆå¤±ï¼šä¸ºä»€ä¹ˆ softmax é¥±å’Œæ˜¯è‡´å‘½çš„

å½“ softmax è¾“å‡ºæ¥è¿‘ one-hot åˆ†å¸ƒæ—¶ï¼Œ**æ¢¯åº¦å‡ ä¹ä¸ºé›¶**ã€‚å›å¿† softmax çš„æ¢¯åº¦ï¼š

$$
\frac{\partial \text{softmax}_i}{\partial z_j} = \text{softmax}_i (\delta_{ij} - \text{softmax}_j)
$$

å½“æŸä¸ª $\text{softmax}_i \approx 1$ æ—¶ï¼Œ$\text{softmax}_j \approx 0$ï¼ˆå¯¹æ‰€æœ‰ $j \neq i$ï¼‰ï¼Œäºæ˜¯æ¢¯åº¦æ¥è¿‘ $1 \times (1 - 1) = 0$ã€‚

ç»“æœå°±æ˜¯ï¼šæ¨¡å‹åœ¨è®­ç»ƒåˆæœŸå°±"åšä¿¡"æŸä¸ªè¯æœ€é‡è¦ï¼Œå†ä¹Ÿä¸æ›´æ–°â€”â€”å½»åº•ä¸§å¤±å­¦ä¹ èƒ½åŠ›ã€‚

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šæ‹–åŠ¨æ»‘å—è°ƒæ•´ç»´åº¦ d_kï¼Œç›´è§‚æ„Ÿå—ç¼©æ”¾å¯¹ softmax åˆ†å¸ƒçš„å½±å“â€”â€”çœ‹çœ‹ä¸ç¼©æ”¾æ—¶æ³¨æ„åŠ›æ˜¯å¦‚ä½•å˜æˆ"one-hot"çš„ã€‚

<IframeEmbed
  src="/visualizations/transformer-softmax-scaling.html"
  minHeight={760}
  title="Softmax ç¼©æ”¾æ•ˆæœå¯è§†åŒ–"
/>

### 4.4 æœ€ä½³å®è·µ

> ğŸ’¡ **æœ€ä½³å®è·µ 1ï¼šå§‹ç»ˆä½¿ç”¨ç¼©æ”¾**
>
> é™¤éä½ æœ‰éå¸¸å¥½çš„ç†ç”±ï¼ˆæ¯”å¦‚ d_k å¾ˆå°ï¼‰ï¼Œå¦åˆ™**æ°¸è¿œé™¤ä»¥ âˆšd_k**ã€‚è¿™æ˜¯è®ºæ–‡çš„é»˜è®¤è®¾å®šï¼Œä¹Ÿæ˜¯ PyTorch `nn.MultiheadAttention` çš„é»˜è®¤è¡Œä¸ºã€‚

> ğŸ’¡ **æœ€ä½³å®è·µ 2ï¼šåˆå§‹åŒ–æƒé‡æ—¶è€ƒè™‘ç»´åº¦**
>
> ä½¿ç”¨ Xavier/Glorot åˆå§‹åŒ–æˆ– Kaiming åˆå§‹åŒ–ï¼Œå®ƒä»¬ä¼šè‡ªåŠ¨æ ¹æ®ç»´åº¦è°ƒæ•´æƒé‡çš„æ–¹å·®ï¼Œé…åˆç¼©æ”¾ç‚¹ç§¯æ•ˆæœæ›´å¥½ã€‚

### 4.5 å¸¸è§å‘ç‚¹

> âš ï¸ **å‘ï¼šæ‰‹åŠ¨å®ç°æ—¶å¿˜äº†ç¼©æ”¾**
>
> å¾ˆå¤šåˆå­¦è€…åœ¨æ‰‹å†™ Attention æ—¶ç›´æ¥å†™ `softmax(Q @ K.T)`ï¼Œå¿˜äº†é™¤ä»¥ `sqrt(d_k)`ã€‚å°ç»´åº¦æ—¶é—®é¢˜ä¸å¤§ï¼Œä¸€æ—¦ d_k > 64 å°±ä¼šå‘ç°æ¨¡å‹å®Œå…¨ä¸æ”¶æ•›ã€‚

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - é™¤ä»¥ $\sqrt{d_k}$ æ˜¯ä¸ºäº†æ§åˆ¶ç‚¹ç§¯çš„æ–¹å·®ï¼Œé˜²æ­¢ softmax é¥±å’Œ
> - æ²¡æœ‰ç¼©æ”¾ï¼šd_k è¶Šå¤§ï¼Œsoftmax è¶Šæ¥è¿‘ one-hotï¼Œæ¢¯åº¦è¶Šæ¥è¿‘é›¶
> - ç¼©æ”¾åï¼šæ— è®º d_k å¤šå¤§ï¼Œsoftmax éƒ½ä¿æŒå¥åº·çš„æ¦‚ç‡åˆ†å¸ƒ
> - è¿™ä¸æ˜¯å¯é€‰çš„ä¼˜åŒ–ï¼Œè€Œæ˜¯**å¿…éœ€çš„**æ“ä½œ

---

## äº”ã€å¤šå¤´æ³¨æ„åŠ›ï¼šä¸€ä¸ªè„‘è¢‹æ€ä¹ˆå¤Ÿï¼Ÿ

ä¸€ä¸ªè‡ªæ³¨æ„åŠ›å¤´åªèƒ½å­¦åˆ°ä¸€ç§"å…³æ³¨æ¨¡å¼"ã€‚ä½†è¯­è¨€ä¸­çš„å…³ç³»æ˜¯å¤šç§å¤šæ ·çš„â€”â€”è¯­æ³•å…³ç³»ã€è¯­ä¹‰å…³ç³»ã€æŒ‡ä»£å…³ç³»ã€ä½ç½®å…³ç³»â€¦â€¦ä¸€ä¸ªå¤´æ€ä¹ˆèƒ½åŒæ—¶æ•è·æ‰€æœ‰è¿™äº›ï¼Ÿ

### 5.1 ç›´è§‰ç±»æ¯”ï¼šé˜…è¯»ç†è§£å°ç»„ ğŸ“–

æƒ³è±¡ä¸€ä¸ªé˜…è¯»ç†è§£å°ç»„ï¼Œæ¯ä¸ªäººè´Ÿè´£ä»**ä¸åŒè§’åº¦**åˆ†æä¸€æ®µæ–‡å­—ï¼š

- **å°æ˜**ï¼ˆå¤´ 1ï¼‰è´Ÿè´£æ‰¾**è¯­æ³•å…³ç³»**ï¼šä¸»è¯­-è°“è¯­-å®¾è¯­
- **å°çº¢**ï¼ˆå¤´ 2ï¼‰è´Ÿè´£æ‰¾**æŒ‡ä»£å…³ç³»**ï¼šä»£è¯æŒ‡ä»£ä»€ä¹ˆ
- **å°å**ï¼ˆå¤´ 3ï¼‰è´Ÿè´£æ‰¾**ç›¸é‚»å…³ç³»**ï¼šå‰åè¯çš„æ­é…
- **å°æ**ï¼ˆå¤´ 4ï¼‰è´Ÿè´£æ‰¾**è¯­ä¹‰å…³ç³»**ï¼šè¿‘ä¹‰è¯ã€åä¹‰è¯

æœ€åï¼Œä»–ä»¬æŠŠå„è‡ªçš„å‘ç°**æ±‡æ€»**ï¼Œå¾—åˆ°ä¸€ä»½å…¨é¢çš„åˆ†ææŠ¥å‘Šã€‚

è¿™å°±æ˜¯**å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šæŠŠæ³¨æ„åŠ›æœºåˆ¶å¤åˆ¶å¤šä»½ï¼Œæ¯ä»½å…³æ³¨ä¸åŒç±»å‹çš„å…³ç³»ï¼Œæœ€åæŠŠç»“æœæ‹¼æ¥èµ·æ¥ï¼‰çš„æ ¸å¿ƒæ€æƒ³ã€‚

### 5.2 æŠ€æœ¯å®ç°ï¼šåˆ‡åˆ†ã€å¹¶è¡Œã€æ‹¼æ¥

å…·ä½“æ¥è¯´ï¼Œå¤šå¤´æ³¨æ„åŠ›åšäº†ä¸‰ä»¶äº‹ï¼š

**Step 1ï¼šåˆ‡åˆ†**

æŠŠè¾“å…¥çš„åµŒå…¥ç»´åº¦ $d_{model}$ å‡åŒ€åˆ‡åˆ†æˆ $h$ ä»½ï¼ˆ$h$ = å¤´çš„æ•°é‡ï¼‰ï¼š

$$
d_k = d_v = d_{model} / h
$$

æ¯”å¦‚ $d_{model} = 512$, $h = 8$ï¼Œåˆ™æ¯ä¸ªå¤´å¤„ç† $d_k = 64$ ç»´çš„å­ç©ºé—´ã€‚

**Step 2ï¼šå¹¶è¡Œè®¡ç®—**

æ¯ä¸ªå¤´**ç‹¬ç«‹**åšä¸€æ¬¡å®Œæ•´çš„æ³¨æ„åŠ›è®¡ç®—ï¼š

$$
\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
$$

**Step 3ï¼šæ‹¼æ¥ + æŠ•å½±**

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

$W^O$ æ˜¯ä¸€ä¸ª **è¾“å‡ºæŠ•å½±çŸ©é˜µ**ï¼ˆç™½è¯ç‰ˆï¼šæŠŠæ‹¼æ¥åçš„é•¿å‘é‡å‹ç¼©å›åŸå§‹ç»´åº¦ $d_{model}$ï¼‰ã€‚

### 5.3 ä»£ç å®ç°

```python
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0, "d_model å¿…é¡»èƒ½è¢« n_heads æ•´é™¤"

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # å››ä¸ªæŠ•å½±çŸ©é˜µ
        self.W_Q = nn.Linear(d_model, d_model, bias=False)
        self.W_K = nn.Linear(d_model, d_model, bias=False)
        self.W_V = nn.Linear(d_model, d_model, bias=False)
        self.W_O = nn.Linear(d_model, d_model, bias=False)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # 1. çº¿æ€§æŠ•å½±
        Q = self.W_Q(Q)  # [batch, seq_len, d_model]
        K = self.W_K(K)
        V = self.W_V(V)

        # 2. åˆ‡åˆ†æˆå¤šä¸ªå¤´: [batch, seq, d_model] â†’ [batch, n_heads, seq, d_k]
        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)

        # 3. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attention_weights = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, V)

        # 4. æ‹¼æ¥æ‰€æœ‰å¤´: [batch, n_heads, seq, d_k] â†’ [batch, seq, d_model]
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # 5. è¾“å‡ºæŠ•å½±
        output = self.W_O(context)

        return output, attention_weights

# æµ‹è¯•
mha = MultiHeadAttention(d_model=512, n_heads=8)
x = torch.randn(2, 10, 512)  # batch=2, seq_len=10
output, weights = mha(x, x, x)  # è‡ªæ³¨æ„åŠ›ï¼šQ=K=V=x
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")     # [2, 10, 512]
print(f"æƒé‡å½¢çŠ¶: {weights.shape}")     # [2, 8, 10, 10] â€” 8 ä¸ªå¤´çš„æ³¨æ„åŠ›çŸ©é˜µ
```

> ğŸ’¡ **å…³é”®æ´å¯Ÿï¼šå‚æ•°é‡å®Œå…¨ç›¸åŒï¼**
>
> ä¸€ä¸ªå®¹æ˜“æ··æ·†çš„ç‚¹ï¼šå¤šå¤´æ³¨æ„åŠ›å’Œå•å¤´æ³¨æ„åŠ›çš„**å‚æ•°é‡æ˜¯ä¸€æ ·çš„**ï¼
>
> | æ–¹å¼          | Q æŠ•å½±  | K æŠ•å½±  | V æŠ•å½±  | æ€»è®¡ |
> | ------------- | ------- | ------- | ------- | ---- |
> | å•å¤´ (d=512)  | 512Ã—512 | 512Ã—512 | 512Ã—512 | 786K |
> | 8 å¤´ (d_k=64) | 512Ã—512 | 512Ã—512 | 512Ã—512 | 786K |
>
> åŒºåˆ«åœ¨äºå¤šå¤´ä¼šå¤šä¸€ä¸ª $W^O$ï¼ˆ512Ã—512ï¼‰ï¼Œä½†è¿™åªå¢åŠ äº† 25% çš„å‚æ•°ã€‚æ ¸å¿ƒåŸå› æ˜¯ï¼šå¤šå¤´å¹¶ä¸æ˜¯æŠŠå‚æ•°é‡ç¿» h å€ï¼Œè€Œæ˜¯æŠŠåŒä¸€ç»´åº¦çš„ç©ºé—´**åˆ‡åˆ†**æˆ h ä¸ªå­ç©ºé—´ã€‚

### 5.4 GPT-3 æœ‰ 96 ä¸ªå¤´ï¼

å®é™…çš„å¤§æ¨¡å‹ä½¿ç”¨çš„å¤´æ•°è¿œæ¯” 8 å¤šï¼š

| æ¨¡å‹       | d_model | n_heads | d_k |
| ---------- | ------- | ------- | --- |
| BERT-base  | 768     | 12      | 64  |
| BERT-large | 1024    | 16      | 64  |
| GPT-2      | 768     | 12      | 64  |
| GPT-3      | 12288   | 96      | 128 |
| LLaMA-7B   | 4096    | 32      | 128 |

ç ”ç©¶è¡¨æ˜ä¸åŒçš„å¤´ç¡®å®ä¼šå­¦åˆ°ä¸åŒçš„"æŠ€èƒ½"â€”â€”æœ‰çš„æ“…é•¿è¯­æ³•åˆ†æï¼Œæœ‰çš„æ“…é•¿æŒ‡ä»£æ¶ˆè§£ï¼Œæœ‰çš„åªå…³æ³¨ç›¸é‚»çš„è¯ã€‚

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–å±•ç¤ºäº†å¤šå¤´æ³¨æ„åŠ›çš„å·¥ä½œè¿‡ç¨‹â€”â€”æ¯ä¸ªå¤´ç”¨ä¸åŒé¢œè‰²è¡¨ç¤ºä¸åŒçš„å…³æ³¨æ¨¡å¼ï¼Œä½ å¯ä»¥åˆ‡æ¢æŸ¥çœ‹å„ä¸ªå¤´åˆ†åˆ«å­¦åˆ°äº†ä»€ä¹ˆã€‚

<IframeEmbed
  src="/visualizations/transformer-multi-head.html"
  minHeight={820}
  title="å¤šå¤´æ³¨æ„åŠ›å¯è§†åŒ–"
/>

### 5.5 å¸¸è§å‘ç‚¹

> âš ï¸ **å‘ 1ï¼šd_model ä¸èƒ½è¢« n_heads æ•´é™¤**
>
> å¦‚æœ d_model=100ï¼Œn_heads=8ï¼Œé‚£ 100/8=12.5â€”â€”æ— æ³•å‡åŒ€åˆ‡åˆ†ï¼æ‰€ä»¥ d_model å¿…é¡»æ˜¯ n_heads çš„æ•´æ•°å€ã€‚å¸¸è§ç»„åˆï¼š512/8ã€768/12ã€1024/16ã€‚

> âš ï¸ **å‘ 2ï¼šå¿˜äº† transpose å’Œ contiguous**
>
> å¤šå¤´çš„ view/reshape æ“ä½œéœ€è¦å…ˆ transpose å† contiguousã€‚å¦‚æœä¸åŠ  `.contiguous()`ï¼Œåé¢çš„ `.view()` ä¼šæŠ¥é”™ "view size is not compatible with input tensor's size and stride"ã€‚

> âš ï¸ **å‘ 3ï¼šä»¥ä¸ºæ¯ä¸ªå¤´æœ‰ç‹¬ç«‹çš„ W_Q/W_K/W_V**
>
> å®é™…å®ç°ä¸­ï¼Œæˆ‘ä»¬åªæœ‰**ä¸€ä¸ª** `nn.Linear(d_model, d_model)` ä½œä¸º W_Qï¼Œç„¶åé€šè¿‡ view/reshape æŠŠè¾“å‡ºåˆ‡åˆ†æˆå¤šä¸ªå¤´ã€‚ä¸æ˜¯æ¯ä¸ªå¤´ä¸€ä¸ª `nn.Linear(d_model, d_k)`â€”â€”é‚£æ ·å‚æ•°é‡å°±çœŸçš„ç¿»å€äº†ã€‚

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - å¤šå¤´æ³¨æ„åŠ› = æŠŠåµŒå…¥ç©ºé—´åˆ‡åˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹åšæ³¨æ„åŠ›
> - æ­¥éª¤ï¼šæŠ•å½± â†’ åˆ‡åˆ† â†’ å¹¶è¡Œæ³¨æ„åŠ› â†’ æ‹¼æ¥ â†’ è¾“å‡ºæŠ•å½±
> - å‚æ•°é‡ä¸å•å¤´ç›¸å½“ï¼Œä½†ä¿¡æ¯æå–èƒ½åŠ›æ›´å¼º
> - ä¸åŒçš„å¤´è‡ªåŠ¨å­¦ä¼šå…³æ³¨ä¸åŒç±»å‹çš„å…³ç³»

---

## å…­ã€ä½ç½®ç¼–ç ï¼šè®©æ¨¡å‹çŸ¥é“"è°åœ¨å‰è°åœ¨å"

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å¯èƒ½å·²ç»å‘ç°äº†ä¸€ä¸ªé—®é¢˜ï¼šæ³¨æ„åŠ›æœºåˆ¶æ˜¯**å¯¹ç§°çš„**â€”â€”å®ƒä¸å…³å¿ƒè¯çš„é¡ºåºï¼

### 6.1 ç›´è§‰ï¼šæ‰“ä¹±é¡ºåºçš„ç¾éš¾ ğŸ”€

```
"çŒ« è¿½ ç‹—"  â†’  çŒ«åœ¨è¿½ç‹—
"ç‹— è¿½ çŒ«"  â†’  ç‹—åœ¨è¿½çŒ«
```

è¿™ä¸¤å¥è¯æ„æ€å®Œå…¨ç›¸åï¼ä½†å¯¹äºè‡ªæ³¨æ„åŠ›æ¥è¯´ï¼Œå®ƒåªå…³å¿ƒ"æ¯ä¸ªè¯å’Œå…¶ä»–è¯çš„å…³ç³»"ï¼Œ**å®Œå…¨ä¸çŸ¥é“è°åœ¨å‰é¢ã€è°åœ¨åé¢**ã€‚

å› ä¸ºæ³¨æ„åŠ›çš„è®¡ç®—å…¬å¼ $\text{softmax}(QK^T)V$ ä¸­ï¼Œ$Q$ã€$K$ã€$V$ æ˜¯é€šè¿‡**é€å…ƒç´ **çš„çº¿æ€§å˜æ¢å¾—åˆ°çš„ï¼Œä¸åŒ…å«ä»»ä½•ä½ç½®ä¿¡æ¯ã€‚ä½ æ‰“ä¹±è¾“å…¥è¯çš„é¡ºåºï¼Œæ¯ä¸ªè¯å¾—åˆ°çš„ Q/K/V å‘é‡**å®Œå…¨ä¸å˜**ï¼Œåªæ˜¯æ³¨æ„åŠ›çŸ©é˜µçš„è¡Œåˆ—é¡ºåºå˜äº†â€”â€”ä½† softmax æ˜¯å¯¹ç§°çš„ï¼Œæ‰€ä»¥ç»“æœä¸­çš„ä¿¡æ¯ä¹Ÿä¸å˜ã€‚

> ğŸ’¡ **ç™½è¯ç‰ˆ**ï¼šè‡ªæ³¨æ„åŠ›å°±åƒä¸€ç¾¤äººå›´ååœ¨åœ†æ¡Œæ—å¼€ä¼šâ€”â€”å®ƒèƒ½å¬æ‡‚æ¯ä¸ªäººåœ¨è¯´ä»€ä¹ˆï¼Œä½†ä¸çŸ¥é“è°ååœ¨è°æ—è¾¹ã€‚

### 6.2 è§£å†³æ–¹æ¡ˆï¼šç»™æ¯ä¸ªä½ç½®ä¸€ä¸ª"èº«ä»½è¯"

**ä½ç½®ç¼–ç ï¼ˆPositional Encoding, PEï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šç»™åºåˆ—ä¸­æ¯ä¸ªä½ç½®åˆ†é…ä¸€ä¸ªç‹¬ç‰¹çš„æ•°å­—å‘é‡ï¼ŒåŠ åˆ°è¯åµŒå…¥ä¸Šï¼Œè®©æ¨¡å‹çŸ¥é“æ¯ä¸ªè¯çš„ä½ç½®ï¼‰çš„æ€æƒ³å¾ˆç®€å•ï¼šåœ¨è¾“å…¥åµŒå…¥ä¸Š**åŠ ä¸Š**ä¸€ä¸ªå’Œä½ç½®ç›¸å…³çš„å‘é‡ã€‚

$$
\text{Input} = \text{TokenEmbedding}(x) + \text{PositionalEncoding}(pos)
$$

ä½†æ€ä¹ˆè®¾è®¡è¿™ä¸ªä½ç½®å‘é‡å‘¢ï¼Ÿ

### 6.3 å¤±è´¥çš„å°è¯•ï¼šçº¿æ€§ç¼–ç 

æœ€ç®€å•çš„æƒ³æ³•ï¼šä½ç½® 0 ç¼–ç ä¸º 0ï¼Œä½ç½® 1 ç¼–ç ä¸º 1ï¼Œä½ç½® 2 ç¼–ç ä¸º 2â€¦â€¦

```python
# æ–¹æ¡ˆ 1ï¼šçº¿æ€§ç¼–ç ï¼ˆæœ‰é—®é¢˜ï¼ï¼‰
pe = torch.arange(0, seq_len).float()
# ä½ç½® 0: [0, 0, 0, ...]
# ä½ç½® 1: [1, 1, 1, ...]
# ä½ç½® 100: [100, 100, 100, ...]
```

**é—®é¢˜**ï¼šä½ç½® 100 çš„ç¼–ç å€¼æ˜¯ä½ç½® 1 çš„ 100 å€ï¼æ•°å€¼èŒƒå›´å·®å¼‚å·¨å¤§ï¼Œä¼šä¸¥é‡å¹²æ‰°è¯åµŒå…¥çš„ä¿¡æ¯ã€‚è€Œä¸”æ¨¡å‹æ— æ³•æ¨å¹¿åˆ°è®­ç»ƒæ—¶æ²¡è§è¿‡çš„æ›´é•¿åºåˆ—ã€‚

### 6.4 æ­£å¼¦ä½™å¼¦ç¼–ç ï¼šTransformer çš„ä¼˜é›…æ–¹æ¡ˆ

è®ºæ–‡ä¸­ä½¿ç”¨çš„æ–¹æ¡ˆéå¸¸å·§å¦™â€”â€”ç”¨ä¸åŒé¢‘ç‡çš„ **æ­£å¼¦ï¼ˆsinï¼‰** å’Œ **ä½™å¼¦ï¼ˆcosï¼‰** å‡½æ•°ï¼š

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

å…¶ä¸­ $pos$ æ˜¯ä½ç½®ç´¢å¼•ï¼Œ$i$ æ˜¯ç»´åº¦ç´¢å¼•ã€‚

**ä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡ï¼Ÿä¸‰å¤§ä¼˜ç‚¹ï¼š**

**â‘  æœ‰ç•Œæ€§**ï¼šsin å’Œ cos çš„å€¼æ°¸è¿œåœ¨ $[-1, 1]$ ä¹‹é—´ï¼Œä¸ä¼šåƒçº¿æ€§ç¼–ç é‚£æ ·æ•°å€¼çˆ†ç‚¸ã€‚

**â‘¡ å”¯ä¸€æ€§**ï¼šä¸åŒä½ç½®çš„ç¼–ç æ˜¯**å”¯ä¸€**çš„ã€‚å› ä¸ºä¸åŒç»´åº¦ä½¿ç”¨ä¸åŒé¢‘ç‡â€”â€”ä½ç»´åº¦ç”¨é«˜é¢‘ï¼ˆå˜åŒ–å¿«ï¼‰ï¼Œé«˜ç»´åº¦ç”¨ä½é¢‘ï¼ˆå˜åŒ–æ…¢ï¼‰ï¼Œå°±åƒé’Ÿè¡¨çš„ç§’é’ˆã€åˆ†é’ˆã€æ—¶é’ˆä¸€æ ·ï¼Œç»„åˆèµ·æ¥å¯ä»¥å”¯ä¸€è¡¨ç¤ºä»»ä½•æ—¶åˆ»ã€‚

**â‘¢ ç›¸å¯¹è·ç¦»å¯å­¦ä¹ **ï¼šå¯¹äºä»»æ„å›ºå®šåç§» $k$ï¼Œ$PE_{pos+k}$ å¯ä»¥è¡¨ç¤ºä¸º $PE_{pos}$ çš„çº¿æ€§å˜æ¢ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥é€šè¿‡å­¦ä¹ ç®€å•çš„çº¿æ€§å…³ç³»æ¥æ•è·**ç›¸å¯¹ä½ç½®**ä¿¡æ¯ã€‚

### 6.5 ä»£ç å®ç°

```python
import torch
import math

class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        # é¢‘ç‡é€’å‡ï¼šä½ç»´åº¦é«˜é¢‘ï¼Œé«˜ç»´åº¦ä½é¢‘
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )

        pe[:, 0::2] = torch.sin(position * div_term)   # å¶æ•°ç»´åº¦ç”¨ sin
        pe[:, 1::2] = torch.cos(position * div_term)    # å¥‡æ•°ç»´åº¦ç”¨ cos

        pe = pe.unsqueeze(0)   # å¢åŠ  batch ç»´åº¦: [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]

# ä½¿ç”¨ç¤ºä¾‹
d_model = 128
pe = PositionalEncoding(d_model)
x = torch.randn(2, 20, d_model)  # 2 ä¸ªå¥å­ï¼Œæ¯å¥ 20 ä¸ªè¯
output = pe(x)
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")     # [2, 20, 128]
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}") # [2, 20, 128] â€” åŠ äº†ä½ç½®ç¼–ç ä½†å½¢çŠ¶ä¸å˜
```

> ğŸ’¡ **ç†è§£é¢‘ç‡é€’å‡**
>
> ```
> ç»´åº¦ 0-1 (i=0):  é¢‘ç‡ = 1/10000^0 = 1       â†’ å˜åŒ–æœ€å¿«ï¼ˆç§’é’ˆï¼‰
> ç»´åº¦ 2-3 (i=1):  é¢‘ç‡ = 1/10000^(2/d) â‰ˆ 0.98 â†’ ç¨æ…¢
> ...
> ç»´åº¦ d-2, d-1:   é¢‘ç‡ = 1/10000^1 = 0.0001  â†’ å˜åŒ–æœ€æ…¢ï¼ˆæ—¶é’ˆï¼‰
> ```
>
> è¿™ç§"å¤šå°ºåº¦"çš„è®¾è®¡è®©æ¨¡å‹æ—¢èƒ½æ•è·**è¿‘è·ç¦»**çš„ä½ç½®å…³ç³»ï¼ˆé€šè¿‡é«˜é¢‘ç»´åº¦ï¼‰ï¼Œä¹Ÿèƒ½æ•è·**è¿œè·ç¦»**çš„ä½ç½®å…³ç³»ï¼ˆé€šè¿‡ä½é¢‘ç»´åº¦ï¼‰ã€‚

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–å±•ç¤ºäº†ä½ç½®ç¼–ç çš„çƒ­åŠ›å›¾â€”â€”æ‹–åŠ¨æ»‘å—è°ƒæ•´ç»´åº¦å’Œåºåˆ—é•¿åº¦ï¼Œè§‚å¯Ÿ sin/cos å‡½æ•°å¦‚ä½•ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆå”¯ä¸€ç¼–ç ã€‚

<IframeEmbed
  src="/visualizations/transformer-positional-encoding.html"
  minHeight={800}
  title="ä½ç½®ç¼–ç å¯è§†åŒ–"
/>

### 6.6 RoPEï¼šæ›´ç°ä»£çš„ä½ç½®ç¼–ç 

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œç°ä»£å¤§æ¨¡å‹ï¼ˆå¦‚ LLaMAã€GPT-Neoï¼‰æ™®éä½¿ç”¨ **RoPEï¼ˆRotary Position Embeddingï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šé€šè¿‡æ—‹è½¬å‘é‡çš„æ–¹å¼ç¼–ç ç›¸å¯¹ä½ç½®ï¼Œè®©æ³¨æ„åŠ›åˆ†æ•°è‡ªç„¶åŒ…å«è·ç¦»ä¿¡æ¯ï¼‰è€ŒéåŸå§‹çš„ sin/cos ç¼–ç ã€‚RoPE çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯èƒ½æ›´å¥½åœ°å¤–æ¨åˆ°è®­ç»ƒæ—¶æ²¡è§è¿‡çš„é•¿åºåˆ—ã€‚

### 6.7 å¸¸è§å‘ç‚¹

> âš ï¸ **å‘ 1ï¼šä½ç½®ç¼–ç æ˜¯åŠ æ³•ï¼Œä¸æ˜¯æ‹¼æ¥**
>
> æ­£ç¡®åšæ³•æ˜¯ `embedding + PE`ï¼Œä¸æ˜¯ `concat(embedding, PE)`ã€‚åŠ æ³•ä¸æ”¹å˜ç»´åº¦ï¼Œæ‹¼æ¥ä¼šæŠŠç»´åº¦ç¿»å€ã€‚

> âš ï¸ **å‘ 2ï¼šå¿˜äº† register_buffer**
>
> ä½ç½®ç¼–ç ä¸éœ€è¦æ¢¯åº¦ï¼ˆå®ƒæ˜¯å›ºå®šçš„ï¼‰ï¼Œæ‰€ä»¥åº”è¯¥ç”¨ `self.register_buffer('pe', pe)` è€Œä¸æ˜¯ `self.pe = nn.Parameter(pe)`ã€‚register_buffer è®©å®ƒè·Ÿéšæ¨¡å‹ä¸€èµ·ç§»åŠ¨åˆ° GPUï¼Œä½†ä¸å‚ä¸æ¢¯åº¦æ›´æ–°ã€‚

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - è‡ªæ³¨æ„åŠ›ä¸æ„ŸçŸ¥ä½ç½®ï¼Œéœ€è¦é¢å¤–çš„ä½ç½®ç¼–ç 
> - sin/cos ç¼–ç çš„ä¸‰å¤§ä¼˜ç‚¹ï¼šæœ‰ç•Œã€å”¯ä¸€ã€å¯å­¦ä¹ ç›¸å¯¹è·ç¦»
> - ä½ç»´åº¦é«˜é¢‘ï¼ˆæ•è·è¿‘è·ç¦»ï¼‰ï¼Œé«˜ç»´åº¦ä½é¢‘ï¼ˆæ•è·è¿œè·ç¦»ï¼‰
> - ä½ç½®ç¼–ç é€šè¿‡**åŠ æ³•**èå…¥è¯åµŒå…¥ï¼Œä¸æ”¹å˜ç»´åº¦

---

## ä¸ƒã€Encoder æ¶æ„ï¼šé˜…è¯»ç†è§£ä¸“å®¶

æœ‰äº†å‰é¢çš„ç»„ä»¶â€”â€”å¤šå¤´æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç â€”â€”ç°åœ¨æ˜¯æ—¶å€™æŠŠå®ƒä»¬ç»„è£…æˆå®Œæ•´çš„ **Encoderï¼ˆç¼–ç å™¨ï¼‰** äº†ã€‚

### 7.1 ç›´è§‰ç±»æ¯”ï¼šä¸€ä¸ªè¶Šæ¥è¶Šæ‡‚ä½ çš„é˜…è¯»ç†è§£ä¸“å®¶ ğŸ“š

Encoder å°±åƒä¸€ä¸ªé˜…è¯»ç†è§£ä¸“å®¶ï¼Œå®ƒçš„å·¥ä½œæµç¨‹æ˜¯ï¼š

1. **é˜…è¯»å…¨æ–‡**ï¼ˆSelf-Attentionï¼‰ï¼šçœ‹å®Œæ‰€æœ‰è¯ï¼Œç†è§£è¯ä¸è¯ä¹‹é—´çš„å…³ç³»
2. **åšç¬”è®°**ï¼ˆFeed-Forward Networkï¼‰ï¼šå¯¹æ¯ä¸ªè¯çš„ç†è§£åšæ·±å…¥åŠ å·¥
3. **åå¤ç²¾è¯»**ï¼ˆå †å å¤šå±‚ï¼‰ï¼šæ¯ä¸€å±‚éƒ½åœ¨ä¸Šä¸€å±‚ç†è§£çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ·±åŒ–

### 7.2 Encoder Layer çš„å››ä¸ªç»„ä»¶

ä¸€ä¸ª Encoder Layer åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œï¼š

```
è¾“å…¥ x
  â†“
[Multi-Head Self-Attention]  â† å…¨å±€ä¿¡æ¯äº¤äº’
  â†“
[Add & Norm]                 â† æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
  â†“
[Feed-Forward Network]       â† é€ä½ç½®æ·±åº¦åŠ å·¥
  â†“
[Add & Norm]                 â† æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
  â†“
è¾“å‡º
```

**ç»„ä»¶ â‘  Multi-Head Self-Attention**

å·²ç»åœ¨ç¬¬äº”èŠ‚è¯¦ç»†è®²è¿‡äº†â€”â€”è®©æ¯ä¸ªè¯çœ‹åˆ°æ‰€æœ‰å…¶ä»–è¯ã€‚

**ç»„ä»¶ â‘¡ Add & Normï¼ˆæ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–ï¼‰**

**æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šæŠŠè¾“å…¥ç›´æ¥"è·³è¿‡"æŸä¸ªå­å±‚ï¼Œå’Œå­å±‚çš„è¾“å‡ºç›¸åŠ ã€‚è¿™æ ·å³ä½¿å­å±‚å­¦ä¸å¥½ï¼Œä¿¡å·ä¹Ÿèƒ½é€šè¿‡"è·³çº¿"ç›´æ¥ä¼ è¿‡å»ï¼Œé˜²æ­¢æ·±å±‚ç½‘ç»œé€€åŒ–ï¼‰ï¼š

$$
\text{output} = \text{LayerNorm}(x + \text{SubLayer}(x))
$$

**å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalization, LayerNormï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾åšå½’ä¸€åŒ–â€”â€”å‡å»å‡å€¼ã€é™¤ä»¥æ ‡å‡†å·®â€”â€”è®©æ•°å€¼ä¿æŒåœ¨åˆç†èŒƒå›´å†…ï¼ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼‰ï¼š

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sigma + \epsilon} + \beta
$$

> ğŸ’¡ **ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥ï¼Ÿ**
>
> Transformer é€šå¸¸å †å  6-96 å±‚ã€‚å¦‚æœæ²¡æœ‰æ®‹å·®è¿æ¥ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶éœ€è¦ç»è¿‡æ¯ä¸€å±‚çš„å˜æ¢ï¼Œå¾ˆå®¹æ˜“æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚æ®‹å·®è¿æ¥æä¾›äº†ä¸€æ¡"é«˜é€Ÿå…¬è·¯"ï¼Œè®©æ¢¯åº¦å¯ä»¥ç›´æ¥è·³è¿‡ä»»æ„å¤šå±‚ï¼Œä¿æŒä¿¡å·å¼ºåº¦ã€‚

**ç»„ä»¶ â‘¢ Feed-Forward Network (FFN)**

**å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä¸¤ä¸ªçº¿æ€§å±‚ä¸­é—´å¤¹ä¸€ä¸ª ReLU æ¿€æ´»å‡½æ•°ï¼Œå¯¹æ¯ä¸ªä½ç½®**ç‹¬ç«‹**åšå˜æ¢â€”â€”ä¸æ¶‰åŠè¯ä¸è¯çš„äº¤äº’ï¼Œè€Œæ˜¯å¯¹å•ä¸ªè¯çš„è¡¨ç¤ºåšæ·±åº¦åŠ å·¥ï¼‰ï¼š

$$
\text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2
$$

ä¸€ä¸ªæœ‰è¶£çš„è®¾è®¡ï¼šFFN çš„éšè—å±‚ç»´åº¦ $d_{ff}$ é€šå¸¸æ˜¯ $d_{model}$ çš„ **4 å€**ã€‚æ¯”å¦‚ $d_{model} = 512$ æ—¶ $d_{ff} = 2048$ã€‚è¿™ä¸ª"å…ˆæ‰©å±•å†å‹ç¼©"çš„ç“¶é¢ˆç»“æ„æ˜¯éå¸¸æœ‰æ•ˆçš„ä¿¡æ¯åŠ å·¥æ¨¡å¼ã€‚

### 7.3 ä»£ç å®ç°

```python
import torch
import torch.nn as nn
import math

class FeedForward(nn.Module):
    """å‰é¦ˆç½‘ç»œï¼šä¸¤å±‚çº¿æ€§å˜æ¢ + ReLU"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.linear2(self.dropout(self.relu(self.linear1(x))))


class EncoderLayer(nn.Module):
    """å•ä¸ª Encoder å±‚"""
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # å­å±‚ 1: Multi-Head Self-Attention + Add & Norm
        attn_output, _ = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout1(attn_output))  # æ®‹å·® + å½’ä¸€åŒ–

        # å­å±‚ 2: Feed-Forward + Add & Norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout2(ff_output))     # æ®‹å·® + å½’ä¸€åŒ–

        return x


class Encoder(nn.Module):
    """å®Œæ•´ Encoderï¼šè¯åµŒå…¥ + ä½ç½®ç¼–ç  + N ä¸ª Encoder å±‚"""
    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, max_len=5000, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # è¯åµŒå…¥ + ç¼©æ”¾ + ä½ç½®ç¼–ç 
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        # é€šè¿‡ N ä¸ª Encoder å±‚
        for layer in self.layers:
            x = layer(x, mask)

        return x

# æµ‹è¯•
encoder = Encoder(
    vocab_size=10000, d_model=512, n_heads=8,
    d_ff=2048, n_layers=6
)
src = torch.randint(0, 10000, (2, 20))  # batch=2, seq_len=20
output = encoder(src)
print(f"Encoder è¾“å‡º: {output.shape}")  # [2, 20, 512]
```

> ğŸ’¡ **æ³¨æ„ `* math.sqrt(d_model)` è¿™ä¸€æ­¥**
>
> åµŒå…¥å‘é‡é€šå¸¸åˆå§‹åŒ–ä¸ºå¾ˆå°çš„å€¼ï¼ˆå‡å€¼ 0ï¼Œæ–¹å·® $1/d_{model}$ï¼‰ï¼Œè€Œä½ç½®ç¼–ç çš„å€¼åœ¨ $[-1, 1]$ èŒƒå›´ã€‚ä¸ºäº†é˜²æ­¢ä½ç½®ç¼–ç çš„ä¿¡å·å‹è¿‡åµŒå…¥çš„ä¿¡å·ï¼Œæˆ‘ä»¬æŠŠåµŒå…¥ä¹˜ä»¥ $\sqrt{d_{model}}$ æ¥æ”¾å¤§å®ƒã€‚

### 7.4 Encoder å°ç»“

```
å®Œæ•´ Encoder æ•°æ®æµ:

Token IDs  â†’  Embedding  â†’  Ã— âˆšd_model  â†’  + PositionalEncoding
                                                      â†“
                                              EncoderLayer Ã— N
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚ Self-Attention   â”‚
                                              â”‚ Add & Norm       â”‚
                                              â”‚ Feed-Forward     â”‚
                                              â”‚ Add & Norm       â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â†“
                                              Encoder Output
                                          [batch, seq_len, d_model]
```

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - Encoder = è¯åµŒå…¥ + ä½ç½®ç¼–ç  + N ä¸ªç›¸åŒç»“æ„çš„å±‚
> - æ¯å±‚ï¼šSelf-Attention â†’ Add&Norm â†’ FFN â†’ Add&Norm
> - æ®‹å·®è¿æ¥ä¿è¯æ¢¯åº¦æµåŠ¨ï¼ŒLayerNorm ç¨³å®šè®­ç»ƒ
> - FFN çš„éšè—ç»´åº¦é€šå¸¸æ˜¯ d_model çš„ 4 å€
> - Encoder å¯ä»¥çœ‹åˆ°å…¨éƒ¨è¾“å…¥ï¼ˆåŒå‘æ³¨æ„åŠ›ï¼‰

---

## å…«ã€Decoder æ¶æ„ï¼šä¸¥ç¦å·çœ‹çš„å†™ä½œä¸“å®¶

å¦‚æœ Encoder æ˜¯é˜…è¯»ç†è§£ä¸“å®¶ï¼Œé‚£ Decoder å°±æ˜¯å†™ä½œä¸“å®¶â€”â€”å®ƒçš„ä»»åŠ¡æ˜¯**é€è¯ç”Ÿæˆ**è¾“å‡ºåºåˆ—ã€‚ä½†å®ƒæœ‰ä¸€ä¸ªä¸¥æ ¼çš„è§„çŸ©ï¼š**å†™åˆ°ç¬¬ n ä¸ªè¯æ—¶ï¼Œç»å¯¹ä¸èƒ½å·çœ‹ç¬¬ n+1 ä¸ªè¯ä»¥åçš„å†…å®¹**ã€‚

### 8.1 ç›´è§‰ç±»æ¯”ï¼šè€ƒè¯•ä¸­çš„ä½œæ–‡é¢˜ âœï¸

æƒ³è±¡ä½ åœ¨å†™ä¸€ç¯‡è‹±è¯­ä½œæ–‡ï¼Œè€ƒè¯•è§„åˆ™æ˜¯ï¼š

1. **ä¸èƒ½å·çœ‹ç­”æ¡ˆ**ï¼ˆMasked Self-Attentionï¼‰ï¼šä½ åªèƒ½åŸºäºå·²ç»å†™ä¸‹çš„å†…å®¹æ¥å†³å®šä¸‹ä¸€ä¸ªè¯
2. **å¯ä»¥å‚è€ƒé˜…è¯»ææ–™**ï¼ˆCross-Attentionï¼‰ï¼šä½ å¯ä»¥å›å¤´ç¿»çœ‹é˜…è¯»ç†è§£éƒ¨åˆ†ï¼ˆEncoder çš„è¾“å‡ºï¼‰æ¥è·å–ä¿¡æ¯
3. **ä»å·¦åˆ°å³å†™**ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰ï¼šä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°å†™ï¼Œå‰é¢çš„è¯å†³å®šåé¢çš„è¯

### 8.2 Decoder Layer çš„å…­ä¸ªç»„ä»¶

Decoder æ¯” Encoder å¤šäº†ä¸€ä¸ªå­å±‚ï¼š

```
è¾“å…¥ xï¼ˆå·²ç”Ÿæˆçš„è¯ï¼‰
  â†“
[Masked Multi-Head Self-Attention]  â† åªçœ‹å·²ç”Ÿæˆçš„è¯ï¼Œä¸èƒ½å·çœ‹æœªæ¥
  â†“
[Add & Norm]
  â†“
[Multi-Head Cross-Attention]        â† Q æ¥è‡ª Decoderï¼ŒK/V æ¥è‡ª Encoder
  â†“
[Add & Norm]
  â†“
[Feed-Forward Network]
  â†“
[Add & Norm]
  â†“
è¾“å‡º
```

**æ–°ç»„ä»¶ â‘  Masked Self-Attention**

**å› æœæ©ç ï¼ˆCausal Maskï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼ŒæŠŠæœªæ¥ä½ç½®çš„æ³¨æ„åŠ›åˆ†æ•°è®¾ä¸ºè´Ÿæ— ç©·ï¼Œç»è¿‡ softmax åå˜æˆ 0ï¼Œä»è€Œ"é®ä½"æœªæ¥çš„ä¿¡æ¯ï¼‰ï¼š

```python
def create_causal_mask(seq_len):
    """åˆ›å»ºå› æœæ©ç ï¼šä¸Šä¸‰è§’ä¸º Trueï¼ˆå°†è¢«é®ä½ï¼‰"""
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    return mask

# å¯¹äº seq_len=5:
# [[0, 1, 1, 1, 1],    â† ä½ç½® 0 åªèƒ½çœ‹ä½ç½® 0
#  [0, 0, 1, 1, 1],    â† ä½ç½® 1 èƒ½çœ‹ä½ç½® 0, 1
#  [0, 0, 0, 1, 1],    â† ä½ç½® 2 èƒ½çœ‹ä½ç½® 0, 1, 2
#  [0, 0, 0, 0, 1],    â† ä½ç½® 3 èƒ½çœ‹ä½ç½® 0, 1, 2, 3
#  [0, 0, 0, 0, 0]]    â† ä½ç½® 4 èƒ½çœ‹ä½ç½® 0, 1, 2, 3, 4
```

åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œè¢«é®ä½çš„ä½ç½®è®¾ä¸º $-\infty$ï¼š

$$
\text{scores}_{masked} = \text{scores} + \text{mask} \times (-\infty)
$$

ç»è¿‡ softmax åï¼Œ$e^{-\infty} = 0$ï¼Œè¿™äº›ä½ç½®çš„æ³¨æ„åŠ›æƒé‡å°±å˜æˆäº† 0ã€‚

**æ–°ç»„ä»¶ â‘¡ Cross-Attentionï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰**

**äº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šDecoder çš„ Query å»"è¯¢é—®" Encoder çš„è¾“å‡ºï¼Œä»ä¸­è·å–æºåºåˆ—çš„ä¿¡æ¯ã€‚å°±åƒç¿»è¯‘æ—¶å›å¤´çœ‹åŸæ–‡ï¼‰ï¼š

$$
\text{CrossAttn}(Q_{dec}, K_{enc}, V_{enc}) = \text{softmax}\left(\frac{Q_{dec} K_{enc}^T}{\sqrt{d_k}}\right) V_{enc}
$$

å…³é”®ç‚¹ï¼š**Q æ¥è‡ª Decoderï¼ŒK å’Œ V æ¥è‡ª Encoder**ã€‚

### 8.3 ä»£ç å®ç°

```python
class DecoderLayer(nn.Module):
    """å•ä¸ª Decoder å±‚"""
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        # ä¸‰ä¸ªå­å±‚
        self.masked_self_attention = MultiHeadAttention(d_model, n_heads)
        self.cross_attention = MultiHeadAttention(d_model, n_heads)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        # ä¸‰ä¸ª LayerNorm
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        # ä¸‰ä¸ª Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        # å­å±‚ 1: Masked Self-Attention
        attn1, _ = self.masked_self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout1(attn1))

        # å­å±‚ 2: Cross-Attentionï¼ˆQ=decoder, K=V=encoderï¼‰
        attn2, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout2(attn2))

        # å­å±‚ 3: Feed-Forward
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout3(ff_output))

        return x


class Decoder(nn.Module):
    """å®Œæ•´ Decoder"""
    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, max_len=5000, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)

        return x
```

### 8.4 è‡ªå›å½’ç”Ÿæˆï¼šä»Šå¤©çš„è¾“å‡ºæ˜¯æ˜å¤©çš„è¾“å…¥

è®­ç»ƒå’Œæ¨ç†æ—¶ï¼ŒDecoder çš„è¡Œä¸ºæ˜¯ä¸åŒçš„ï¼š

**è®­ç»ƒæ—¶ï¼ˆTeacher Forcingï¼‰**ï¼š

```
è¾“å…¥: [<BOS>, æˆ‘, çˆ±, è‡ªç„¶, è¯­è¨€]
ç›®æ ‡: [æˆ‘, çˆ±, è‡ªç„¶, è¯­è¨€, <EOS>]
```

æ‰€æœ‰ä½ç½®**å¹¶è¡Œ**è®¡ç®—ï¼ˆå› ä¸ºç›®æ ‡åºåˆ—æ˜¯å·²çŸ¥çš„ï¼‰ï¼Œä½†é€šè¿‡å› æœæ©ç ç¡®ä¿æ¯ä¸ªä½ç½®åªçœ‹åˆ°å‰é¢çš„è¯ã€‚

**æ¨ç†æ—¶ï¼ˆè‡ªå›å½’ï¼‰**ï¼š

```
Step 1: è¾“å…¥ [<BOS>]           â†’ é¢„æµ‹ "æˆ‘"
Step 2: è¾“å…¥ [<BOS>, æˆ‘]       â†’ é¢„æµ‹ "çˆ±"
Step 3: è¾“å…¥ [<BOS>, æˆ‘, çˆ±]   â†’ é¢„æµ‹ "è‡ªç„¶"
...
Step N: è¾“å…¥ [<BOS>, æˆ‘, çˆ±, ... ] â†’ é¢„æµ‹ <EOS> â†’ åœæ­¢
```

æ¯ä¸€æ­¥åªç”Ÿæˆä¸€ä¸ªè¯ï¼Œç„¶åæŠŠè¿™ä¸ªè¯è¿½åŠ åˆ°è¾“å…¥ä¸­ï¼Œå†é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–å¯¹æ¯”äº†ä¸‰ç§æ³¨æ„åŠ›æ©ç â€”â€”åŒå‘ï¼ˆBERTï¼‰ã€å› æœï¼ˆGPTï¼‰å’Œäº¤å‰æ³¨æ„åŠ›â€”â€”ç‚¹å‡»åˆ‡æ¢æŸ¥çœ‹ä¸åŒæ©ç çš„æ•ˆæœã€‚

<IframeEmbed
  src="/visualizations/transformer-attention-mask.html"
  minHeight={760}
  title="æ³¨æ„åŠ›æ©ç å¯¹æ¯”å¯è§†åŒ–"
/>

### 8.5 å¸¸è§å‘ç‚¹

> âš ï¸ **å‘ 1ï¼šè®­ç»ƒæ—¶å¿˜äº†ç”¨å› æœæ©ç **
>
> è®­ç»ƒæ—¶è™½ç„¶ç›®æ ‡åºåˆ—æ˜¯å®Œæ•´çš„ï¼Œä½†å¿…é¡»åŠ å› æœæ©ç ï¼å¦åˆ™æ¨¡å‹åœ¨é¢„æµ‹ç¬¬ 3 ä¸ªè¯æ—¶å·²ç»"çœ‹è¿‡"äº†ç¬¬ 4ã€5 ä¸ªè¯â€”â€”è¿™å°±æ˜¯"ä¿¡æ¯æ³„æ¼ï¼ˆdata leakageï¼‰"ï¼Œæ¨¡å‹ä¼šå¾—åˆ°è™šå‡çš„é«˜åˆ†ä½†æ³›åŒ–æ€§æå·®ã€‚

> âš ï¸ **å‘ 2ï¼šæ··æ·† Padding Mask å’Œ Causal Mask**
>
> - **Padding Mask**ï¼šé®ä½ `<PAD>` ä½ç½®ï¼Œå½¢çŠ¶ä¸º `[batch, 1, 1, seq_len]`
> - **Causal Mask**ï¼šé®ä½æœªæ¥ä½ç½®ï¼Œå½¢çŠ¶ä¸º `[1, 1, seq_len, seq_len]`
> - Decoder éœ€è¦**ä¸¤è€…éƒ½ç”¨**ï¼š`combined_mask = causal_mask | padding_mask`

> âš ï¸ **å‘ 3ï¼šCross-Attention çš„ Q/K/V æå**
>
> Cross-Attention ä¸­ï¼šQ æ¥è‡ª Decoderï¼ŒK å’Œ V æ¥è‡ª Encoderã€‚å†™æˆä»£ç å°±æ˜¯ `cross_attn(dec_output, enc_output, enc_output)`ã€‚å¦‚æœå†™æˆ `cross_attn(enc_output, dec_output, dec_output)` å°±å®Œå…¨åäº†ï¼

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - Decoder æ¯” Encoder å¤šä¸€ä¸ª Cross-Attention å­å±‚
> - Masked Self-Attention ç”¨å› æœæ©ç é˜²æ­¢å·çœ‹æœªæ¥
> - Cross-Attention è®© Decoder è®¿é—® Encoder çš„è¾“å‡ºä¿¡æ¯
> - è®­ç»ƒç”¨ Teacher Forcingï¼ˆå¹¶è¡Œï¼‰ï¼Œæ¨ç†ç”¨è‡ªå›å½’ï¼ˆä¸²è¡Œï¼‰

---

## ä¹ã€å®Œæ•´ Transformerï¼šç»„è£…ç¼–ç å™¨ä¸è§£ç å™¨

ç»ˆäºåˆ°äº†æœ€æ¿€åŠ¨äººå¿ƒçš„æ—¶åˆ»â€”â€”æŠŠ Encoder å’Œ Decoder ç»„è£…æˆå®Œæ•´çš„ Transformerï¼

### 9.1 ç›´è§‰ï¼šç¿»è¯‘æµæ°´çº¿ ğŸ­

æƒ³è±¡ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘æµæ°´çº¿ï¼š

1. **é˜…è¯»éƒ¨é—¨**ï¼ˆEncoderï¼‰ï¼šæŠŠå¾·è¯­åŸæ–‡ä»å¤´åˆ°å°¾è¯»å®Œï¼Œå½¢æˆæ·±åº¦ç†è§£
2. **ç¿»è¯‘éƒ¨é—¨**ï¼ˆDecoderï¼‰ï¼šå‚è€ƒé˜…è¯»éƒ¨é—¨çš„ç†è§£ï¼Œä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°å†™å‡ºè‹±æ–‡ç¿»è¯‘
3. **å®¡æ ¸éƒ¨é—¨**ï¼ˆSoftmax + argmaxï¼‰ï¼šä»æ‰€æœ‰å€™é€‰è¯ä¸­é€‰å‡ºæ¦‚ç‡æœ€é«˜çš„

### 9.2 å®Œæ•´æ¶æ„å›¾

```
æºè¯­è¨€ Token IDs                                ç›®æ ‡è¯­è¨€ Token IDsï¼ˆå³ç§»ä¸€ä½ï¼‰
      â†“                                                â†“
  [Embedding]                                     [Embedding]
      â†“                                                â†“
  [+ Pos Encoding]                              [+ Pos Encoding]
      â†“                                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Encoder Ã— N    â”‚                         â”‚  Decoder Ã— N         â”‚
â”‚                 â”‚    K, V                  â”‚                      â”‚
â”‚  Self-Attn      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  Masked Self-Attn    â”‚
â”‚  Add & Norm     â”‚                         â”‚  Add & Norm          â”‚
â”‚  FFN            â”‚                         â”‚  Cross-Attn (Qâ†Dec)  â”‚
â”‚  Add & Norm     â”‚                         â”‚  Add & Norm          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚  FFN                 â”‚
                                            â”‚  Add & Norm          â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â†“
                                               [Linear Layer]
                                                      â†“
                                                 [Softmax]
                                                      â†“
                                             Output Probabilities
```

### 9.3 å®Œæ•´ä»£ç å®ç°

```python
class Transformer(nn.Module):
    """å®Œæ•´çš„ Transformer æ¨¡å‹"""
    def __init__(
        self,
        src_vocab_size,    # æºè¯­è¨€è¯è¡¨å¤§å°
        tgt_vocab_size,    # ç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°
        d_model=512,       # åµŒå…¥ç»´åº¦
        n_heads=8,         # æ³¨æ„åŠ›å¤´æ•°
        d_ff=2048,         # FFN éšè—å±‚ç»´åº¦
        n_encoder_layers=6,
        n_decoder_layers=6,
        max_len=5000,
        dropout=0.1,
    ):
        super().__init__()
        self.encoder = Encoder(src_vocab_size, d_model, n_heads, d_ff,
                              n_encoder_layers, max_len, dropout)
        self.decoder = Decoder(tgt_vocab_size, d_model, n_heads, d_ff,
                              n_decoder_layers, max_len, dropout)
        # æœ€ç»ˆè¾“å‡ºå±‚ï¼šd_model â†’ vocab_size
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        # 1. Encoder å¤„ç†æºåºåˆ—
        encoder_output = self.encoder(src, src_mask)

        # 2. Decoder å¤„ç†ç›®æ ‡åºåˆ—ï¼ˆå‚è€ƒ Encoder è¾“å‡ºï¼‰
        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)

        # 3. æ˜ å°„åˆ°è¯è¡¨å¤§å°çš„ logits
        logits = self.output_projection(decoder_output)

        return logits

    def generate(self, src, max_len=50, start_token=1, end_token=2):
        """è´ªå¿ƒè§£ç ç”Ÿæˆ"""
        self.eval()
        with torch.no_grad():
            encoder_output = self.encoder(src)

            # ä» <BOS> å¼€å§‹
            tgt = torch.full((src.size(0), 1), start_token, dtype=torch.long, device=src.device)

            for _ in range(max_len):
                # åˆ›å»ºå› æœæ©ç 
                tgt_mask = create_causal_mask(tgt.size(1)).to(src.device)

                decoder_output = self.decoder(tgt, encoder_output, tgt_mask=tgt_mask)
                logits = self.output_projection(decoder_output[:, -1, :])  # åªå–æœ€åä¸€ä¸ªä½ç½®
                next_token = logits.argmax(dim=-1, keepdim=True)

                tgt = torch.cat([tgt, next_token], dim=1)

                # å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½ç”Ÿæˆäº† <EOS>ï¼Œåœæ­¢
                if (next_token == end_token).all():
                    break

            return tgt

# åˆ›å»ºæ¨¡å‹
model = Transformer(
    src_vocab_size=10000,
    tgt_vocab_size=8000,
    d_model=512,
    n_heads=8,
    d_ff=2048,
    n_encoder_layers=6,
    n_decoder_layers=6,
)

# ç»Ÿè®¡å‚æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"æ€»å‚æ•°é‡: {total_params:,}")  # çº¦ 65M å‚æ•°ï¼ˆç±»ä¼¼åŸå§‹è®ºæ–‡çš„ base æ¨¡å‹ï¼‰
```

### 9.4 è®­ç»ƒå¾ªç¯

```python
import torch.optim as optim

# è®­ç»ƒé…ç½®
optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)
criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥ <PAD> çš„æŸå¤±

def train_step(model, src, tgt, optimizer, criterion):
    model.train()
    optimizer.zero_grad()

    # tgt_input: å»æ‰æœ€åä¸€ä¸ª tokenï¼ˆä½œä¸ºè¾“å…¥ï¼‰
    # tgt_output: å»æ‰ç¬¬ä¸€ä¸ª tokenï¼ˆä½œä¸ºç›®æ ‡ï¼‰
    tgt_input = tgt[:, :-1]
    tgt_output = tgt[:, 1:]

    # åˆ›å»ºæ©ç 
    tgt_mask = create_causal_mask(tgt_input.size(1)).to(src.device)

    # å‰å‘ä¼ æ’­
    logits = model(src, tgt_input, tgt_mask=tgt_mask)

    # è®¡ç®—æŸå¤±
    loss = criterion(
        logits.reshape(-1, logits.size(-1)),  # [batch*seq, vocab]
        tgt_output.reshape(-1),                # [batch*seq]
    )

    # åå‘ä¼ æ’­
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()

    return loss.item()
```

> ğŸ’¡ **Teacher Forcing çš„ç»†èŠ‚**
>
> æ³¨æ„ `tgt_input = tgt[:, :-1]` å’Œ `tgt_output = tgt[:, 1:]` çš„é”™ä½ï¼š
>
> ```
> tgt:        [<BOS>,  æˆ‘,  çˆ±, NLP, <EOS>]
> tgt_input:  [<BOS>,  æˆ‘,  çˆ±, NLP]         â† å–‚ç»™ Decoder
> tgt_output: [æˆ‘,     çˆ±, NLP, <EOS>]       â† æœŸæœ› Decoder è¾“å‡º
> ```
>
> è¿™å°±æ˜¯ "Teacher Forcing"â€”â€”è®­ç»ƒæ—¶æŠŠæ­£ç¡®ç­”æ¡ˆçš„å‰ç¼€å–‚ç»™ Decoderï¼Œè®©å®ƒå­¦ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–å±•ç¤ºäº†å®Œæ•´ Transformer çš„æ¶æ„å’Œæ•°æ®æµåŠ¨â€”â€”ç‚¹å‡»ç»„ä»¶æŸ¥çœ‹è¯¦ç»†è¯´æ˜ï¼Œæ’­æ”¾åŠ¨ç”»è§‚å¯Ÿ token å¦‚ä½•ä»è¾“å…¥æµç»æ•´ä¸ªæ¨¡å‹äº§ç”Ÿè¾“å‡ºã€‚

<IframeEmbed
  src="/visualizations/transformer-architecture.html"
  minHeight={900}
  title="å®Œæ•´ Transformer æ¶æ„å¯è§†åŒ–"
/>

### 9.5 å¸¸è§å‘ç‚¹

> âš ï¸ **å‘ 1ï¼šç›®æ ‡åºåˆ—çš„å³ç§»ï¼ˆShifted Rightï¼‰**
>
> Decoder çš„è¾“å…¥å¿…é¡»æ˜¯ç›®æ ‡åºåˆ—"å³ç§»ä¸€ä½"ï¼š`[<BOS>, w1, w2, ..., wN]`ï¼Œè€Œä¸æ˜¯ `[w1, w2, ..., wN, <EOS>]`ã€‚å¦‚æœä¸å³ç§»ï¼Œæ¨¡å‹åœ¨ä½ç½® 0 å°±èƒ½çœ‹åˆ°ç­”æ¡ˆ w1â€”â€”è¿™åˆæ˜¯ä¿¡æ¯æ³„æ¼ï¼

> âš ï¸ **å‘ 2ï¼šEncoder å’Œ Decoder ä¸å…±äº«è¯åµŒå…¥**
>
> åœ¨æœºå™¨ç¿»è¯‘ä¸­ï¼Œæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€å¯èƒ½æœ‰ä¸åŒçš„è¯è¡¨ï¼Œæ‰€ä»¥ Encoder å’Œ Decoder å„è‡ªæœ‰ç‹¬ç«‹çš„ Embedding å±‚ã€‚ä½†å¦‚æœæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ç›¸åŒï¼ˆå¦‚æ–‡æœ¬æ‘˜è¦ï¼‰ï¼Œå¯ä»¥å…±äº«åµŒå…¥ã€‚

> âš ï¸ **å‘ 3ï¼šæ¨ç†æ—¶å¿˜äº† model.eval() å’Œ torch.no_grad()**
>
> æ¨ç†æ—¶å¿…é¡»å…³é—­ Dropoutï¼ˆ`model.eval()`ï¼‰å’Œæ¢¯åº¦è®¡ç®—ï¼ˆ`torch.no_grad()`ï¼‰ï¼Œå¦åˆ™ç»“æœä¸ç¡®å®šä¸”æµªè´¹å†…å­˜ã€‚

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - å®Œæ•´ Transformer = Encoder + Decoder + Output Projection
> - Encoder è¾“å‡ºçš„ K/V ä¼ ç»™ Decoder çš„ Cross-Attention
> - è®­ç»ƒç”¨ Teacher Forcingï¼Œæ¨ç†ç”¨è‡ªå›å½’è´ªå¿ƒ/æŸæœç´¢
> - æ³¨æ„ç›®æ ‡åºåˆ—çš„å³ç§»å’Œæ©ç çš„æ­£ç¡®ä½¿ç”¨

---

## åã€ä¸‰å¤§æ¶æ„å®¶æ—ä¸ä¸»æµ LLM æ¶æ„è¯¦è§£

åŸå§‹ Transformer ä½¿ç”¨äº†å®Œæ•´çš„ Encoder-Decoder ç»“æ„ï¼Œä½†åæ¥çš„ç ”ç©¶å‘ç°ï¼Œ**åªç”¨å…¶ä¸­ä¸€éƒ¨åˆ†**ä¹Ÿèƒ½å–å¾—æƒŠäººçš„æ•ˆæœã€‚è¿™å‚¬ç”Ÿäº†ä¸‰å¤§æ¶æ„å®¶æ—ã€‚

### 10.1 Encoder-Onlyï¼šBERT å®¶æ—

**BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šåªç”¨ Encoder éƒ¨åˆ†ï¼Œè®©æ¯ä¸ªè¯åŒæ—¶çœ‹åˆ°å·¦è¾¹å’Œå³è¾¹çš„æ‰€æœ‰è¯ï¼Œé€šè¿‡"å®Œå½¢å¡«ç©º"è®­ç»ƒå‡ºå¼ºå¤§çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼‰ã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**

- åªæœ‰ Encoderï¼Œæ²¡æœ‰ Decoder
- **åŒå‘æ³¨æ„åŠ›**ï¼šæ¯ä¸ªä½ç½®å¯ä»¥çœ‹åˆ°æ‰€æœ‰å…¶ä»–ä½ç½®
- è®­ç»ƒä»»åŠ¡ï¼š**MLMï¼ˆMasked Language Modelï¼Œæ©ç è¯­è¨€æ¨¡å‹ï¼‰**â€”â€”éšæœºé®ä½ 15% çš„è¯ï¼Œè®©æ¨¡å‹çŒœ

```python
# BERT é£æ ¼çš„é¢„è®­ç»ƒ
# è¾“å…¥: "æˆ‘ [MASK] è‡ªç„¶è¯­è¨€ [MASK]"
# ç›®æ ‡: é¢„æµ‹ [MASK] ä½ç½®çš„è¯ â†’ "çˆ±", "å¤„ç†"

# MLM çš„æ©ç ç­–ç•¥ï¼ˆ80/10/10 è§„åˆ™ï¼‰ï¼š
# è¢«é€‰ä¸­çš„ 15% çš„ token ä¸­ï¼š
# - 80% æ›¿æ¢ä¸º [MASK]
# - 10% æ›¿æ¢ä¸ºéšæœº token
# - 10% ä¿æŒä¸å˜
```

**é€‚ç”¨ä»»åŠ¡ï¼š** æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€é—®ç­”ã€è¯­ä¹‰ç›¸ä¼¼åº¦â€”â€”æ‰€æœ‰éœ€è¦**ç†è§£**æ–‡æœ¬çš„ä»»åŠ¡ã€‚

**ä»£è¡¨æ¨¡å‹ï¼š** BERT, RoBERTa, ALBERT, DeBERTa, ELECTRA

### 10.2 Decoder-Onlyï¼šGPT å®¶æ—

**GPTï¼ˆGenerative Pre-trained Transformerï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šåªç”¨ Decoder éƒ¨åˆ†ï¼Œä½¿ç”¨å› æœæ©ç ä»å·¦åˆ°å³é€è¯ç”Ÿæˆï¼Œé€šè¿‡"ä¸‹ä¸€ä¸ªè¯é¢„æµ‹"è®­ç»ƒå‡ºå¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼‰ã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**

- åªæœ‰ Decoderï¼ˆå»æ‰äº† Cross-Attention å­å±‚ï¼‰
- **å› æœæ³¨æ„åŠ›**ï¼šæ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å·¦è¾¹ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
- è®­ç»ƒä»»åŠ¡ï¼š**CLMï¼ˆCausal Language Modelï¼Œå› æœè¯­è¨€æ¨¡å‹ï¼‰**â€”â€”é¢„æµ‹ä¸‹ä¸€ä¸ªè¯

```python
# GPT é£æ ¼çš„é¢„è®­ç»ƒ
# è¾“å…¥:   "æˆ‘  çˆ±  è‡ªç„¶  è¯­è¨€"
# ç›®æ ‡:   "çˆ± è‡ªç„¶  è¯­è¨€  å¤„ç†"
# ä½ç½® 0: çœ‹åˆ° [æˆ‘]         â†’ é¢„æµ‹ "çˆ±"
# ä½ç½® 1: çœ‹åˆ° [æˆ‘, çˆ±]     â†’ é¢„æµ‹ "è‡ªç„¶"
# ä½ç½® 2: çœ‹åˆ° [æˆ‘, çˆ±, è‡ªç„¶] â†’ é¢„æµ‹ "è¯­è¨€"
```

**é€‚ç”¨ä»»åŠ¡ï¼š** æ–‡æœ¬ç”Ÿæˆã€ä»£ç ç”Ÿæˆã€å¯¹è¯ã€ç¿»è¯‘â€”â€”æ‰€æœ‰éœ€è¦**ç”Ÿæˆ**æ–‡æœ¬çš„ä»»åŠ¡ã€‚ChatGPTã€GPT-4ã€Claude éƒ½å±äºè¿™ä¸ªå®¶æ—ã€‚

**ä»£è¡¨æ¨¡å‹ï¼š** GPT-2, GPT-3, GPT-4, LLaMA, Mistral, Claude

### 10.3 Encoder-Decoderï¼šT5 å®¶æ—

**T5ï¼ˆText-to-Text Transfer Transformerï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä¿ç•™å®Œæ•´çš„ Encoder-Decoder ç»“æ„ï¼ŒæŠŠæ‰€æœ‰ NLP ä»»åŠ¡éƒ½ç»Ÿä¸€ä¸º"è¾“å…¥æ–‡æœ¬â†’è¾“å‡ºæ–‡æœ¬"çš„æ ¼å¼ï¼‰ã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**

- å®Œæ•´çš„ Encoder + Decoder
- Encoder åŒå‘æ³¨æ„åŠ›ï¼ŒDecoder å› æœæ³¨æ„åŠ› + Cross-Attention
- è®­ç»ƒä»»åŠ¡ï¼š**Span Corruption**â€”â€”éšæœºé®ä½è¿ç»­çš„ç‰‡æ®µï¼Œè®©æ¨¡å‹è¿˜åŸ

```python
# T5 é£æ ¼çš„è®­ç»ƒ
# è¾“å…¥: "æˆ‘ <X> è¯­è¨€å¤„ç†"
# ç›®æ ‡: "<X> çˆ± è‡ªç„¶"
# å…¶ä¸­ <X> æ˜¯è¢«é®ä½çš„è¿ç»­ç‰‡æ®µçš„æ ‡è®°
```

**é€‚ç”¨ä»»åŠ¡ï¼š** ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”â€”â€”ç‰¹åˆ«é€‚åˆ**è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯åºåˆ—**çš„ä»»åŠ¡ã€‚

**ä»£è¡¨æ¨¡å‹ï¼š** T5, BART, mBART, UL2

### 10.4 å…¨å±€å¯¹æ¯”

| ç»´åº¦           | BERT (Encoder) | GPT (Decoder)        | T5 (Enc-Dec)          |
| -------------- | -------------- | -------------------- | --------------------- |
| **æ³¨æ„åŠ›æ–¹å‘** | åŒå‘ â†”         | å•å‘ â†’               | åŒå‘(Enc) + å•å‘(Dec) |
| **è®­ç»ƒç›®æ ‡**   | MLM (å®Œå½¢å¡«ç©º) | CLM (é¢„æµ‹ä¸‹ä¸€ä¸ªè¯)   | Span Corruption       |
| **æ“…é•¿ä»»åŠ¡**   | ç†è§£ç±»         | ç”Ÿæˆç±»               | åºåˆ—åˆ°åºåˆ—            |
| **å‚æ•°æ•ˆç‡**   | é«˜ï¼ˆç†è§£ä»»åŠ¡ï¼‰ | éšè§„æ¨¡æ¶¨ï¼ˆæ¶Œç°èƒ½åŠ›ï¼‰ | ä¸­ç­‰                  |
| **æ¨ç†æ–¹å¼**   | ä¸€æ¬¡æ€§è¾“å‡º     | è‡ªå›å½’é€è¯ç”Ÿæˆ       | å…ˆç¼–ç å†é€è¯è§£ç       |
| **å…¸å‹åº”ç”¨**   | åˆ†ç±»ã€NERã€QA  | èŠå¤©ã€å†™ä½œã€ç¼–ç¨‹     | ç¿»è¯‘ã€æ‘˜è¦            |
| **ä»£è¡¨**       | BERT, RoBERTa  | GPT-4, LLaMA, Claude | T5, BART              |

### 10.5 å½“ä»Šä¸»æµ LLM éƒ½ç”¨ä»€ä¹ˆæ¶æ„ï¼Ÿ

è®©æˆ‘ä»¬çœ‹çœ‹ 2024-2025 å¹´**ä½ æ¯å¤©åœ¨ç”¨çš„ LLM** åˆ°åº•æ˜¯ä»€ä¹ˆæ¶æ„ï¼š

| æ¨¡å‹                | å…¬å¸        | æ¶æ„                     | å‚æ•°é‡           | å…³é”®æŠ€æœ¯                                   |
| ------------------- | ----------- | ------------------------ | ---------------- | ------------------------------------------ |
| **GPT-4 / GPT-4o**  | OpenAI      | Decoder-Onlyï¼ˆä¼ é—» MoEï¼‰ | ~1.8Tï¼ˆä¼ é—»ï¼‰    | RLHF, å¤šæ¨¡æ€                               |
| **Claude 3.5/4**    | Anthropic   | Decoder-Only             | æœªå…¬å¼€           | Constitutional AI (RLAIF)                  |
| **Gemini 2.0/2.5**  | Google      | Decoder-Only + MoE       | ~1T+             | å¤šæ¨¡æ€ç»Ÿä¸€ token æµ, Multi-Query Attention |
| **DeepSeek V3/R1**  | DeepSeek    | Decoder-Only + MoE       | 671Bï¼ˆæ¿€æ´» 37Bï¼‰ | MLAï¼ˆå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼‰, 256 ä¸“å®¶å– 8        |
| **LLaMA 3**         | Meta        | Decoder-Onlyï¼ˆå¯†é›†ï¼‰     | 8B / 70B / 405B  | GQA, RoPE, SwiGLU                          |
| **Mistral Large 3** | Mistral     | Decoder-Only + MoE       | 675Bï¼ˆæ¿€æ´» 41Bï¼‰ | GQA, æ»‘åŠ¨çª—å£æ³¨æ„åŠ›                        |
| **Qwen 2.5**        | é˜¿é‡Œ        | Decoder-Onlyï¼ˆå¯†é›†/MoEï¼‰ | 72B / MoE ç‰ˆ     | GQA, RoPE, YaRN                            |
| **T5 / mT5**        | Google      | Encoder-Decoder          | 11B              | Span Corruption                            |
| **BERT / DeBERTa**  | Google/å¾®è½¯ | Encoder-Only             | 340M             | MLM, ç†è§£ç±»ä»»åŠ¡                            |

> ğŸ’¡ **ä¸€ä¸ªéœ‡æ’¼çš„äº‹å®**ï¼šä½ æ¯å¤©ç”¨çš„ ChatGPTã€Claudeã€Geminiã€DeepSeekâ€”â€”**å…¨éƒ¨éƒ½æ˜¯ Decoder-Only æ¶æ„**ï¼Encoder-Decoder å’Œ Encoder-Only åœ¨ç”Ÿæˆç±»ä»»åŠ¡ä¸­å·²ç»è¿‘ä¹æ¶ˆå¤±ã€‚

### 10.6 ä¸ºä»€ä¹ˆ Decoder-Only ç»Ÿæ²»äº†ä¸€åˆ‡ï¼Ÿ

è¿™ä¸æ˜¯å¶ç„¶çš„ï¼Œè€Œæ˜¯æœ‰æ·±å±‚åŸå› çš„ï¼š

**åŸå›  â‘  è§„æ¨¡æ•ˆåº”ï¼ˆScaling Lawï¼‰**

ç ”ç©¶è¡¨æ˜ï¼Œå½“è®¡ç®—é¢„ç®—è¶³å¤Ÿå¤§æ—¶ï¼ŒDecoder-Only æ¶æ„å‡ ä¹æ€»æ˜¯å æ®**æœ€ä¼˜å‰æ²¿**ã€‚2025 å¹´çš„è®ºæ–‡ _"Revisiting Encoder-Decoder LLM"_ è¯å®ï¼šè™½ç„¶åœ¨å°è§„æ¨¡ï¼ˆä¸åˆ° 1B å‚æ•°ï¼‰æ—¶ Encoder-Decoder æœ‰ä¼˜åŠ¿ï¼Œä½†ä¸€æ—¦è§„æ¨¡æ‰©å¤§ï¼ŒDecoder-Only çš„æ€§èƒ½å¢é•¿æ›´å¿«ã€‚

**åŸå›  â‘¡ è®­ç»ƒæ•ˆç‡**

Decoder-Only çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆCLMï¼‰ç›®æ ‡å¤©ç„¶åˆ©ç”¨äº†**æ¯ä¸€ä¸ª token**ä½œä¸ºè®­ç»ƒä¿¡å·â€”â€”åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®éƒ½åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚è€Œ Encoder-Decoder çš„è®­ç»ƒåªèƒ½åˆ©ç”¨è¢«é®ä½çš„é‚£ 15% çš„ tokenã€‚

```
Decoder-Onlyï¼ˆGPTï¼‰è®­ç»ƒæ•ˆç‡ï¼š
è¾“å…¥:   "æˆ‘ çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç†"
ç›®æ ‡:   "çˆ± è‡ªç„¶ è¯­è¨€ å¤„ç† <EOS>"
â†’ æ¯ä¸ªä½ç½®éƒ½æ˜¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼5ä¸ªtoken = 5ä¸ªè®­ç»ƒä¿¡å·

Encoder-Onlyï¼ˆBERTï¼‰è®­ç»ƒæ•ˆç‡ï¼š
è¾“å…¥:   "æˆ‘ [MASK] è‡ªç„¶ [MASK] å¤„ç†"
ç›®æ ‡:   é¢„æµ‹ "çˆ±" å’Œ "è¯­è¨€"
â†’ åªæœ‰è¢«MASKçš„ä½ç½®æœ‰è®­ç»ƒä¿¡å·ï¼5ä¸ªtoken = 2ä¸ªè®­ç»ƒä¿¡å·
```

**åŸå›  â‘¢ æ¶æ„ç®€æ´æ€§**

Decoder-Only åªæœ‰ä¸€ç§æ¨¡å—ï¼ˆå¸¦å› æœæ©ç çš„ Self-Attentionï¼‰ï¼Œæ²¡æœ‰ Cross-Attentionã€‚è¿™æ„å‘³ç€ï¼š

- æ›´å°‘çš„è¶…å‚æ•°éœ€è¦è°ƒä¼˜
- æ›´ç®€å•çš„å¹¶è¡ŒåŒ–ç­–ç•¥
- æ›´å®¹æ˜“åš KV Cache ä¼˜åŒ–

**åŸå›  â‘£ æ¶Œç°èƒ½åŠ›**

å¤§è§„æ¨¡ Decoder-Only æ¨¡å‹å±•ç°å‡ºäº†æƒŠäººçš„ **In-Context Learning** èƒ½åŠ›â€”â€”ä¸éœ€è¦å¾®è°ƒï¼Œåªé  few-shot ç¤ºä¾‹å°±èƒ½å®Œæˆæ–°ä»»åŠ¡ã€‚è¿™ç§èƒ½åŠ›åœ¨ Encoder-Only æ¨¡å‹ä¸­å‡ ä¹ä¸å­˜åœ¨ã€‚

**åŸå›  â‘¤ é€šç”¨æ€§**

ä¸€ä¸ª Decoder-Only æ¨¡å‹å¯ä»¥åšç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ã€ç¼–ç¨‹ã€æ•°å­¦æ¨ç†â€¦â€¦è€Œ Encoder-Only åªæ“…é•¿ç†è§£ç±»ä»»åŠ¡ã€‚ç”¨ä¸€ç§æ¶æ„è§£å†³æ‰€æœ‰é—®é¢˜ï¼Œåœ¨å·¥ç¨‹ä¸Šè¿œæ¯”ç»´æŠ¤å¤šç§æ¶æ„ç®€å•ã€‚

### 10.7 æ¶æ„åˆ›æ–°è¶‹åŠ¿ï¼šä¸æ­¢äº Decoder-Only

è™½ç„¶ Decoder-Only æ˜¯åŸºåº§ï¼Œä½†å„å®¶éƒ½åœ¨ä¸Šé¢åšäº†å¤§é‡åˆ›æ–°ï¼š

**â‘  æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–**

```
åŸå§‹ Transformer:  Multi-Head Attention (MHA)  â†’ æ¯ä¸ªå¤´ç‹¬ç«‹çš„ K/V
     â†“
LLaMA / Mistral:  Grouped-Query Attention (GQA) â†’ å¤šä¸ª Q å¤´å…±äº« K/V
     â†“
DeepSeek V3:      Multi-Head Latent Attention (MLA) â†’ å‹ç¼© K/V åˆ°ä½ç»´å†è¿˜åŸ
```

GQA å’Œ MLA çš„æ ¸å¿ƒç›®çš„æ˜¯ä¸€æ ·çš„ï¼š**å‡å°‘ KV Cache çš„å†…å­˜å ç”¨**ã€‚å½“ä½ ç”¨ ChatGPT èŠäº† 100K ä¸ª token æ—¶ï¼Œæ¨¡å‹éœ€è¦ç¼“å­˜æ‰€æœ‰å†å² token çš„ Key å’Œ Value å‘é‡â€”â€”è¿™ä¸ªç¼“å­˜ï¼ˆKV Cacheï¼‰æ˜¯æ¨ç†æ—¶æœ€å¤§çš„å†…å­˜ç“¶é¢ˆã€‚

**â‘¡ æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰**

DeepSeek V3 å’Œ Gemini 2.5 éƒ½ä½¿ç”¨ MoEï¼šæ¨¡å‹æœ‰å‡ ç™¾ä¸ª"ä¸“å®¶"ç½‘ç»œï¼Œä½†æ¯ä¸ª token åªæ¿€æ´»å…¶ä¸­å‡ ä¸ªã€‚

```
DeepSeek V3:  256 ä¸ªè·¯ç”±ä¸“å®¶ + 1 ä¸ªå…±äº«ä¸“å®¶ï¼Œæ¯ä¸ª token æ¿€æ´» Top-8
              â†’ 671B æ€»å‚æ•°ï¼Œä½†æ¯ä¸ª token åªç”¨ 37B â†’ 10x è®¡ç®—æ•ˆç‡æå‡
```

è¿™å°±åƒä¸€ä¸ªå¤§åŒ»é™¢ï¼šæœ‰éª¨ç§‘ã€çœ¼ç§‘ã€å¿ƒå†…ç§‘â€¦â€¦çš„ä¸“å®¶ï¼Œæ¯ä¸ªç—…äººåªçœ‹æœ€ç›¸å…³çš„å‡ ä¸ªç§‘å®¤ï¼Œä¸éœ€è¦æ‰€æœ‰åŒ»ç”Ÿéƒ½å‡ºåŠ¨ã€‚

**â‘¢ ä½ç½®ç¼–ç è¿›åŒ–**

```
åŸå§‹ Transformer: sin/cos ç»å¯¹ä½ç½®ç¼–ç  â†’ å¤–æ¨èƒ½åŠ›å·®
     â†“
LLaMA / å¤§éƒ¨åˆ†ç°ä»£ LLM: RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰ â†’ æ›´å¥½çš„ç›¸å¯¹è·ç¦»å»ºæ¨¡
     â†“
+ NTK-aware / YaRN æ‰©å±• â†’ æ”¯æŒ 100K+ ä¸Šä¸‹æ–‡
```

> âš ï¸ **ä½† Encoder-Decoder å¹¶æ²¡æœ‰æ­»**
>
> åœ¨**æ¨ç†æ•ˆç‡**ä¸Šï¼ŒEncoder-Decoder æœ‰ä¸€ä¸ªæ€æ‰‹çº§ä¼˜åŠ¿ï¼š2025 å¹´çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŒç­‰è§„æ¨¡çš„ Encoder-Decoder æ¨¡å‹é¦– token å»¶è¿Ÿé™ä½ 47%ï¼Œååé‡æå‡ 4.7 å€ã€‚åŸå› å¾ˆç®€å•â€”â€”Encoder å¯ä»¥å¹¶è¡Œå¤„ç†æ•´ä¸ªè¾“å…¥ï¼Œè€Œ Decoder-Only å¿…é¡»ä¸²è¡Œå¤„ç†é•¿ promptã€‚åœ¨è¾¹ç¼˜è®¾å¤‡å’Œä½å»¶è¿Ÿåœºæ™¯ä¸‹ï¼ŒEncoder-Decoder ä»ç„¶æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–å¯¹æ¯”äº† BERTã€GPT å’Œ T5 ä¸‰ç§æ¶æ„çš„ç»“æ„å·®å¼‚ã€æ³¨æ„åŠ›æ¨¡å¼å’Œè®­ç»ƒç›®æ ‡ã€‚

<IframeEmbed
  src="/visualizations/transformer-family.html"
  minHeight={860}
  title="Transformer ä¸‰å¤§æ¶æ„å®¶æ—å¯¹æ¯”"
/>

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - **BERT**ï¼ˆEncoder-Onlyï¼‰ï¼šåŒå‘æ³¨æ„åŠ›ï¼Œæ“…é•¿ç†è§£ï¼Œé€‚åˆåˆ†ç±»/NER/QA
> - **GPT**ï¼ˆDecoder-Onlyï¼‰ï¼šå› æœæ³¨æ„åŠ›ï¼Œæ“…é•¿ç”Ÿæˆï¼Œæ˜¯å½“å‰ç»å¯¹ä¸»æµ
> - **T5**ï¼ˆEncoder-Decoderï¼‰ï¼šä¸¤è€…å…¼å¤‡ï¼Œé€‚åˆç¿»è¯‘/æ‘˜è¦
> - å½“ä»Šæ‰€æœ‰ä¸»æµ LLMï¼ˆGPT-4ã€Claudeã€Geminiã€DeepSeekã€LLaMAï¼‰å…¨éƒ¨æ˜¯ Decoder-Only
> - Decoder-Only ç»Ÿæ²»çš„æ ¸å¿ƒåŸå› ï¼šæ›´å¥½çš„è§„æ¨¡æ•ˆåº”ã€æ›´é«˜çš„è®­ç»ƒæ•ˆç‡ã€æ¶Œç°èƒ½åŠ›
> - æ¶æ„åˆ›æ–°è¶‹åŠ¿ï¼šGQA/MLA ä¼˜åŒ–æ³¨æ„åŠ›ã€MoE æå‡æ•ˆç‡ã€RoPE æ”¯æŒé•¿ä¸Šä¸‹æ–‡

---

## åä¸€ã€PyTorch ä»£ç å®æˆ˜ï¼šä»é›¶æ­å»º Transformer

è®©æˆ‘ä»¬æŠŠå‰é¢æ‰€æœ‰çš„ç»„ä»¶æ•´åˆèµ·æ¥ï¼Œå†™ä¸€ä¸ªå®Œæ•´çš„ã€å¯è¿è¡Œçš„ Transformer æ¨¡å‹ã€‚

### 11.1 å®Œæ•´æ¨¡å‹ä»£ç ï¼ˆæ•´åˆç‰ˆï¼‰

```python
import torch
import torch.nn as nn
import math


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç ï¼šsin/cos å›ºå®šç¼–ç """
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›"""
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        Q = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        attn_weights = self.dropout(torch.softmax(scores, dim=-1))
        context = torch.matmul(attn_weights, V)
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        return self.W_O(context)


class FeedForward(nn.Module):
    """å‰é¦ˆç½‘ç»œ"""
    def __init__(self, d_model, d_ff=2048, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
        )

    def forward(self, x):
        return self.net(x)


class EncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        x = self.norm1(x + self.dropout1(self.self_attn(x, x, x, mask)))
        x = self.norm2(x + self.dropout2(self.ffn(x)))
        return x


class DecoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):
        x = self.norm1(x + self.dropout1(self.self_attn(x, x, x, tgt_mask)))
        x = self.norm2(x + self.dropout2(self.cross_attn(x, enc_output, enc_output, src_mask)))
        x = self.norm3(x + self.dropout3(self.ffn(x)))
        return x


class Transformer(nn.Module):
    def __init__(self, src_vocab, tgt_vocab, d_model=512, n_heads=8,
                 d_ff=2048, n_enc_layers=6, n_dec_layers=6,
                 max_len=5000, dropout=0.1):
        super().__init__()
        self.d_model = d_model

        # Embeddings + Positional Encoding
        self.src_embed = nn.Embedding(src_vocab, d_model)
        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)
        self.pos_enc = PositionalEncoding(d_model, max_len, dropout)

        # Encoder & Decoder stacks
        self.encoder_layers = nn.ModuleList(
            [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_enc_layers)]
        )
        self.decoder_layers = nn.ModuleList(
            [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_dec_layers)]
        )

        # Output projection
        self.output_proj = nn.Linear(d_model, tgt_vocab)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def encode(self, src, src_mask=None):
        x = self.pos_enc(self.src_embed(src) * math.sqrt(self.d_model))
        for layer in self.encoder_layers:
            x = layer(x, src_mask)
        return x

    def decode(self, tgt, enc_output, src_mask=None, tgt_mask=None):
        x = self.pos_enc(self.tgt_embed(tgt) * math.sqrt(self.d_model))
        for layer in self.decoder_layers:
            x = layer(x, enc_output, src_mask, tgt_mask)
        return x

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        enc_output = self.encode(src, src_mask)
        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)
        return self.output_proj(dec_output)


# ===== å·¥å…·å‡½æ•° =====
def create_causal_mask(size):
    """åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’ä¸º 1ï¼Œä¸Šä¸‰è§’ä¸º 0ï¼‰"""
    return torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(0)

def create_padding_mask(seq, pad_idx=0):
    """åˆ›å»º padding æ©ç """
    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)
```

### 11.2 å¿«é€ŸéªŒè¯

```python
# åˆ›å»ºæ¨¡å‹
model = Transformer(src_vocab=5000, tgt_vocab=5000, d_model=256, n_heads=8, d_ff=1024, n_enc_layers=3, n_dec_layers=3)

# æ¨¡æ‹Ÿè¾“å…¥
src = torch.randint(1, 5000, (4, 20))  # batch=4, src_len=20
tgt = torch.randint(1, 5000, (4, 15))  # batch=4, tgt_len=15

# åˆ›å»ºæ©ç 
src_mask = create_padding_mask(src)
tgt_mask = create_causal_mask(tgt.size(1)) & create_padding_mask(tgt)

# å‰å‘ä¼ æ’­
logits = model(src, tgt, src_mask, tgt_mask)
print(f"è¾“å‡º logits å½¢çŠ¶: {logits.shape}")  # [4, 15, 5000]

# å‚æ•°ç»Ÿè®¡
total = sum(p.numel() for p in model.parameters())
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"æ€»å‚æ•°é‡: {total:,}")
print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable:,}")
```

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - å®Œæ•´ Transformer çº¦ 200 è¡Œ PyTorch ä»£ç å³å¯å®ç°
> - å…³é”®ç»„ä»¶ï¼šPositionalEncoding â†’ MultiHeadAttention â†’ FeedForward â†’ EncoderLayer â†’ DecoderLayer
> - å·¥å…·å‡½æ•°ï¼š`create_causal_mask` å’Œ `create_padding_mask`
> - Xavier åˆå§‹åŒ–å¯¹è®­ç»ƒç¨³å®šæ€§å¾ˆé‡è¦

---

## åäºŒã€å¸¸è§å‘ç‚¹ä¸æœ€ä½³å®è·µ

ç»è¿‡å‰é¢åä¸€èŠ‚çš„å­¦ä¹ ï¼Œä½ å·²ç»ç†è§£äº† Transformer çš„æ–¹æ–¹é¢é¢ã€‚ä½†åœ¨å®é™…å¼€å‘ä¸­ï¼Œè¿˜æœ‰å¾ˆå¤š"è¸©å‘ç»éªŒ"å€¼å¾—åˆ†äº«ã€‚

### 12.1 å­¦ä¹ ç‡ Warmupï¼šä¸èƒ½ä¸€å¼€å§‹å°±å¤§æ­¥è·‘

åŸè®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªç‰¹æ®Šçš„å­¦ä¹ ç‡è°ƒåº¦ï¼šå…ˆ warmup å† decayã€‚

$$
lr = d_{model}^{-0.5} \cdot \min(step^{-0.5}, step \cdot warmup\_steps^{-1.5})
$$

```python
class TransformerScheduler:
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0

    def step(self):
        self.step_num += 1
        lr = self.d_model ** (-0.5) * min(
            self.step_num ** (-0.5),
            self.step_num * self.warmup_steps ** (-1.5)
        )
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
```

> ğŸ’¡ **ä¸ºä»€ä¹ˆéœ€è¦ Warmupï¼Ÿ**
>
> è®­ç»ƒåˆæœŸæ¨¡å‹å‚æ•°æ˜¯éšæœºçš„ï¼Œæ¢¯åº¦æ–¹å‘å¾ˆä¸ç¨³å®šã€‚å¦‚æœä¸€å¼€å§‹å­¦ä¹ ç‡å°±å¾ˆå¤§ï¼Œæ¨¡å‹å¯èƒ½ç›´æ¥"è·‘é£"ã€‚Warmup è®©æ¨¡å‹å…ˆç”¨å°å­¦ä¹ ç‡ç¨³ä½ï¼Œç„¶åå†é€æ¸åŠ å¤§ã€‚

### 12.2 Label Smoothingï¼šä¸è¦å¤ªè‡ªä¿¡

**æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šè®­ç»ƒæ—¶ä¸è®©æ¨¡å‹ 100% ç¡®ä¿¡æ­£ç¡®ç­”æ¡ˆï¼Œè€Œæ˜¯ç»™æ­£ç¡®ç­”æ¡ˆ 90% çš„æ¦‚ç‡ï¼Œå‰©ä¸‹ 10% å‡åŒ€åˆ†é…ç»™å…¶ä»–é€‰é¡¹ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡ï¼‰ï¼š

```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, vocab_size, smoothing=0.1, padding_idx=0):
        super().__init__()
        self.smoothing = smoothing
        self.vocab_size = vocab_size
        self.padding_idx = padding_idx

    def forward(self, logits, target):
        log_probs = torch.log_softmax(logits, dim=-1)
        # å‡åŒ€åˆ†å¸ƒ
        smooth_loss = -log_probs.mean(dim=-1)
        # NLL loss
        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)
        # æ··åˆ
        loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss
        # å¿½ç•¥ padding
        mask = (target != self.padding_idx).float()
        return (loss * mask).sum() / mask.sum()
```

### 12.3 æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

```python
# åœ¨ loss.backward() ä¹‹åï¼Œoptimizer.step() ä¹‹å‰
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 12.4 å¸¸è§é”™è¯¯æ±‡æ€»

| é”™è¯¯                    | ç—‡çŠ¶                                 | è§£å†³æ–¹æ¡ˆ                                |
| ----------------------- | ------------------------------------ | --------------------------------------- |
| å¿˜äº†é™¤ä»¥ âˆšd_k           | è®­ç»ƒä¸æ”¶æ•›ï¼Œloss ä¸é™                | æ£€æŸ¥ attention çš„ scale                 |
| å› æœæ©ç æ–¹å‘åäº†        | æ¨¡å‹"ä½œå¼Š"ï¼Œè®­ç»ƒ loss æä½ä½†æ¨ç†ä¹±ç  | ç¡®ä¿ä¸Šä¸‰è§’è¢«é®ä½                        |
| ä½ç½®ç¼–ç ç»´åº¦ä¸åŒ¹é…      | RuntimeError: size mismatch          | æ£€æŸ¥ d_model ä¸€è‡´æ€§                     |
| æ²¡ç”¨ Teacher Forcing    | è®­ç»ƒææ…¢                             | è®­ç»ƒæ—¶ç”¨ ground truth ä½œä¸º Decoder è¾“å…¥ |
| Embedding æ²¡ä¹˜ âˆšd_model | ä½ç½®ç¼–ç ä¿¡å·ç›–è¿‡è¯åµŒå…¥               | åŠ ä¸Š `* math.sqrt(d_model)`             |
| æ¨ç†æ—¶æ²¡å…³ Dropout      | æ¯æ¬¡ç”Ÿæˆç»“æœä¸åŒ                     | `model.eval()`                          |
| Batch ä¸­åºåˆ—é•¿åº¦ä¸åŒ    | æŠ¥é”™æˆ–ç»“æœé”™è¯¯                       | æ­£ç¡®ä½¿ç”¨ padding + padding mask         |

### 12.5 Attention é•¿ä¸Šä¸‹æ–‡çš„ç¨€é‡Šé—®é¢˜

å½“åºåˆ—å¾ˆé•¿æ—¶ï¼ˆæ¯”å¦‚ GPT-4 æ”¯æŒ 128K tokenï¼‰ï¼Œæ³¨æ„åŠ›ä¼šè¢«**ç¨€é‡Š**ï¼šæ¯ä¸ªä½ç½®çš„æ³¨æ„åŠ›æƒé‡åˆ†æ•£åˆ°å¤ªå¤šä½ç½®ä¸Šï¼Œå¯¼è‡´å…³é”®ä¿¡æ¯è¢«æ·¹æ²¡ã€‚

å·²çŸ¥çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ï¼š

- **ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰**ï¼šåªå…³æ³¨å±€éƒ¨çª—å£ + å°‘é‡å…¨å±€ä½ç½®
- **FlashAttention**ï¼šç®—æ³•ä¼˜åŒ–ï¼Œå‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°
- **RoPE + NTK-aware Scaling**ï¼šè®©ä½ç½®ç¼–ç æ›´å¥½åœ°å¤–æ¨åˆ°é•¿åºåˆ—
- **Ring Attention**ï¼šåˆ†å¸ƒå¼åœºæ™¯ä¸‹çš„é•¿ä¸Šä¸‹æ–‡æ–¹æ¡ˆ

### 12.6 Prompt å·¥ç¨‹çš„åº•å±‚åŸç†ï¼ˆé¢„å‘Šï¼‰

ç†è§£äº† Transformer çš„å·¥ä½œåŸç†åï¼Œå¾ˆå¤š Prompt Engineering çš„æŠ€å·§å°±æœ‰äº†ç†è®ºè§£é‡Šã€‚ä¸‹ä¸€èŠ‚æˆ‘ä»¬ä¼š**æ·±å…¥å±•å¼€**è¿™ä¸ªè¯é¢˜ã€‚

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - å­¦ä¹ ç‡ Warmup æ˜¯ Transformer è®­ç»ƒçš„æ ‡é…
> - Label Smoothing é˜²æ­¢è¿‡åº¦è‡ªä¿¡ï¼Œæå‡æ³›åŒ–
> - æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
> - é•¿ä¸Šä¸‹æ–‡ç¨€é‡Šæ˜¯å½“å‰ç ”ç©¶çƒ­ç‚¹
> - ç†è§£ Transformer åŸç†èƒ½è®©ä½ å†™å‡ºæ›´å¥½çš„ Prompt

---

## åä¸‰ã€ä» Transformer åŸç†åˆ° LLM ä½¿ç”¨æŠ€å·§

> ğŸ¯ **æœ¬èŠ‚ç›®æ ‡**ï¼šç†è§£ Transformer åŸç†ä¸åªæ˜¯"å­¦æœ¯è¶£å‘³"â€”â€”å®ƒç›´æ¥å†³å®šäº†ä½ **å¦‚ä½•æ›´å¥½åœ°ä½¿ç”¨ ChatGPTã€Claudeã€DeepSeek ç­‰ LLM**ã€‚æœ¬èŠ‚å°†æ­ç¤ºæ¯ä¸€ä¸ª Prompt æŠ€å·§èƒŒåçš„ Transformer åŸç†ã€‚

### 13.1 ä¸ºä»€ä¹ˆå¼€å¤´å’Œç»“å°¾æœ€é‡è¦ï¼Ÿâ€”â€”é¦–å°¾è®°å¿†æ•ˆåº”

ä½ å¯èƒ½å¬è¿‡è¿™æ ·çš„å»ºè®®ï¼š"æŠŠæœ€é‡è¦çš„æŒ‡ä»¤æ”¾åœ¨ prompt çš„**å¼€å¤´æˆ–ç»“å°¾**"ã€‚è¿™ä¸æ˜¯ç„å­¦ï¼Œè€Œæ˜¯ç”± Transformer çš„æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥å†³å®šçš„ã€‚

**åŸç†è§£æï¼šå› æœæ³¨æ„åŠ›çš„ U å‹æ›²çº¿**

åœ¨ Decoder-Only æ¨¡å‹ä¸­ï¼Œæœ€åä¸€ä¸ª tokenï¼ˆå³å°†ç”Ÿæˆå›å¤çš„ä½ç½®ï¼‰å¯¹ä¹‹å‰æ‰€æœ‰ token è®¡ç®—æ³¨æ„åŠ›ã€‚ç”±äºï¼š

1. **å¼€å¤´çš„ token** å‚ä¸äº†åç»­æ‰€æœ‰ token çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œè¢«åå¤"å¼ºåŒ–"ï¼Œå½¢æˆäº†ç¨³å®šçš„è¡¨ç¤º
2. **ç»“å°¾çš„ token** åœ¨ä½ç½®ä¸Šæœ€æ¥è¿‘å½“å‰ç”Ÿæˆä½ç½®ï¼Œæ³¨æ„åŠ›æƒé‡å¤©ç„¶æ›´é«˜ï¼ˆè¿‘è·ç¦»åå¥½ï¼‰
3. **ä¸­é—´çš„ token** æ—¢æ²¡æœ‰å¼€å¤´çš„"ç´¯è®¡å¼ºåŒ–"ä¼˜åŠ¿ï¼Œä¹Ÿæ²¡æœ‰ç»“å°¾çš„"è·ç¦»è¿‘"ä¼˜åŠ¿

è¿™å½¢æˆäº†ä¸€ä¸ª **U å‹æ³¨æ„åŠ›åˆ†å¸ƒ**â€”â€”å¼€å¤´å’Œç»“å°¾è·å¾—æœ€å¤šå…³æ³¨ï¼Œä¸­é—´æœ€å®¹æ˜“è¢«"é—å¿˜"ã€‚

```
æ³¨æ„åŠ›å¼ºåº¦
  â–²
  â”‚  â–ˆâ–ˆ                                          â–ˆâ–ˆ
  â”‚  â–ˆâ–ˆ â–ˆâ–ˆ                                    â–ˆâ–ˆ â–ˆâ–ˆ
  â”‚  â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ                              â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ
  â”‚  â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ                        â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ
  â”‚  â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Token ä½ç½®
     å¼€å¤´        â† ä¸­é—´ï¼ˆæ³¨æ„åŠ›æœ€å¼±ï¼‰â†’        ç»“å°¾
```

2024 å¹´çš„è®ºæ–‡ _"Lost in the Middle"_ é€šè¿‡å®éªŒè¯å®äº†è¿™ä¸€ç°è±¡ã€‚

**âœ… å®è·µæŠ€å·§ï¼š**

```
âŒ å·®çš„ Prompt ç»“æ„ï¼š
"è¿™é‡Œæœ‰ä¸€å †èƒŒæ™¯ä¿¡æ¯â€¦â€¦ï¼ˆ10000 å­—ï¼‰â€¦â€¦
è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ï¼šxxx æ˜¯ä»€ä¹ˆï¼Ÿ"

âœ… å¥½çš„ Prompt ç»“æ„ï¼š
"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šxxx æ˜¯ä»€ä¹ˆï¼Ÿ

ã€å‚è€ƒèµ„æ–™å¼€å§‹ã€‘
è¿™é‡Œæœ‰ä¸€å †èƒŒæ™¯ä¿¡æ¯â€¦â€¦ï¼ˆ10000 å­—ï¼‰â€¦â€¦
ã€å‚è€ƒèµ„æ–™ç»“æŸã€‘

å†æ¬¡æé†’ï¼šè¯·ä¸¥æ ¼åŸºäºä»¥ä¸Šå‚è€ƒèµ„æ–™å›ç­” xxx æ˜¯ä»€ä¹ˆã€‚"
```

> ğŸ’¡ **ç™½è¯ç‰ˆ**ï¼šæŠŠ"ä»»åŠ¡æŒ‡ä»¤"æ”¾å¼€å¤´å’Œç»“å°¾ï¼ŒæŠŠ"å‚è€ƒèµ„æ–™"å¤¹åœ¨ä¸­é—´ã€‚è¿™æ ·æ¨¡å‹æœ€å…ˆå’Œæœ€åçœ‹åˆ°çš„éƒ½æ˜¯ä½ çš„æŒ‡ä»¤ï¼Œä¸å®¹æ˜“è·‘åã€‚

### 13.2 æ¸©åº¦å‚æ•°ï¼šSoftmax çš„"éŸ³é‡æ—‹é’®"

ä½ åœ¨ä½¿ç”¨ ChatGPT æˆ– Claude API æ—¶ï¼Œç»å¸¸ä¼šè®¾ç½®ä¸€ä¸ª `temperature` å‚æ•°ã€‚è¿˜è®°å¾—ç¬¬å››èŠ‚çš„ Softmax å—ï¼Ÿæ¸©åº¦å°±æ˜¯åœ¨ Softmax ä¹‹å‰å¤šäº†ä¸€æ­¥ï¼š

$$
P(w_i) = \frac{e^{z_i / T}}{\sum_j e^{z_j / T}}
$$

å…¶ä¸­ $T$ å°±æ˜¯æ¸©åº¦ï¼ˆTemperatureï¼‰ã€‚

**æ¸©åº¦å¦‚ä½•å½±å“è¾“å‡ºï¼š**

| æ¸©åº¦ T   | Softmax è¡Œä¸º           | è¾“å‡ºç‰¹ç‚¹                 | é€‚ç”¨åœºæ™¯                     |
| -------- | ---------------------- | ------------------------ | ---------------------------- |
| T â†’ 0    | æ¥è¿‘ argmaxï¼ˆone-hotï¼‰ | å‡ ä¹ç¡®å®šæ€§åœ°é€‰æœ€é«˜åˆ†è¯   | ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ã€äº‹å®é—®ç­” |
| T = 0.3  | é«˜å³°ï¼Œä½å¤šæ ·æ€§         | å¤§æ¦‚ç‡é€‰é«˜åˆ†è¯ï¼Œå¶å°”å˜åŒ– | ç¿»è¯‘ã€æ‘˜è¦ã€ç»“æ„åŒ–è¾“å‡º       |
| T = 1.0  | åŸå§‹åˆ†å¸ƒ               | æ¨¡å‹çš„"æœ¬è‰²"è¾“å‡º         | é€šç”¨å¯¹è¯ã€é»˜è®¤è®¾ç½®           |
| T = 1.5+ | åˆ†å¸ƒå˜å¹³å¦             | ä½åˆ†è¯ä¹Ÿæœ‰æœºä¼šè¢«é€‰ä¸­     | åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´           |

```python
import torch

logits = torch.tensor([5.0, 3.0, 1.0, 0.5, 0.1])
words = ["è‹¹æœ", "æ°´æœ", "é£Ÿç‰©", "ç”µè„‘", "å¤©æ°”"]

for T in [0.1, 0.5, 1.0, 2.0]:
    probs = torch.softmax(logits / T, dim=0)
    print(f"T={T:.1f}: ", end="")
    for w, p in zip(words, probs):
        print(f"{w}={p:.3f} ", end="")
    print()

# T=0.1:  è‹¹æœ=1.000 æ°´æœ=0.000 é£Ÿç‰©=0.000 ç”µè„‘=0.000 å¤©æ°”=0.000
# T=0.5:  è‹¹æœ=0.880 æ°´æœ=0.108 é£Ÿç‰©=0.010 ç”µè„‘=0.003 å¤©æ°”=0.001
# T=1.0:  è‹¹æœ=0.618 æ°´æœ=0.226 é£Ÿç‰©=0.031 ç”µè„‘=0.017 å¤©æ°”=0.012
# T=2.0:  è‹¹æœ=0.392 æ°´æœ=0.261 é£Ÿç‰©=0.113 ç”µè„‘=0.082 å¤©æ°”=0.063
```

> ğŸ’¡ **ç›´è§‰ç†è§£**ï¼šæ¸©åº¦å°±æ˜¯ Softmax çš„"éŸ³é‡æ—‹é’®"ï¼ˆç¬¬å››èŠ‚è®²è¿‡ï¼ï¼‰ã€‚ä½æ¸©åº¦ = åªå¬å£°éŸ³æœ€å¤§çš„ï¼Œé«˜æ¸©åº¦ = æ‰€æœ‰å£°éŸ³éƒ½å·®ä¸å¤šå¤§ã€‚

**âœ… å®è·µæŠ€å·§ï¼š**

- å†™ä»£ç  / åšæ•°å­¦ â†’ `temperature = 0`~`0.3`ï¼ˆä½ å¸Œæœ›æ¨¡å‹ç»™å‡ºæœ€"ç¡®å®š"çš„ç­”æ¡ˆï¼‰
- æ—¥å¸¸å¯¹è¯ â†’ `temperature = 0.7`~`1.0`ï¼ˆå¹³è¡¡å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦ï¼‰
- åˆ›æ„å†™ä½œ / å–åå­— â†’ `temperature = 1.0`~`1.5`ï¼ˆé¼“åŠ±"æ„æƒ³ä¸åˆ°"çš„ç»„åˆï¼‰

### 13.3 Top-K å’Œ Top-Pï¼šæ›´ç²¾ç»†çš„é‡‡æ ·æ§åˆ¶

é™¤äº†æ¸©åº¦ï¼ŒLLM è¿˜æœ‰ä¸¤ä¸ªé‡è¦çš„é‡‡æ ·å‚æ•°ï¼š

**Top-K é‡‡æ ·**ï¼ˆç™½è¯ç‰ˆï¼šåªä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ªè¯ä¸­é€‰æ‹©ï¼Œå…¶ä»–è¯ç›´æ¥æ’é™¤ï¼‰ï¼š

```
åŸå§‹åˆ†å¸ƒ: [è‹¹æœ=0.6, æ°´æœ=0.2, é£Ÿç‰©=0.1, ç”µè„‘=0.05, å¤©æ°”=0.03, ...]
Top-K=3:  [è‹¹æœ=0.67, æ°´æœ=0.22, é£Ÿç‰©=0.11]  â† åªä¿ç•™å‰ 3 ä¸ªï¼Œé‡æ–°å½’ä¸€åŒ–
```

**Top-P é‡‡æ ·ï¼ˆNucleus Samplingï¼‰**ï¼ˆç™½è¯ç‰ˆï¼šä»æ¦‚ç‡æœ€é«˜çš„è¯å¼€å§‹ç´¯åŠ ï¼Œç›´åˆ°ç´¯è®¡æ¦‚ç‡è¶…è¿‡ Pï¼Œåªä»è¿™äº›è¯ä¸­é€‰æ‹©ï¼‰ï¼š

```
åŸå§‹åˆ†å¸ƒ: [è‹¹æœ=0.6, æ°´æœ=0.2, é£Ÿç‰©=0.1, ç”µè„‘=0.05, ...]
Top-P=0.9: 0.6 + 0.2 + 0.1 = 0.9 â‰¥ P â†’ ä¿ç•™å‰ 3 ä¸ª
Top-P=0.8: 0.6 + 0.2 = 0.8 â‰¥ P â†’ åªä¿ç•™å‰ 2 ä¸ª
```

> ğŸ’¡ **ä¸ºä»€ä¹ˆ Top-P æ¯” Top-K æ›´æ™ºèƒ½ï¼Ÿ** å› ä¸º Top-K å›ºå®šé€‰ K ä¸ªè¯ï¼Œä½†æœ‰æ—¶æ¨¡å‹éå¸¸ç¡®ä¿¡ï¼ˆä¸€ä¸ªè¯å  99%ï¼‰ï¼Œæœ‰æ—¶å¾ˆçŠ¹è±«ï¼ˆå‰ 20 ä¸ªè¯éƒ½å·®ä¸å¤šï¼‰ã€‚Top-P èƒ½è‡ªé€‚åº”â€”â€”ç¡®ä¿¡æ—¶åªå– 1-2 ä¸ªè¯ï¼ŒçŠ¹è±«æ—¶å–æ›´å¤šè¯ã€‚

### 13.4 Chain-of-Thoughtï¼šä¸ºä»€ä¹ˆ"å…ˆæƒ³åç­”"æ•ˆæœå¥½ï¼Ÿ

**Chain-of-Thought (CoT)**ï¼ˆç™½è¯ç‰ˆï¼šè®© LLM å…ˆå†™å‡ºæ¨ç†æ­¥éª¤ï¼Œå†ç»™æœ€ç»ˆç­”æ¡ˆï¼‰æ˜¯ç›®å‰æœ€é‡è¦çš„ Prompt æŠ€å·§ä¹‹ä¸€ã€‚å®ƒçš„æ•ˆæœæƒŠäººï¼Œä½†åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ

**Transformer è§†è§’çš„è§£é‡Šï¼š**

åœ¨ Decoder-Only æ¨¡å‹ä¸­ï¼Œç”Ÿæˆç¬¬ N ä¸ª token æ—¶ï¼Œå®ƒåªèƒ½é€šè¿‡**æ³¨æ„åŠ›æœºåˆ¶**ä»å‰é¢çš„ token ä¸­è·å–ä¿¡æ¯ã€‚

```
ä¸ç”¨ CoT:
"9876 Ã— 4321 = "  â†’  æ¨¡å‹éœ€è¦ä¸€æ­¥ä»é—®é¢˜ç›´æ¥è·³åˆ°ç­”æ¡ˆ
                      æ³¨æ„åŠ›å¿…é¡»è·¨è¶Šå¾ˆé•¿çš„"æ¨ç†é“¾"

ç”¨ CoT:
"9876 Ã— 4321
å…ˆç®— 9876 Ã— 1 = 9876
å†ç®— 9876 Ã— 20 = 197520
å†ç®— 9876 Ã— 300 = 2962800
å†ç®— 9876 Ã— 4000 = 39504000
åŠ èµ·æ¥: 9876 + 197520 + 2962800 + 39504000 = 42674196"
â†’ æ¯ä¸€æ­¥éƒ½ä¸ºä¸‹ä¸€æ­¥æä¾›äº†"è¿‘è·ç¦»çš„ä¸Šä¸‹æ–‡"
```

**æ ¸å¿ƒåŸç†ï¼šCoT åˆ›é€ äº†æ³¨æ„åŠ›çš„"å«è„šçŸ³"**

æ²¡æœ‰ CoT æ—¶ï¼Œæœ€ç»ˆç­”æ¡ˆéœ€è¦"è¿œè·ç¦»"ä»é—®é¢˜ä¸­æå–ä¿¡æ¯ã€‚æœ‰äº† CoTï¼Œä¸­é—´æ­¥éª¤å°±åƒä¸€è¿ä¸²å«è„šçŸ³â€”â€”æ¯ä¸€æ­¥åªéœ€è¦ä»**æœ€è¿‘çš„å‡ ä¸ª token** ä¸­è·å–ä¿¡æ¯ï¼Œæ³¨æ„åŠ›ä¸éœ€è¦è·¨è¶Šå¤ªè¿œã€‚

> ğŸ’¡ **ç™½è¯ç‰ˆ**ï¼šCoT å°±åƒåšæ•°å­¦é¢˜æ—¶çš„"è‰ç¨¿çº¸"â€”â€”ä½ ä¸æ˜¯ç›´æ¥å†™ç­”æ¡ˆï¼Œè€Œæ˜¯æŠŠä¸­é—´æ­¥éª¤å†™ä¸‹æ¥ã€‚æ¯ä¸€æ­¥ä¸­é—´ç»“æœéƒ½ä¼šè¢«"å­˜å‚¨"åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæˆä¸ºä¸‹ä¸€æ­¥çš„"è¿‘è·ç¦»å‚è€ƒ"ã€‚

**âœ… å®è·µæŠ€å·§ï¼š**

```
âŒ "è¯·å›ç­”ï¼šå¦‚æœ A æ¯” B å¤§ï¼ŒB æ¯” C å¤§ï¼ŒC æ¯” D å¤§ï¼Œé‚£ A å’Œ D è°å¤§ï¼Ÿ"

âœ… "è¯·ä¸€æ­¥ä¸€æ­¥æ€è€ƒï¼š
   å¦‚æœ A æ¯” B å¤§ï¼ŒB æ¯” C å¤§ï¼ŒC æ¯” D å¤§ï¼Œé‚£ A å’Œ D è°å¤§ï¼Ÿ
   è¯·å…ˆåˆ—å‡ºå·²çŸ¥æ¡ä»¶ï¼Œç„¶åé€æ­¥æ¨ç†ã€‚"
```

### 13.5 Few-Shot ç¤ºä¾‹ï¼šIn-Context Learning çš„æ³¨æ„åŠ›æœ¬è´¨

ç»™ LLM ä¸€äº›ç¤ºä¾‹å°±èƒ½è®©å®ƒåšæ–°ä»»åŠ¡â€”â€”è¿™ç§ **In-Context Learning** èƒ½åŠ›çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ

**Transformer è§†è§’ï¼šç¤ºä¾‹è®¾å®šäº†æ³¨æ„åŠ›æ¨¡å¼**

å½“ä½ ç»™æ¨¡å‹å‡ ä¸ª `è¾“å…¥â†’è¾“å‡º` çš„ç¤ºä¾‹æ—¶ï¼š

```
è¯·å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘ä¸ºä¸­æ–‡ï¼š
è‹±æ–‡: Hello â†’ ä¸­æ–‡: ä½ å¥½
è‹±æ–‡: Thank you â†’ ä¸­æ–‡: è°¢è°¢
è‹±æ–‡: Good morning â†’ ä¸­æ–‡:
```

æ¨¡å‹åœ¨ç”Ÿæˆæœ€åä¸€è¡Œçš„è¾“å‡ºæ—¶ï¼Œæ³¨æ„åŠ›ä¼šï¼š

1. å‘ç°å‰é¢æœ‰**é‡å¤çš„æ¨¡å¼**ï¼ˆ"è‹±æ–‡: X â†’ ä¸­æ–‡: Y"ï¼‰
2. å¯¹è¿™äº›æ¨¡å¼å½¢æˆ**æ³¨æ„åŠ›é”šç‚¹**
3. å°† "Good morning" çš„ Query ä¸å‰é¢ç¤ºä¾‹çš„ Key åšåŒ¹é…
4. åˆ©ç”¨åŒ¹é…åˆ°çš„æ¨¡å¼æ¥ç”Ÿæˆå¯¹åº”çš„ç¿»è¯‘

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ few-shot ç¤ºä¾‹è¶Šå¤šã€è¶Šä¸€è‡´ï¼Œæ•ˆæœå°±è¶Šå¥½â€”â€”å®ƒä»¬ä¸ºæ¨¡å‹å»ºç«‹äº†æ›´ç¨³å®šçš„**æ³¨æ„åŠ›æ¨¡å¼æ¨¡æ¿**ã€‚

**âœ… å®è·µæŠ€å·§ï¼š**

- ç¤ºä¾‹æ ¼å¼è¦**å®Œå…¨ä¸€è‡´**ï¼ˆè®©æ³¨æ„åŠ›æ›´å®¹æ˜“å‘ç°æ¨¡å¼ï¼‰
- ç¤ºä¾‹æ•°é‡ 3-5 ä¸ªé€šå¸¸æœ€ä½³ï¼ˆå¤ªå°‘æ¨¡å¼ä¸ç¨³å®šï¼Œå¤ªå¤šæµªè´¹ä¸Šä¸‹æ–‡ï¼‰
- ç¤ºä¾‹çš„**é¡ºåºå¾ˆé‡è¦**ï¼šå’Œä½ çš„é—®é¢˜æœ€ç›¸ä¼¼çš„ç¤ºä¾‹æ”¾åœ¨æœ€åï¼ˆæœ€è¿‘çš„æ³¨æ„åŠ›æ›´å¼ºï¼‰

### 13.6 ç»“æ„åŒ– Promptï¼šæ³¨æ„åŠ›çš„"åˆ†éš”ç¬¦"

ä¸ºä»€ä¹ˆä½¿ç”¨ `---`ã€`###`ã€XML æ ‡ç­¾ç­‰åˆ†éš”ç¬¦èƒ½æå‡æ•ˆæœï¼Ÿ

**Transformer è§†è§’ï¼š**

åˆ†éš”ç¬¦åœ¨ token åºåˆ—ä¸­åˆ›é€ äº†**æ³¨æ„åŠ›æ–­ç‚¹**ã€‚å½“æ¨¡å‹çœ‹åˆ° `---` æˆ– `###` æ—¶ï¼Œè¿™äº›ç‰¹æ®Š token å½¢æˆäº†æ³¨æ„åŠ›çš„"å¢™å£"â€”â€”å‰åå†…å®¹çš„æ³¨æ„åŠ›æ›´å€¾å‘äº**åœ¨åŒä¸€ä¸ªåŒºåŸŸå†…éƒ¨**æµåŠ¨ï¼Œè€Œä¸æ˜¯è·¨åŒºåŸŸæ··æ‚ã€‚

```
âŒ æ— ç»“æ„ï¼š
"ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¸“å®¶è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘ä¸ºä¸­æ–‡Hello Worldè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"
â†’ æ‰€æœ‰ token çš„æ³¨æ„åŠ›æ··åœ¨ä¸€èµ·ï¼Œæ¨¡å‹ä¸ç¡®å®šå“ªäº›æ˜¯æŒ‡ä»¤ã€å“ªäº›æ˜¯å¾…ç¿»è¯‘å†…å®¹

âœ… æœ‰ç»“æ„ï¼š
"ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¸“å®¶ã€‚
---
è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘ä¸ºä¸­æ–‡ï¼š
---
Hello World
è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"
â†’ åˆ†éš”ç¬¦å¸®åŠ©æ³¨æ„åŠ›"åˆ†åŒºç®¡ç†"
```

**âœ… æœ€ä½³å®è·µâ€”â€”System Prompt çš„é»„é‡‘ç»“æ„ï¼š**

```
ã€è§’è‰²å®šä¹‰ã€‘ä½ æ˜¯ä¸€ä¸ª xxx ä¸“å®¶ã€‚

ã€ä»»åŠ¡æè¿°ã€‘è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
1. xxx
2. xxx

ã€çº¦æŸæ¡ä»¶ã€‘
- è¾“å‡ºæ ¼å¼ï¼šJSON
- è¯­è¨€ï¼šä¸­æ–‡
- é•¿åº¦ï¼šä¸è¶…è¿‡ 200 å­—

ã€è¾“å…¥å†…å®¹ã€‘
{ç”¨æˆ·çš„å®é™…è¾“å…¥}

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºã€‚
```

### 13.7 ä¸Šä¸‹æ–‡çª—å£ä¸ "Lost in the Middle"

ç°ä»£ LLM æ”¯æŒè¶Šæ¥è¶Šé•¿çš„ä¸Šä¸‹æ–‡â€”â€”GPT-4 Turbo æ”¯æŒ 128Kï¼ŒGemini 2.0 Pro æ”¯æŒ 2M tokenã€‚ä½†æ›´é•¿ä¸ä¸€å®šæ›´å¥½ã€‚

**é—®é¢˜ï¼šæ³¨æ„åŠ›ç¨€é‡Š**

å›å¿† Softmax çš„æ€§è´¨ï¼šæ‰€æœ‰æ³¨æ„åŠ›æƒé‡åŠ èµ·æ¥ç­‰äº 1ã€‚å½“ä¸Šä¸‹æ–‡æœ‰ N ä¸ª token æ—¶ï¼Œå¹³å‡æ¯ä¸ª token åªèƒ½è·å¾— $1/N$ çš„æ³¨æ„åŠ›ã€‚

```
ä¸Šä¸‹æ–‡ 100 token:   æ¯ä¸ª token å¹³å‡ 1% çš„æ³¨æ„åŠ› â†’ å…³é”®ä¿¡æ¯å®¹æ˜“è¢«"çœ‹åˆ°"
ä¸Šä¸‹æ–‡ 10000 token:  æ¯ä¸ª token å¹³å‡ 0.01% çš„æ³¨æ„åŠ› â†’ å…³é”®ä¿¡æ¯å®¹æ˜“è¢«"æ·¹æ²¡"
ä¸Šä¸‹æ–‡ 100000 token: æ¯ä¸ª token å¹³å‡ 0.001% çš„æ³¨æ„åŠ› â†’ å¤§æµ·æé’ˆï¼
```

**âœ… å®è·µæŠ€å·§ï¼š**

- åˆ«ä¸€è‚¡è„‘æŠŠæ‰€æœ‰ä¿¡æ¯å¡è¿›ä¸Šä¸‹æ–‡â€”â€”**ç²¾é€‰æœ€ç›¸å…³çš„å†…å®¹**
- å…³é”®ä¿¡æ¯æ”¾åœ¨**å¼€å¤´å’Œç»“å°¾**ï¼ˆU å‹æ³¨æ„åŠ›æ›²çº¿ï¼‰
- ä½¿ç”¨ **RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰** åªæ£€ç´¢æœ€ç›¸å…³çš„ç‰‡æ®µï¼Œè€Œä¸æ˜¯å¡å…¥æ•´ä¸ªæ–‡æ¡£
- å¦‚æœå¿…é¡»ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡ï¼Œç”¨**åˆ†éš”ç¬¦å’Œæ ‡é¢˜**å¸®åŠ©æ¨¡å‹"ç´¢å¼•"

### 13.8 ä¸ºä»€ä¹ˆ"é‡å¤æŒ‡ä»¤"æœ‰æ•ˆï¼Ÿ

ä½ å¯èƒ½æ³¨æ„åˆ°ï¼Œåœ¨é•¿ prompt ä¸­**é‡å¤å…³é”®æŒ‡ä»¤**æ•ˆæœæ›´å¥½ã€‚åŸç†å¾ˆç®€å•ï¼š

1. **å¤šä¸ªä½ç½®çš„æ³¨æ„åŠ›å åŠ **ï¼šå…³é”®æŒ‡ä»¤å‡ºç°åœ¨å¤šä¸ªä½ç½®ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆæ—¶æœ‰æ›´å¤š"é”šç‚¹"å¯ä»¥æ³¨æ„åˆ°
2. **å¯¹æŠ—ç¨€é‡Š**ï¼šåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­ï¼Œå•ä¸ªä½ç½®çš„æ³¨æ„åŠ›å¾ˆå¼±ï¼Œä½†å¤šæ¬¡é‡å¤è®©æ€»æ³¨æ„åŠ›å¢å¼º
3. **é¦–å°¾æ•ˆåº”å¼ºåŒ–**ï¼šå¦‚æœæŒ‡ä»¤åœ¨å¼€å¤´å’Œç»“å°¾éƒ½å‡ºç°ï¼Œå°±åˆ©ç”¨äº† U å‹æ³¨æ„åŠ›æ›²çº¿çš„ä¸¤ä¸ªå³°å€¼

```
âœ… "è¯·ç”¨ä¸­æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚
   [å¤§é‡å‚è€ƒèµ„æ–™...]
   å†æ¬¡æé†’ï¼šè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
   é—®é¢˜æ˜¯ï¼šxxxï¼Ÿ
   æ³¨æ„ï¼šå›ç­”å¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚"
```

### 13.9 æŠ€å·§é€ŸæŸ¥è¡¨

| æŠ€å·§             | Transformer åŸç†            | æ¨èåšæ³•                             |
| ---------------- | --------------------------- | ------------------------------------ |
| æŒ‡ä»¤æ”¾é¦–å°¾       | U å‹æ³¨æ„åŠ›æ›²çº¿ï¼ˆé¦–å°¾æ•ˆåº”ï¼‰  | ä»»åŠ¡æè¿°æ”¾å¼€å¤´ + ç»“å°¾é‡å¤å…³é”®çº¦æŸ    |
| ä½æ¸©åº¦           | Softmax å˜å°–é” â†’ ç¡®å®šæ€§è¾“å‡º | ä»£ç /æ•°å­¦ç”¨ T=0~0.3                  |
| é«˜æ¸©åº¦           | Softmax å˜å¹³å¦ â†’ å¤šæ ·æ€§è¾“å‡º | åˆ›æ„å†™ä½œç”¨ T=0.7~1.5                 |
| Chain-of-Thought | ä¸­é—´ token ä½œä¸ºæ³¨æ„åŠ›å«è„šçŸ³ | "è¯·ä¸€æ­¥ä¸€æ­¥æ€è€ƒ"                     |
| Few-shot ç¤ºä¾‹    | å»ºç«‹æ³¨æ„åŠ›æ¨¡å¼æ¨¡æ¿          | 3-5 ä¸ªæ ¼å¼ä¸€è‡´çš„ç¤ºä¾‹                 |
| ç»“æ„åŒ–åˆ†éš”ç¬¦     | åˆ›é€ æ³¨æ„åŠ›è¾¹ç•Œï¼Œåˆ†åŒºç®¡ç†    | ç”¨ `---`ã€`###`ã€XML æ ‡ç­¾åˆ†éš”        |
| ç²¾ç®€ä¸Šä¸‹æ–‡       | å‡å°‘æ³¨æ„åŠ›ç¨€é‡Š              | ç”¨ RAG æ£€ç´¢è€Œéå…¨æ–‡å¡å…¥              |
| é‡å¤å…³é”®æŒ‡ä»¤     | å¤šä½ç½®æ³¨æ„åŠ›å åŠ             | é‡è¦çº¦æŸåœ¨å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾å„å‡ºç°ä¸€æ¬¡ |
| Top-P é‡‡æ ·       | è‡ªé€‚åº”æˆªæ–­ä½æ¦‚ç‡è¯          | API è°ƒç”¨è®¾ top_p=0.9                 |
| ç¤ºä¾‹æ”¾æœ€å       | è¿‘è·ç¦»æ³¨æ„åŠ›æ›´å¼º            | æœ€ç›¸ä¼¼çš„ç¤ºä¾‹æ”¾åœ¨ prompt æœ«å°¾         |

> ğŸ¨ **äº¤äº’å¼æ¼”ç¤º**ï¼šä¸‹é¢çš„å¯è§†åŒ–é€šè¿‡ 4 ä¸ªäº’åŠ¨ Demo å±•ç¤ºè¿™äº›åŸç†â€”â€”ä½“éªŒé¦–å°¾è®°å¿†æ•ˆåº”ã€æ¸©åº¦å‚æ•°è°ƒèŠ‚ã€ä¸Šä¸‹æ–‡ç¨€é‡Šå’Œ CoT æ³¨æ„åŠ›æµåŠ¨ã€‚

<IframeEmbed
  src="/visualizations/transformer-llm-tips.html"
  minHeight={900}
  title="LLM ä½¿ç”¨æŠ€å·§ä¸ Transformer åŸç†å¯è§†åŒ–"
/>

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - é¦–å°¾è®°å¿†æ•ˆåº” â†’ é‡è¦æŒ‡ä»¤æ”¾å¼€å¤´å’Œç»“å°¾
> - æ¸©åº¦ = Softmax çš„éŸ³é‡æ—‹é’® â†’ ä¸åŒä»»åŠ¡ç”¨ä¸åŒæ¸©åº¦
> - Top-K / Top-P = å€™é€‰è¯çš„ç­›é€‰ç­–ç•¥
> - CoT = ä¸ºæ³¨æ„åŠ›åˆ›é€ "å«è„šçŸ³"
> - Few-shot = å»ºç«‹æ³¨æ„åŠ›æ¨¡å¼æ¨¡æ¿
> - ç»“æ„åŒ– Prompt = åˆ›é€ æ³¨æ„åŠ›åˆ†åŒº
> - é•¿ä¸Šä¸‹æ–‡ â‰  å¥½ä¸Šä¸‹æ–‡ï¼Œç²¾é€‰æœ€ç›¸å…³çš„ä¿¡æ¯
> - ç†è§£ Transformer åŸç†ï¼Œè®©ä½ ä»"è°ƒå‚ç„å­¦"å˜æˆ"æœ‰ç†æœ‰æ®çš„å·¥ç¨‹"

---

## åå››ã€2025-2026 Transformer å‰æ²¿è¶‹åŠ¿

> ğŸ¯ **æœ¬èŠ‚ç›®æ ‡**ï¼šäº†è§£ Transformer æ¶æ„åœ¨ 2025-2026 å¹´çš„æœ€æ–°è¿›å±•å’Œæœªæ¥æ–¹å‘ã€‚å³ä½¿ä½ ä¸åšæ¨¡å‹ç ”å‘ï¼Œäº†è§£è¿™äº›è¶‹åŠ¿ä¹Ÿèƒ½å¸®ä½ ç†è§£"ä¸ºä»€ä¹ˆ AI è¶Šæ¥è¶Šå¼ºã€è¶Šæ¥è¶Šä¾¿å®œ"ã€‚

### 14.1 æ³¨æ„åŠ›æœºåˆ¶çš„æ•ˆç‡é©å‘½

åŸå§‹ Transformer çš„æ³¨æ„åŠ›å¤æ‚åº¦æ˜¯ **O(NÂ²)**â€”â€”N æ˜¯åºåˆ—é•¿åº¦ã€‚è¿™æ„å‘³ç€ä¸Šä¸‹æ–‡ä» 4K ç¿»åˆ° 128Kï¼Œè®¡ç®—é‡ç¿»äº† 1024 å€ï¼ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ2024-2025 å¹´æ¶Œç°äº†å¤§é‡æ³¨æ„åŠ›ä¼˜åŒ–æŠ€æœ¯ï¼š

**FlashAttention ç³»åˆ—ï¼šè®©ç¡¬ä»¶è·‘æ»¡**

| ç‰ˆæœ¬             | å¹´ä»½ | GPU åˆ©ç”¨ç‡      | æ ¸å¿ƒåˆ›æ–°                            |
| ---------------- | ---- | --------------- | ----------------------------------- |
| FlashAttention-1 | 2022 | ~50%            | IO-aware åˆ†å—è®¡ç®—ï¼Œå‡å°‘å†…å­˜è¯»å†™     |
| FlashAttention-2 | 2023 | ~72%            | æ›´å¥½çš„å¹¶è¡ŒåŒ–ï¼Œæ”¯æŒå¤šç§æ³¨æ„åŠ›å˜ä½“    |
| FlashAttention-3 | 2024 | **~85%** (BF16) | å¼‚æ­¥æµæ°´çº¿ã€FP8 é‡åŒ–ã€H100 æ·±åº¦ä¼˜åŒ– |

FlashAttention æ²¡æœ‰æ”¹å˜æ³¨æ„åŠ›çš„æ•°å­¦â€”â€”å®ƒçš„è¾“å‡ºå’Œæ ‡å‡†æ³¨æ„åŠ›**å®Œå…¨ä¸€è‡´**ã€‚å®ƒåªæ˜¯é€šè¿‡å·§å¦™çš„å†…å­˜ç®¡ç†ï¼Œè®© GPU çš„åˆ©ç”¨ç‡ä» 35% æå‡åˆ° 85%ã€‚è¿™å°±åƒåŒä¸€æ¡é«˜é€Ÿå…¬è·¯ï¼Œé€šè¿‡æ›´å¥½çš„äº¤é€šç®¡ç†è®©é€šè½¦é‡ç¿»å€ã€‚

> ğŸ’¡ FlashAttention æ˜¯ LLM ä¸Šä¸‹æ–‡é•¿åº¦ä» 2Kâ†’4Kâ†’128Kâ†’1M çš„æ ¸å¿ƒæ¨åŠ¨åŠ›ä¹‹ä¸€ã€‚æ²¡æœ‰å®ƒï¼Œ128K ä¸Šä¸‹æ–‡çš„ GPT-4 åœ¨å½“å‰ç¡¬ä»¶ä¸Šå‡ ä¹ä¸å¯èƒ½å®ç°ã€‚

**Ring Attentionï¼šåˆ†å¸ƒå¼é•¿ä¸Šä¸‹æ–‡**

å½“ä¸€å— GPU æ”¾ä¸ä¸‹æ•´ä¸ªæ³¨æ„åŠ›çŸ©é˜µæ—¶ï¼ŒRing Attention å°†æ³¨æ„åŠ›è®¡ç®—**åˆ†æ•£åˆ°å¤šå— GPU ä¸Š**ï¼Œæ¯å— GPU åªè®¡ç®—ä¸€éƒ¨åˆ†ï¼Œç„¶ååƒ"ä¼ ç¯"ä¸€æ ·ä¼ é€’ä¸­é—´ç»“æœã€‚è¿™ä½¿å¾—ç™¾ä¸‡çº§ token çš„ä¸Šä¸‹æ–‡æˆä¸ºå¯èƒ½ã€‚

**çº¿æ€§æ³¨æ„åŠ›ï¼šä» O(NÂ²) åˆ° O(N)**

| æ–¹æ³•             | å¤æ‚åº¦              | ç²¾ç¡®?   | çŠ¶æ€                          |
| ---------------- | ------------------- | ------- | ----------------------------- |
| æ ‡å‡†æ³¨æ„åŠ›       | O(NÂ²)               | âœ…      | åŸºçº¿                          |
| FlashAttention   | O(NÂ²)ï¼ˆIO ä¼˜åŒ–ï¼‰    | âœ…      | ç”Ÿäº§å°±ç»ª                      |
| çº¿æ€§æ³¨æ„åŠ›       | **O(N)**            | âŒ è¿‘ä¼¼ | å¿«é€Ÿæˆç†Ÿä¸­                    |
| Flash-çº¿æ€§æ³¨æ„åŠ› | **O(N)**ï¼ˆIO ä¼˜åŒ–ï¼‰ | âŒ è¿‘ä¼¼ | 2025 å·²è¿›å…¥ç”Ÿäº§ï¼ˆQwen3-Nextï¼‰ |

çº¿æ€§æ³¨æ„åŠ›é€šè¿‡**æ ¸å‡½æ•°æŠ€å·§**å’Œ**çŸ©é˜µä¹˜æ³•ç»“åˆå¾‹**ï¼Œé¿å…è®¡ç®—å®Œæ•´çš„ NÃ—N æ³¨æ„åŠ›çŸ©é˜µã€‚è™½ç„¶æ˜¯è¿‘ä¼¼çš„ï¼Œä½†åœ¨ 2025 å¹´å·²ç»æˆç†Ÿåˆ°è¿›å…¥ç”Ÿäº§æ¨¡å‹ã€‚

### 14.2 Mamba ä¸ SSMï¼šTransformer çš„æŒ‘æˆ˜è€…

**çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆState Space Model, SSMï¼‰** æ˜¯è¿‘å¹´æ¥æœ€æœ‰åŠ›çš„ Transformer æŒ‘æˆ˜è€…ã€‚å…¶ä¸­æœ€çŸ¥åçš„æ˜¯ **Mamba**ã€‚

**æ ¸å¿ƒæ€æƒ³å¯¹æ¯”ï¼š**

```
Transformerï¼ˆæ³¨æ„åŠ›ï¼‰:
  æ¯ä¸ª token éƒ½å¯ä»¥"å›å¤´çœ‹"æ‰€æœ‰ä¹‹å‰çš„ token
  â†’ ä¿¡æ¯é€šè¿‡ å…¨å±€æ³¨æ„åŠ›çŸ©é˜µ ä¼ é€’
  â†’ ä»£ä»·ï¼šO(NÂ²) è®¡ç®—ï¼ŒO(N) å†…å­˜

Mambaï¼ˆé€‰æ‹©æ€§ SSMï¼‰:
  æŠŠ"è®°å¿†"å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå¤§å°çš„éšçŠ¶æ€ä¸­
  â†’ ä¿¡æ¯é€šè¿‡ çŠ¶æ€é€’æ¨ ä¼ é€’ï¼ˆç±»ä¼¼å‡çº§ç‰ˆ RNNï¼‰
  â†’ ä»£ä»·ï¼šO(N) è®¡ç®—ï¼ŒO(1) æ¯æ­¥å†…å­˜
```

**Mamba çš„"é€‰æ‹©æ€§"åˆ›æ–°ï¼š**

ä¼ ç»Ÿ SSM å¯¹æ‰€æœ‰è¾“å…¥ä¸€è§†åŒä»ï¼ˆçŠ¶æ€è½¬ç§»çŸ©é˜µæ˜¯å›ºå®šçš„ï¼‰ã€‚Mamba çš„çªç ´åœ¨äºè®©çŠ¶æ€è½¬ç§»**ä¾èµ–äºè¾“å…¥å†…å®¹**â€”â€”æ¨¡å‹å¯ä»¥æ ¹æ®å½“å‰ token çš„å†…å®¹å†³å®šè¦"è®°ä½"ä»€ä¹ˆã€"é—å¿˜"ä»€ä¹ˆã€‚è¿™ä½¿å¾— Mamba åœ¨è¯­è¨€å»ºæ¨¡ä¸Šé¦–æ¬¡åŒ¹é…ç”šè‡³è¶…è¶Šäº†åŒè§„æ¨¡çš„ Transformerã€‚

**ä½† Transformer å¹¶æ²¡æœ‰è¢«å–ä»£ï¼š**

| ç»´åº¦       | Transformer       | Mamba/SSM              |
| ---------- | ----------------- | ---------------------- |
| é•¿åºåˆ—æ•ˆç‡ | O(NÂ²)ï¼Œé•¿åºåˆ—æ˜‚è´µ | O(N)ï¼Œçº¿æ€§æ‰©å±•         |
| ä¿¡æ¯æ£€ç´¢   | å¼ºï¼ˆå…¨å±€æ³¨æ„åŠ›ï¼‰  | å¼±ï¼ˆæœ‰é™çš„çŠ¶æ€å®¹é‡ï¼‰   |
| å¹¶è¡Œè®­ç»ƒ   | âœ… å¤©ç„¶å¹¶è¡Œ       | âœ…ï¼ˆç»“æ„åŒ–é€’æ¨å¯å¹¶è¡Œï¼‰ |
| ä¸Šä¸‹æ–‡å­¦ä¹  | âœ… å¼º ICL èƒ½åŠ›    | ç›¸å¯¹è¾ƒå¼±               |
| å®é™…éƒ¨ç½²   | æåº¦æˆç†Ÿ          | å¿«é€Ÿå¢é•¿ä¸­             |

### 14.3 æ··åˆæ¶æ„ï¼šå–å„å®¶ä¹‹é•¿

2025-2026 æœ€æ˜ç¡®çš„è¶‹åŠ¿æ˜¯ **æ··åˆæ¶æ„**â€”â€”ç»“åˆ Transformer å’Œ SSM çš„ä¼˜ç‚¹ï¼š

```
Hybrid Architectureï¼ˆæ··åˆæ¶æ„ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº•å±‚ï¼šMamba/SSM å±‚ï¼ˆå¤„ç†é•¿è·ç¦»ä¾èµ–ï¼‰    â”‚
â”‚  â†“                                      â”‚
â”‚  ä¸­é—´ï¼šç©¿æ’å°‘é‡æ³¨æ„åŠ›å±‚ï¼ˆå…¨å±€ä¿¡æ¯æ£€ç´¢ï¼‰    â”‚
â”‚  â†“                                      â”‚
â”‚  é¡¶å±‚ï¼šæ³¨æ„åŠ›å±‚ï¼ˆå…³é”®å†³ç­–å’Œç”Ÿæˆï¼‰         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ æ—¢æœ‰ SSM çš„çº¿æ€§æ•ˆç‡ï¼Œåˆæœ‰æ³¨æ„åŠ›çš„å…¨å±€æ£€ç´¢èƒ½åŠ›
```

ä»£è¡¨æ¨¡å‹ï¼š**Jamba**ï¼ˆAI21 Labsï¼ŒMamba + Transformer æ··åˆï¼‰ã€**Mamba-2**ï¼ˆå¯ä»¥åŒæ—¶çœ‹ä½œ SSM å’Œçº¿æ€§æ³¨æ„åŠ›ï¼‰ã€‚

### 14.4 å…¶ä»–å€¼å¾—å…³æ³¨çš„æ–¹å‘

**â‘  Titansï¼šå¯åœ¨æ¨ç†æ—¶å­¦ä¹ çš„æ¶æ„**

Google Research çš„ Titans æ¶æ„å¼•å…¥äº†**ç¥ç»é•¿æœŸè®°å¿†ï¼ˆNeural Long-Term Memoryï¼‰** æ¨¡å—ï¼Œè®©æ¨¡å‹åœ¨æ¨ç†æ—¶èƒ½"å­¦ä¹ "æ–°ä¿¡æ¯â€”â€”è€Œä¸ä»…ä»…ä¾èµ–è®­ç»ƒæ—¶å­¦åˆ°çš„çŸ¥è¯†ã€‚è¿™è§£å†³äº† Transformer çš„ä¸€ä¸ªæ ¹æœ¬é™åˆ¶ï¼šä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹çš„çŸ¥è¯†å°±æ˜¯å›ºå®šçš„ã€‚

**â‘¡ æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDiffusion LLMï¼‰**

ä¼ ç»Ÿ LLM é€è¯ç”Ÿæˆï¼ˆè‡ªå›å½’ï¼‰ï¼Œè€Œæ‰©æ•£æ¨¡å‹å¯ä»¥**å¹¶è¡Œç”Ÿæˆ**æ•´æ®µæ–‡æœ¬ï¼Œç„¶åé€æ­¥"å»å™ª"ç²¾ç‚¼ã€‚è¿™èƒ½å¤§å¹…é™ä½æ¨ç†å»¶è¿Ÿï¼Œä½†ç›®å‰åœ¨æ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸Šè¿˜ä¸å¦‚è‡ªå›å½’æ¨¡å‹ã€‚

**â‘¢ MoE æˆä¸ºä¸»æµ**

2025 å¹´ï¼Œå‡ ä¹æ‰€æœ‰æ–°å‘å¸ƒçš„å¤§æ¨¡å‹éƒ½é‡‡ç”¨äº† **MoEï¼ˆæ··åˆä¸“å®¶ï¼‰** æ¶æ„ã€‚DeepSeek V3 çš„æˆåŠŸè¯æ˜äº† MoE å¯ä»¥ç”¨ 1/10 çš„è®¡ç®—é‡è¾¾åˆ°å¯†é›†æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸æ˜¯ Transformer çš„æ›¿ä»£å“ï¼Œè€Œæ˜¯ Transformer çš„**å‡çº§é…ä»¶**ã€‚

### 14.5 ä¸€å¼ å›¾çœ‹æœªæ¥

```
2017 â”€â”€â”€â”€ åŸå§‹ Transformerï¼ˆAttention Is All You Needï¼‰
  â”‚
  â”œâ”€â”€ 2018 BERTï¼ˆEncoder-Only, åŒå‘æ³¨æ„åŠ›ï¼‰
  â”œâ”€â”€ 2018 GPT-1ï¼ˆDecoder-Only, å› æœæ³¨æ„åŠ›ï¼‰
  â”‚
  â”œâ”€â”€ 2020 GPT-3ï¼ˆè§„æ¨¡ + In-Context Learningï¼‰
  â”œâ”€â”€ 2022 FlashAttentionï¼ˆæ³¨æ„åŠ›åŠ é€Ÿï¼‰
  â”œâ”€â”€ 2023 Mambaï¼ˆSSM æŒ‘æˆ˜è€…ï¼‰
  â”œâ”€â”€ 2024 DeepSeek V3ï¼ˆMoE + MLAï¼‰
  â”œâ”€â”€ 2024 FlashAttention-3ï¼ˆH100 æ·±åº¦ä¼˜åŒ–ï¼‰
  â”œâ”€â”€ 2025 æ··åˆæ¶æ„ï¼ˆTransformer + SSMï¼‰
  â”‚        çº¿æ€§æ³¨æ„åŠ›è¿›å…¥ç”Ÿäº§ï¼ˆQwen3-Nextï¼‰
  â”‚        Flash-Linear-Attention ç”Ÿæ€ç¹è£
  â”‚
  â””â”€â”€ 2026 â”€â”€â”€â”€â†’ ???
            å¯èƒ½çš„æ–¹å‘ï¼š
            â€¢ æ··åˆ SSM + æ³¨æ„åŠ›æˆä¸ºé»˜è®¤æ¶æ„
            â€¢ çº¿æ€§æ³¨æ„åŠ›å…¨é¢æ›¿ä»£æ ‡å‡†æ³¨æ„åŠ›
            â€¢ æ¨ç†æ—¶å­¦ä¹ ï¼ˆTitans ç±»æ¨¡å‹ï¼‰
            â€¢ æ‰©æ•£å¼å¹¶è¡Œç”Ÿæˆ
```

> ğŸ“ **æœ¬èŠ‚å°ç»“**
>
> - FlashAttention ç³»åˆ—ï¼šä¸æ”¹å˜æ•°å­¦ï¼Œåªä¼˜åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡ â†’ è®©é•¿ä¸Šä¸‹æ–‡æˆä¸ºå¯èƒ½
> - Ring Attentionï¼šåˆ†å¸ƒå¼æ³¨æ„åŠ›è®¡ç®— â†’ ç™¾ä¸‡çº§ token ä¸Šä¸‹æ–‡
> - çº¿æ€§æ³¨æ„åŠ›ï¼šO(NÂ²) â†’ O(N)ï¼Œ2025 å¹´å·²è¿›å…¥ç”Ÿäº§
> - Mamba/SSMï¼šTransformer æœ€æœ‰åŠ›çš„æŒ‘æˆ˜è€…ï¼Œçº¿æ€§å¤æ‚åº¦ + é€‰æ‹©æ€§è®°å¿†
> - æ··åˆæ¶æ„æ˜¯å½“å‰æœ€æ˜ç¡®çš„è¶‹åŠ¿ï¼šç»“åˆæ³¨æ„åŠ›å’Œ SSM çš„ä¼˜ç‚¹
> - MoE æˆä¸ºå¤§æ¨¡å‹æ ‡é…ï¼šç”¨å°‘é‡æ¿€æ´»å‚æ•°è¾¾åˆ°å¯†é›†æ¨¡å‹çš„æ€§èƒ½
> - Transformer æ²¡æœ‰è¢«å–ä»£ï¼Œè€Œæ˜¯åœ¨**è¿›åŒ–**

---

## åäº”ã€æ€»ç»“ä¸å­¦ä¹ è·¯çº¿

æ­å–œä½ è¯»å®Œäº†è¿™ç¯‡è¶…é•¿çš„æ•™ç¨‹ï¼ğŸ‰ è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æ•´ä¸ª Transformer çš„çŸ¥è¯†ä½“ç³»ã€‚

### 15.1 ä¸ºä»€ä¹ˆæ¯ä¸ªäººéƒ½åº”è¯¥äº†è§£ Transformerï¼Ÿ

ä½ å¯èƒ½ä¼šæƒ³ï¼š"æˆ‘åˆä¸åšå¤§æ¨¡å‹ç®—æ³•ï¼Œå­¦è¿™äº›æœ‰ä»€ä¹ˆç”¨ï¼Ÿ"

**ç­”æ¡ˆæ˜¯ï¼šç†è§£ Transformer è®©ä½ æ›´å¥½åœ°ä½¿ç”¨ AIã€‚**

| åœºæ™¯         | ä¸æ‡‚åŸç†                       | æ‡‚åŸç†                                                     |
| ------------ | ------------------------------ | ---------------------------------------------------------- |
| å†™ Prompt    | å‡­æ„Ÿè§‰ï¼Œè¯•é”™ï¼Œ"ç„å­¦è°ƒå‚"       | çŸ¥é“æŠŠé‡ç‚¹æ”¾é¦–å°¾ï¼ˆU å‹æ³¨æ„åŠ›ï¼‰ï¼ŒçŸ¥é“ç”¨åˆ†éš”ç¬¦ï¼ˆæ³¨æ„åŠ›åˆ†åŒºï¼‰ |
| é€‰æ¸©åº¦å‚æ•°   | "0.7 å¥½åƒä¸é”™ï¼Ÿ"               | çŸ¥é“æ¸©åº¦æ˜¯ Softmax çš„ç¼©æ”¾å› å­ï¼Œä»£ç ç”¨ 0ï¼Œåˆ›æ„ç”¨ 1.0+       |
| å¤„ç†é•¿æ–‡æœ¬   | "å¡è¿›å»å°±è¡Œäº†å§ï¼Ÿ"             | çŸ¥é“å­˜åœ¨æ³¨æ„åŠ›ç¨€é‡Šå’Œ Lost in the Middleï¼Œç”¨ RAG ç²¾é€‰å†…å®¹   |
| è¯„ä¼° AI å·¥å…· | "è¿™ä¸ªæ¨¡å‹å¥½ç¥å¥‡ï¼"             | çŸ¥é“å®ƒåªæ˜¯åœ¨åšä¸‹ä¸€ä¸ª token é¢„æµ‹ï¼Œç†è§£å®ƒçš„å±€é™æ€§            |
| è¯»æŠ€æœ¯æ–°é—»   | "MoEï¼ŸSSMï¼Ÿçœ‹ä¸æ‡‚"             | èƒ½ç†è§£ä¸ºä»€ä¹ˆ DeepSeek ç”¨ 1/10 è®¡ç®—é‡è¾¾åˆ° GPT-4 æ°´å¹³        |
| å†™å¤æ‚æŒ‡ä»¤   | "ä¸ºä»€ä¹ˆ AI æ€»æ˜¯å¿˜äº†æˆ‘çš„è¦æ±‚ï¼Ÿ" | çŸ¥é“è¦é‡å¤å…³é”®æŒ‡ä»¤ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å åŠ å¯¹æŠ—ç¨€é‡Š                 |

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼šå­¦ Transformer ä¸æ˜¯ä¸ºäº†è‡ªå·±é€ å¤§æ¨¡å‹ï¼Œè€Œæ˜¯ä¸ºäº†**æˆä¸º AI æ—¶ä»£çš„æ˜ç™½äºº**â€”â€”çŸ¥å…¶ç„¶ï¼Œä¹ŸçŸ¥å…¶æ‰€ä»¥ç„¶ã€‚

### 15.2 æ ¸å¿ƒæ¦‚å¿µå›é¡¾

```
Transformerï¼ˆ2017 â†’ 2026 è¿›åŒ–ä¸­ï¼‰
â”œâ”€â”€ æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ ç‚¹ç§¯ç›¸ä¼¼åº¦ â†’ Softmax â†’ åŠ æƒç»„åˆ
â”‚   â”œâ”€â”€ Q/K/V çº¿æ€§æŠ•å½±ï¼ˆåŒä¸€è¾“å…¥çš„ä¸‰é‡è§’è‰²ï¼‰
â”‚   â”œâ”€â”€ ç¼©æ”¾å› å­ âˆšd_kï¼ˆé˜²æ­¢ Softmax é¥±å’Œï¼‰
â”‚   â””â”€â”€ å¤šå¤´æ³¨æ„åŠ›ï¼ˆå¤šè§’åº¦åŒæ—¶å…³æ³¨ï¼‰
â”œâ”€â”€ ä½ç½®ç¼–ç 
â”‚   â”œâ”€â”€ sin/cos å›ºå®šç¼–ç ï¼ˆåŸå§‹æ–¹æ¡ˆï¼‰
â”‚   â””â”€â”€ RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼Œç°ä»£ä¸»æµï¼‰
â”œâ”€â”€ Encoderï¼ˆåŒå‘æ³¨æ„åŠ›ï¼Œç†è§£ä¸“å®¶ï¼‰
â”‚   â”œâ”€â”€ Self-Attention + Add&Norm
â”‚   â””â”€â”€ FFN + Add&Norm
â”œâ”€â”€ Decoderï¼ˆå› æœæ³¨æ„åŠ›ï¼Œç”Ÿæˆä¸“å®¶ï¼‰
â”‚   â”œâ”€â”€ Masked Self-Attention + Add&Norm
â”‚   â”œâ”€â”€ Cross-Attention + Add&Norm
â”‚   â””â”€â”€ FFN + Add&Norm
â”œâ”€â”€ ä¸‰å¤§æ¶æ„å®¶æ—
â”‚   â”œâ”€â”€ BERTï¼ˆEncoder-Only â†’ ç†è§£ç±»ä»»åŠ¡ï¼‰
â”‚   â”œâ”€â”€ GPTï¼ˆDecoder-Only â†’ ç”Ÿæˆç±»ä»»åŠ¡ï¼Œå½“å‰ç»å¯¹ä¸»æµï¼‰
â”‚   â””â”€â”€ T5ï¼ˆEncoder-Decoder â†’ åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼‰
â”œâ”€â”€ ç°ä»£ä¼˜åŒ–
â”‚   â”œâ”€â”€ GQA / MLAï¼ˆå‡å°‘ KV Cache å†…å­˜ï¼‰
â”‚   â”œâ”€â”€ MoEï¼ˆç”¨å°‘é‡è®¡ç®—è¾¾åˆ°å¤§æ¨¡å‹æ€§èƒ½ï¼‰
â”‚   â”œâ”€â”€ FlashAttentionï¼ˆç¡¬ä»¶çº§ä¼˜åŒ–ï¼Œä¸æ”¹å˜æ•°å­¦ï¼‰
â”‚   â””â”€â”€ çº¿æ€§æ³¨æ„åŠ›ï¼ˆO(NÂ²) â†’ O(N)ï¼‰
â””â”€â”€ LLM ä½¿ç”¨æŠ€å·§
    â”œâ”€â”€ é¦–å°¾æ•ˆåº” â†’ é‡è¦ä¿¡æ¯æ”¾å¼€å¤´å’Œç»“å°¾
    â”œâ”€â”€ æ¸©åº¦å‚æ•° â†’ Softmax çš„éŸ³é‡æ—‹é’®
    â”œâ”€â”€ CoT â†’ æ³¨æ„åŠ›çš„å«è„šçŸ³
    â””â”€â”€ ç»“æ„åŒ– Prompt â†’ æ³¨æ„åŠ›åˆ†åŒºç®¡ç†
```

### 15.3 æ¨èå­¦ä¹ è·¯çº¿

| é˜¶æ®µ     | å†…å®¹                      | èµ„æº                                |
| -------- | ------------------------- | ----------------------------------- |
| **å…¥é—¨** | ç†è§£æ³¨æ„åŠ›æœºåˆ¶ç›´è§‰        | æœ¬æ–‡ç¬¬ä¸€åˆ°å››èŠ‚                      |
| **è¿›é˜¶** | æ‰‹å†™ Transformer          | æœ¬æ–‡ç¬¬äº”åˆ°åä¸€èŠ‚ + PyTorch å®˜æ–¹æ•™ç¨‹ |
| **åº”ç”¨** | å­¦ä¼šé«˜æ•ˆä½¿ç”¨ LLM          | æœ¬æ–‡ç¬¬åä¸‰èŠ‚ï¼ˆLLM ä½¿ç”¨æŠ€å·§ï¼‰        |
| **å®æˆ˜** | ä½¿ç”¨ HuggingFace å¾®è°ƒ     | HuggingFace Transformers æ–‡æ¡£       |
| **æ·±å…¥** | é˜…è¯»åŸå§‹è®ºæ–‡              | _Attention Is All You Need_ (2017)  |
| **å‰æ²¿** | Flash Attentionã€MoEã€SSM | æœ¬æ–‡ç¬¬åå››èŠ‚ + å„æ¨¡å‹æŠ€æœ¯æŠ¥å‘Š       |

### 15.4 å…³é”®è®ºæ–‡ä¸å‚è€ƒèµ„æ–™

> ğŸ“š **å‚è€ƒèµ„æ–™**
>
> - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Transformer åŸå§‹è®ºæ–‡ (Vaswani et al., 2017)
> - [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) â€” BERT è®ºæ–‡ (Devlin et al., 2018)
> - [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) â€” GPT-2 è®ºæ–‡ (Radford et al., 2019)
> - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) â€” T5 è®ºæ–‡ (Raffel et al., 2019)
> - [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135) â€” FlashAttention (Dao et al., 2022)
> - [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) â€” RoPE (Su et al., 2021)
> - [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) â€” DeepSeek V3 (DeepSeek-AI, 2024)
> - [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) â€” Mamba (Gu & Dao, 2023)
> - [The Big LLM Architecture Comparison](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison) â€” Sebastian Raschka çš„ LLM æ¶æ„å¯¹æ¯”

---

_æœ¬æ–‡å‚è€ƒäº† IBM Skills Network çš„ Transformer ç³»åˆ—å®éªŒã€å´æ©è¾¾æ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹çš„ Transformer ä½œä¸šï¼Œä»¥åŠå¤šç¯‡åŸå§‹è®ºæ–‡ã€‚æ„Ÿè°¢è¿™äº›ä¼˜ç§€çš„æ•™è‚²èµ„æºã€‚_

_æœ€åæ›´æ–°ï¼š2026-02-09_
